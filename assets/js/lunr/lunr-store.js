var store = [{
        "title": "Home and End keys - PuTTY - bash - Linux",
        "excerpt":"Here’s a simple one. But first, some backstory.   I’d been using the tcsh shell on my Linux servers for years, but I am now working in an environment where bash is the preferred shell. I view it as an opportunity to poke around in a shell that’s relatively unfamiliar to me as a dyed-in-the-wool tcsh user[1]. I’d also been used to connecting to my Linux servers with the SSH Communications Security version of the Secure Shell client. Unfortunately, the newest version of this client, which is available to me, will not connect to our department’s boxes due to the version (or configuration) of the OpenSSH libraries that have been installed. I’m new to this job, so I’m accepting this more or less on the authority of comments made by coworkers; I haven’t independently verified this, though I can confirm that I can’t connect with the SSH client. I switched to the PuTTY SSH client, since it’s free, widely used, and supported by my colleagues. And, oh yeah, it connects to our Linux boxes.   One of the neat features of the SSH Communications Security client is the ability map keys to certain other keys. For instance, I used to map the Home and End keys to &lt;Ctrl-a&gt; and &lt;Ctrl-e&gt; for ease of navigation on the command-line; sometimes the command strings I have to type are, to put it technically, really frickin’ long and it’s nice to be able to make edits to the beginning and end of these command strings without holding down the left and right arrow keys for an hour. PuTTY does not have a key-mapping feature, so I was wondering how to get my Home and End keys to do more than just generate the ‘~’ character. (Though such a feature is on PuTTY’s wish-list.)   It turns out there’s a simple fix: just set the environment variable, TERM, to the value of “linux” in ~/.bash_profile (or .bashrc).   export TERM=linux   should do the trick. Make sure you logout and login again, or just source your bash config files. It’s possible that other TERM values do the trick as well, but “linux” has worked for me. Prior to that, the TERM variable was set to the value “xterm”.   For some more shell-based key mapping geared toward the Backspace and Delete keys, see the Consistent BackSpace and Delete Configuration page.   1. While I’ve used tcsh as my default shell for years, I do acknowledge the argument that csh programming is considered harmful. I use the Bourne shell and Perl for scripting, though primarily the latter.  ","categories": [],
        "tags": [],
        "url": "/blog/2005/11/30/home-and-end-keys-putty-bash-linux/",
        "teaser": null
      },{
        "title": "Problems joining a Windows 2003 Active Directory domain",
        "excerpt":"One of the first tasks that has been assigned to me is the installation and configuration of a pair of network load-balanced Terminal Servers running Windows Server 2003. The department has already cobbled together documentation on how to build servers within the locally developed and recognized best practices, and I am loath to deviate from these in my first month of employment. I got up to the point of joining the first TS node (“TS1”) to the AD domain pretty smoothly. When I attempted to move TS1 out of its workgroup and into the domain, I was prompted for a password (which is a good thing, and is to be expected).  When I attempted to use my domain admin account in the form “DOMAIN\\account”, I was rudely greeted with an “unknown username or bad password” error.  When I tried to provide my credentials in the form of “account@domain.university.edu”, I received the unfamiliar “Element not found” message.   After poking around for a few hours, I came up with the following fix:   \tThe administrative account being used to join the server to the domain must be allowed logon rights on the server being added. This must be done on the domain controller. \tNTLM v2 authentication must be enabled in the Local Security Policy of the new server. Go to Administrative Tools / Local Security Policy and navigate to Security Settings / Local Policies / Security Options. In the right-hand pane, locate the policy named Network security : LAN Manager authentication level and change its value to Send NTLMv2 response only. (Note: I am unsure what other repercussions might be caused by changing this setting.)  These steps might not work for you, as they were likely necessary in my environment due to networking and domain configuration particulars.  ","categories": [],
        "tags": [],
        "url": "/blog/2005/11/30/problems-joining-a-windows-2003-active-directory-domain/",
        "teaser": null
      },{
        "title": "A restrictive IPSec script",
        "excerpt":"What do you do when you’ve got a server to install and you’re too lazy to burn a CD with all the latest service packs and hotfixes? I suppose you could attach the server to the Internet and head over to Microsoft’s Windows Update website. But then you would be committing a grievous faux pas among systems people. Only connect an unpatched machine to the Internet if you wish to have it 0wN3d in seconds flat.   One strategy for patching up your server is to install on your local network a server running Windows Software Update Services, and configure IPSec on your new server to allow connections only to the local WSUS server. For the sake of convenience, I have also allowed outgoing DNS requests. If you know the IP address of the WSUS server, these are probably unnecessary, but otherwise shouldn’t pose much of a risk.   Here’s an IPSec script, which I called newServerLockdown.txt, that you may use to accomplish this task.    # IPSec Configurations to Lock Down a New Server # # WHAT IS THE POINT? # Well, good security practices dictate keeping servers off the network until they have been # fully patched, which is rarely achievable from system CDs. Thus, before a server is conn- # ected to the network, we use IPSec to restrict traffic such that no host may initiate an # incoming connection, and only the local Windows Software Update Server may be contacted. # # HOW TO RUN THIS SCRIPT # netsh -f newServerLockdown.txt # # THEN WHAT? # Once the server is fully patched, hotfixed, and service packed, these IPSec rules may be # blown away with two commands, or so it is believed by the author: # netsh ipsec dynamic delete all # netsh ipsec static delete all # # NOTE # Originally tested on November 23, 2005 # Inspiration from: # http://www.unisanet.unisa.edu.au/staff/davidgardiner/ipsec/netsh-script.txt # and # http://www.windowsitpro.com/Articles/Print.cfm?ArticleID=41571  ############# Set IPSec mode to dynamic ############  pushd ipsec dynamic  # Dump packet drops to the Event Log set config property=ipsecdiagnostics value=7  # Allow as few exemptions as possible set config property=ipsecexempt value=3  # During boot sequence, allow only stateful connections initiated by the server set config property=bootmode value=stateful  popd  ############ Set IPSec mode back to dynamic ############  pushd ipsec static set store location=local  # Clean the slate and remember these settings # DO NOT DO THIS IF YOU DO NOT WANT YOUR STATIC CONFIGS ZAPPED! delete all  # Create a new policy add policy name=\"Restrict to WSUS\" activatedefaultrule=NO  # Create actions for filters to use add filteraction name=\"PERMIT\" action=PERMIT add filteraction name=\"BLOCK\" action=BLOCK  # Default policy - block everything add filterlist name=\"All incoming traffic\" add filter filterlist=\"All incoming traffic\" protocol=ANY srcaddr=ANY dstaddr=ANY description=\"Block all incoming traffic\" add rule name=\"Default incoming block\" policy=\"Restrict to WSUS\" filterlist=\"All incoming traffic\" filteraction=\"BLOCK\"  # Allow outgoing DNS requests add filterlist name=\"DNS resolution\" add filter filterlist=\"DNS resolution\" protocol=TCP srcaddr=ME srcport=0 dstaddr=DNS dstport=53 mirrored=YES add filter filterlist=\"DNS resolution\" protocol=UDP srcaddr=ME srcport=0 dstaddr=DNS dstport=53 mirrored=YES add rule name=\"Allow DNS resolution\" policy=\"Restrict to WSUS\" filterlist=\"DNS resolution\" filteraction=\"PERMIT\"  # Allow outgoing HTTP connections to WSUS add filterlist name=\"HTTP\" description=\"Allow outbound HTTP connections to WSUS\" add filter filterlist=\"HTTP\" protocol=TCP srcaddr=ME srcport=0 dstaddr=YOUR.WSUS.HOST.NAME dstport=80 mirrored=YES add filter filterlist=\"HTTP\" protocol=TCP srcaddr=ME srcport=0 dstaddr=YOUR.WSUS.HOST.NAME dstport=443 mirrored=YES add rule name=\"Allow HTTP traffic to WSUS\" policy=\"Restrict to WSUS\" filterlist=\"HTTP\" filteraction=\"PERMIT\"  # Activate policy set policy name=\"Restrict to WSUS\" assign=YES  popd exit  ","categories": [],
        "tags": [],
        "url": "/blog/2005/11/30/a-restrictive-ipsec-script/",
        "teaser": null
      },{
        "title": "List of Free Software",
        "excerpt":"Here’s a list of all the free software I’m running on my Windows XP workstation, or least the subset that I deem noteworthy. Rather than annotate the list, which would be far too helpful, I will merely provide links.   Foundstone Vision 1.0 (system util) Process Explorer 9.25 (system util) Microsoft PowerToys XP (OS pimpage) Notepad++ 3.3 (text editor) Cygwin 1.5.18-1 (X server / *nix tools) FileZilla 2.2.17 (FTP client) FileZilla Server 0.9.11b (FTP server) Mozilla Firefox 1.5 (WWW browser) Mozilla Thunderbird 1.5 (mail client) Trillian Basic 3.1 (multi-network chat client) PuTTY 0.57 (SSH client) Semagic 1.5.5.6U (blog client) iTunes 6.0.1.3 (aural pleasure)   P.S. I do have Firefox installed but I don’t use it. Internet Explorer is the only way to browse.  ","categories": [],
        "tags": [],
        "url": "/blog/2005/11/30/list-of-free-software/",
        "teaser": null
      },{
        "title": "Google Maps JavaScript problem in IE",
        "excerpt":"Internet Explorer likes to throw the “Operation Aborted” error when trying to hook into the Google Maps API via JavaScript, at least when the JavaScript is placed where it is supposed to, i.e., a reference to the Google Maps JavaScript in the page HEAD and the actual rendering of the map within the DIV tag.   To fix this in IE, move the DIV block JavaScript code near the bottom of your HTML. Place it right between the terminating BODY tag and the terminating HTML tag. The problem seems to be that IE gets all confused when JavaScript attempts to make modifications to the page – e.g., sucking down a map from Google – while still rendering the body HTML. There are a couple other fix options here:   http://www.mapki.com/index.php?title=FAQs#Browser_Problems   It is worth noting that this “fix” does not break functionality in Firefox. And, really, who cares about the other browsers? Communists and robots, my friend.   In the HEAD of your page, you may include the initial JavaScript  &lt;script src=\"http://maps.google.com/maps?file=api&amp;v=1&amp;key=BLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAH\" mce_src=\"http://maps.google.com/maps?file=api&amp;v=1&amp;key=BLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAH\" type=\"text/javascript\"&gt;&lt;/script&gt;Â  Though it surprised me that I couldn’t remove the terminating SCRIPT tag and instead make the first SCRIPT tag self-terminating, i.e., instead of &lt;script foo=\"bar\"&gt;&lt;/script&gt;, I tried &lt;script foo=\"bar\"/&gt; and it didn’t work.   The JavaScript that actually renders the map within the DIV tag, cleverly named “map”, should look similar to the following:   &lt;/BODY&gt; &lt;script type=\"text/javascript\"&gt; //&lt;![CDATA[Â  var map = new GMap(document.getElementById(\"map\")); map.setMapType(G_HYBRID_TYPE); map.addControl(new GSmallMapControl()); map.addControl(new GMapTypeControl()); map.centerAndZoom(new GPoint(-666.666,66.6666), 2); map.openInfoWindowHtml(map.getCenterLatLng(), \"100 Main St.&lt;BR&gt;Nowheresville, ZZ 99999 ZZZ\");  //]]&gt; &lt;/script&gt; &lt;/HTML&gt;  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/02/google-maps-javascript-problem-in-ie/",
        "teaser": null
      },{
        "title": "Automated System Recovery",
        "excerpt":"Here is a link describing how one might use ASR – the new term for Emergency Repair Disk – in Windows Server 2003.   Of particular note is how to use ASR when a server does not have a floppy drive, a predicament I now find myself in.   http://hacks.oreilly.com/pub/h/1196  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/05/automated-system-recovery/",
        "teaser": null
      },{
        "title": "WSF - good for metascripting?",
        "excerpt":"The Windows Script File, .wsf, allows one to mark-up in XML different blocks of scripting. One can, in effect, write a script hooking VBScript, JavaScript, and PerlScript together. This looks to be quite powerful for scripters, especially within the domain of systems administration. Often one knows one scripting language better than another, and even more commonly one may exploit the strengths of multiple languages. Imagine combining the ease of Windows operability in VB with the regular expression power of PerlScript with... whatever it is that JavaScript does well. Could be quite a nifty tool.  http://msdn.microsoft.com/library/default.asp?url=/library/en-us/script56/html/wsAdvantagesOfWs.asp   ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/05/wsf-good-for-metascripting/",
        "teaser": null
      },{
        "title": "NLB vs. MSCS, or load-balancing versus clustering",
        "excerpt":"Not quite sure you grasp the subtleties of difference between load-balancing (NLB) and server clustering (MSCS)? After all, both are technologies that allow distinct server nodes to be externally visible via a virtual server, and support failover. The fundamental similarities might overshadow their concisely stated difference:   Server clusters â€“ uses a shared-nothing architecture, which means that a resource can be active on only one server in the cluster at any one time. Because of this, it is well suited to applications that maintain some sort of state (for example, a database).   Network Load Balancing (NLB) â€“ uses a load balancing architecture, which means that a resource can be active on ALL servers in the cluster at any one time. Because of this, it is well suited to applications that do not maintain state (for example, a Web server).   Note: This information is copyright Â© 2003, Microsoft Corporation, gleaned from the freely available documentation on cluster quorums for Windows Server 2003. I provide it here for the sake of convenience and exposure.  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/06/nlb-vs-mscs-or-load-balancing-versus-clustering/",
        "teaser": null
      },{
        "title": "Microsoft Office Viewers",
        "excerpt":"For users who have neither access to Microsoft Office nor desire to dive headfirst into OpenOffice*, Microsoft provides freely downloadable viewers for Office documents. http://office.microsoft.com/en-us/assistance/HA010449811033.aspx Most of these viewers are at the Office 2003 version, e.g., Word, PowerPoint, and Excel. Others, like Access, are at the 2002 version. * For the record, though, OpenOffice 2.0 is fabulous. I've exclusively been using OO at home for at least a year or two, and the new version is tres magnifique.   ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/06/microsoft-office-viewers/",
        "teaser": null
      },{
        "title": "Windows Server 2003 still dependent upon WINS / NetBIOS?",
        "excerpt":"Perhaps I am mistaken, but I thought one of the biggest purported benefits of the newest incarnation of Active Directory was its supposed reliance upon the more commonly used DNS system for computer name lookups rather the old WINS and NetBIOS lookup system. I’ve recently installed a new server running Windows Server 2003 Enterprise and hooked it into our Active Directory 2003 domain and configured it to run Terminal Services. Upon this assumption, I disabled NetBIOS in the TCP/IP stack and turned off the TCP/IP NetBIOS Helper service.   When I connect to the server via Remote Desktop and login with a local administrative account, I get in just fine. When I specify my domain credentials, however, my connection is refused because Terminal Services apparently has problems reaching the RPC Server. Says it is unavailable. Turning back on the NetBT Helper service clears this up, but I did not think NetBIOS would be required for name resolution given AD integration with DNS. In the eventlog, TS shows the following error after the “RPC Server is unavailable” error: “Unable to obtain Terminal Server User Configuration”.   Any ideas? I’m alright with leaving on the NetBT service, but I just don’t understand why it’s necessary.   I have tried re-enabling NetBIOS in the stack and that has zero effect on the connection behavior. I have also checked the DNS settings and the AD domain name is in the list of default domains to search through.   NOTE: Perhaps it is due to having external University DNS servers listed instead of the AD DNS servers. Come to think of it, I’m not sure why we would be using external DNS servers if we run AD. Giving this a try.   ADDENDUM: Apparently our AD servers do not run their own DNS; the records are offloaded to the campus DNS system. Perhaps this is the culprit?  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/07/windows-server-2003-still-dependent-upon-wins-netbios/",
        "teaser": null
      },{
        "title": "Access-based Enumeration &amp; Windows Server 2003 R2",
        "excerpt":"As of the SP1 release, Windows Server 2003 now supports access-based enumeration of file shares. Basically, files and folders to which users lack access will not be visible within file shares. No more double-clicking shared resources only to be greeted with “Access denied.” This is quite a nice feature, and one which is long overdue.   http://www.microsoft.com/windowsserver2003/techinfo/overview/abe.mspx   Also, the new release of Windows Server 2003 R2 appears to have a number of new features that were not included in SP1 (and that probably will not be featured in SP2). As far as I can figure out, R2 is related to 2003 SP1 as NT4 Option Pack was related to NT4.   http://www.microsoft.com/windowsserver2003/r2/whatsnewinr2.mspx  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/08/access-based-enumeration-windows-server-2003-r2/",
        "teaser": null
      },{
        "title": "Unrelated points",
        "excerpt":"I’ve been hearing a lot about the Ruby programming language lately, and specifically about Ruby on Rails. After looking at different strategies to get this sucker up and running, I decided to take the path of the cowardly and install Instant Rails. I am still trying to figure out what the heck it is and does, but a lot of people seem to like it for throwing together quick, powerful, open, and structured web applications.   On a related “gaining in popularity” note is S5, an open-source, open-standards application for producing web-enabled presentations. No more must one be a slave to that wicked master, Microsoft Powerpoint (or OO.o Impress, for that matter, or whatever you crazy Mac heathens use). Now one can produce a web presentation using S5, which puts all the material into one XHTML file. It uses CSS and JavaScript for styles and functionality, respectively. I think I might check that out as well. Counterpoint: OpenOffice 2.0’s Impress application does a bang-up job as well.   And finally, what is all this talk of the Web 2.0? Here’s a more or less full run-down of Web 2.0, and five reasons why it matters.   In the meantime, I’m still cranking away on my NLB’d terminal servers.   (And another trackback test).Â Â  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/09/unrelated-points/",
        "teaser": null
      },{
        "title": "Ruby on Rails, Revisited",
        "excerpt":"Forget about that Instant Rails stuff. Try the following tutorial instead, which gave me a better sense of how Rails actually works. Very, very helpful.   http://www.onlamp.com/pub/a/onlamp/2005/01/20/rails.html   Ruby on Rails is quite cool. What I don’t like about it, so far:   \tI don't know Ruby, so I can't do anything sophisticated with Rails \tI'm not sure how much I like the idea of all Rails applications being so intimately tied to the Rails framework.  ","categories": [],
        "tags": [],
        "url": "/blog/2005/12/12/ruby-on-rails-revisited/",
        "teaser": null
      },{
        "title": "A Mere Test",
        "excerpt":"I used to post my work-related notes here, but decided to give WP a shot.   And this is a trackback test.Â   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/02/21/a-mere-test/",
        "teaser": null
      },{
        "title": "unAPI, COinS-PMH, OpenSearch support",
        "excerpt":"I’ve decided to use this blog partly to write about work-related points of interest but also to futz around with blogging technologies and such.Â   Thanks to a couple of very helpful posts and some neat WordPress hacks   http://www.wallandbinkley.com/quaedam/?p=59Â http://www.wallandbinkley.com/quaedam/?p=50Â   Technosophia is now unAPI-1.0- and COinS-PMH-compliant!Â  Under each post there are shortcut links to view the Dublin Core, MODS, and other unAPI formats (should I add them in at a later date).   It started dawning on me today just how useful unAPI might turn out to be, mostly because I re-read Dan Chudnov’s slides for the first time seeing his talk live at code4libCon ‘06.   Addendum the first: This is also OpenSearch 1.1-compliant, thanks to: http://www.williamsburger.com/wb/archives/opensearch-v-1-1.Â  Excellent.   Addendum the second: I’m also PURL-friendly - http://purl.org/maint/linkpurl.html.Â  Well, at least if you have a friendly client (read: not IE).  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/01/joining-the-21st-century-one-hack-at-a-time/",
        "teaser": null
      },{
        "title": "Translating 'pig' to 'pork' and vice versa: A metaphor for library technologists",
        "excerpt":"I’ve been slowly making my way through Melvyn Bragg’s “The Adventure of English,” which tells the story of the English language [1]. I read a bit the other night that I found rather fascinating, highlighting the symbiotic relationship between culture and language. Moreover, I found therein a perhaps sloppy metaphor for the unique role that library technologists play. Here’s an excerpt from the section about how the English language persisted, perhaps even thrived, despite the threat posed to it by the Normans who had recently come to power in England:  While the English-speaking peasants lived in small, often one-roomed mud and wattle cottages, or huts, their French-speaking masters lived in high stone castles. Many aspects of our modern vocabulary reflect the distinctions between them.  English speakers tended the living cattle, for instance, which we still call by the Old English words \"ox\" or, more usually today, \"cow.\" French speakers ate prepared meat which came to the table, which we call by the French word \"beef.\" In the same way the English \"sheep\" became the French \"mutton,\" \"calf\" became \"veal,\" \"deer\" became \"venison,\" \"pig\" \"pork,\" English animal, French meat in every case.  The English laboured, the French feasted. Though I risk making broad generalizations here, I’ll share my thoughts on this briefly. I tend to think of technologists as being better-versed in speaking about the animal, the “pig,” the low-level guts, nuts, and bolts, the “implementation details” than are librarians, generally speaking. On the flip side of the coin, I tend to think that librarians do have a better grasp on the user’s needs, or at least have their needs in mind, and thus focus more on the end-product, the “pork.” Library technologists, then, fill the gap between pig and pork. We know enough about the pig to know that we can produce pork with it. And we know enough about how to prepare pork that we can procure pigs if the need arises.   This sounded better in my head. If it accomplished nothing else, it sure gave me a craving for bacon.      Bragg, M. (2003). The adventure of English: The biography of a language. United Kingdom: Hodder &amp; Stoughton. [Available: http://www.amazon.co.uk/exec/obidos/ASIN/0340829931/203-0353691-6278303]  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/04/translating-pig-to-pork-and-vice-versa-a-metaphor-for-library-technologists/",
        "teaser": null
      },{
        "title": "The impact of open access on academic libraries [excerpt]",
        "excerpt":"Broadly defined, open access makes scholarly materials accessible to users at no cost. More specifically, the term is used to describe a model of scholarly communication in which users may freely view, download, copy, and print scholarly articles, books, conference proceedings, squibs, and so forth. Such a model is in stark contrast to existing models of scholarly communication in that many of the most widely-used peer-reviewed journals are accessible to libraries primarily through expensive bulk package plans, forcing libraries to pay top-dollar for the resources their faculties require. In so doing, libraries add to their collections a number of rarely-used journals of minimal impact and value simply because they were bundled in with the journals they could not do without: a model not unlike those provided by the local cable company â€“ i.e., if one wants the Food Network, one is also saddled with the Golf Channel.  In actuality, there are a number of different models of open access that adhere more or less to the principle of providing scholarly materials free of charge. Tenopir (2004) explains that open access:    \"includes many publication and distribution schemes. E-journals that are published, distributed electronically, and subsidized by universities, government agencies, and volunteer organizations are the most common. In addition, collections of separate articles or research reports could fit the definition, including e-print servers such as arXiv.org, institutional repositories, and author web pages.\" (p. 33)   The numerous models of open access may typically be categorized under one of the two rubrics proposed by open access champion, Steven Harnad. In the \"gold\" open access model, materials are freely and immediately provided in universally accessible electronic journals. The \"green\" open access model might be seen as an intermediate phase between current fee-based access models and the gold model, in which authors continue to publish in journals, whether they be print-based or electronic, but deposit copies, perhaps pre-prints, into an institutional or subject repository (Crawford, 2005b).  There are thus many forms that open access publications may take, each having its own costs and benefits. What they share is the very general principle which is poignantly stated by Harnad; \"the objective of open access is to maximize research impact by maximizing research access.\" While the benefits are many and clear, the issue of cost is one that has to be agreed upon.  Open access publishing typically implies that the user is able to freely access scholarly materials because the price of publication has been assumed by another party, usually the author of the material, the author's institution, or the grant which funded the research (Tenopir, 2004). One can see that open access publication is not, therefore, a completely cost-free endeavor. Indeed, the costs have merely been shifted from the consumers of information to the producers, or those who fund them (Wren, 2005), which applies equally to both the gold and green models of open access.  It is observed in this paper that all flavors and forms of open access impact the roles filled by academic libraries, but it is worth noting that these may vary. For instance, while the green model of open access will undoubtedly benefit scholars by globally providing scholarly material at no cost, with no access restrictions, other benefits such as budget relief may not be realized (Crawford, 2005b). In fact, it may strain budgets that are already being stretched by commercial journals.  The scope of this paper is limited to academic libraries primarily because of the close relationship they have with university faculties, i.e., those who both contribute the most to scholarly journals, and have strong needs for access to same. Many of the impacts discussed in this paper might also apply to public, school, and special libraries, but the scope is limited due to the proximity academic libraries have to the world of scholarly communication.  It is not the intention of the author to paint a simple, rosy picture of the issues surrounding open access, nor to advocate a radical, wholesale shift thereto. Rather, it is suggested only that the issues surrounding open access be brought out into the open and discussed. While there are reasons academic libraries might be cautious about modifying the ways they support scholarly communication, there are myriad reasons to consider how they might best serve their communities with open access.  …  Read the paper in its entirety.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/06/the-impact-of-open-access-on-academic-libraries/",
        "teaser": null
      },{
        "title": "The role of skepticism in human-information behavior [excerpt]",
        "excerpt":"NOTE: This article has been revised and published in the Library Student Journal.   Even a cursory review of social science literature reveals a wealth of research into the role that skepticism plays in the forms of information behavior studied within communication, consumer psychology, education, journalism and media studies, and public policy, to name only a handful of disciplines. In much of this research, the effects of skepticism are found to be strong and numerous, and yet it seems that skepticism has not been studied to a great extent within the body of human-information behavior research. The goals of this paper are two-fold: the first being to establish skepticism as a factor which ought to be considered in cognitive-affective models of human-information behavior, via a large-scale overview of social science research; and the second being to show that a rational form of skepticism is a healthy trait to cultivate among information-seekers.  I am interested in the role of skepticism â€“ defined in the Merriam-Webster dictionary as \"an attitude of doubt or a disposition to incredulity either in general or toward a particular object\" â€“ in human-information behavior (HIB), i.e., in information needs, seeking, evaluation, and usage. An operational definition of skepticism will be derived from a broad range of research in the social sciences, primarily in communication, psychology, marketing, media studies, and education, and will be expressed within the parlance of the cognitive viewpoint as a knowledge structure. Skepticism is established as a  significant issue in the research of other social science disciplines, and it will be argued that HIB research would benefit from examining as well the role of skepticism. The many facets of skepticism will be explored and then applied to HIB with suggestions as to how the issue might be approached in future research. A skeptical attitude may initially be seen as a drawback to information behavior; after all, how may one seek and use information from the multitudes of sources that one has not yet come to trust as authorities? There may, however, be important and unexpected benefits of skepticism. Finally, it will be argued that rational skepticism is beneficial, and methods of cultivating skepticism are discussed.  … Read the original paper in its entirety (though, really, you should read the published version instead).  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/06/the-role-of-skepticism-in-human-information-behavior-a-cognitive-affective-analysis/",
        "teaser": null
      },{
        "title": "A comparative analysis of keyword extraction techniques [excerpt]",
        "excerpt":"With widespread digitization of printed materials and steady growth of \"born-digital\" resources, there arise certain questions about access and discoverability. One such question is whether the full-text of this content, produced by advanced optical character recognition (OCR) techniques, is sufficient as a descriptor of the content. Will the model of mass digitization and full-text searching enable users to find the information they need? Or will we need to continue employing the classification skills of highly qualified human beings in order to ensure information is discoverable? The latter model seems to have worked well for the library community, with trained indexers and catalogers summarizing documents according to established standards and widely used thesauri or controlled vocabularies. The predictability of these techniques has some obvious benefits, such as consistency across different systems, the ability to construct browse interfaces in addition to search ones, and reduction of common errors such as differences in case, punctuation, spelling, and so forth.  The process of human classification has thus proven to be quite effective in our endeavors to organize information.  The question of whether we will continue to classify digital content in a similar manner ought to be asked. Is there any hope to keep up with the dizzying pace with which documents are digitized? Classification is a costly, time-consuming process, requiring highly trained individuals to consume a large amount of information and summarize it. If the goal is to continue digitizing and making accessible information at the current rate, it is improbable that human  catalogers and indexers will be able to keep up without sacrificing some of the quality that results from their considerable skills. Yet the goal of enhancing access and discoverability of digital content is one that ought to be pursued, and will likely not be realized through full-text searching alone. Indeed, why should we put so much time and effort into the process of digitization if it does not benefit our users?  Fortunately, the process of automatic extraction of keywords is one that has received much attention. As implied by the phrase, automatic keyword extraction is a process by which representative terms are systematically extracted from a text with either minimal or no human intervention, depending on the model. The goal of automatic extraction is to apply the power and speed of computation to the problems of access and discoverability, adding value to information organization and retrieval without the significant costs and drawbacks associated with human indexers. Research is taking place in numerous fields across the globe, and there is no clear frontrunner among the technologies and algorithms. This paper explores five approaches to keyword extraction, as presented in research papers, to demonstrate the different ways keywords may be extracted, to reflect commonalities between the approaches, and to evaluate the results thereof. Each paper is presented in a different section, for ease of organization.  ... Read the paper in its entirety.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/06/a-comparative-analysis-of-keyword-extraction-techniques/",
        "teaser": null
      },{
        "title": "unAPI revision 1-compliant",
        "excerpt":"I noticed Dan Chudnov’s earlierÂ note about the launch of the unAPI websiteÂ and noted in particular the the unAPI revision 1 specification.Â  I decided to give Technosophia a run through some error cases and some of the errors came up as 400 where they should have been 404 or 406.Â  I made a few minor tweaks to pbinkley’s unAPI WordPress plug-inÂ and I believe Technosophia is now fully compliant with unAPI revision 1.Â   Here are the test cases I used:   \t/unapi.php 200 Ok \t/unapi.php?uri=oai%3Alackoftalent.org%3Atechnosophia%3A25 300 Multiple Choices \t/unapi.php?uri=oai%3Alackoftalent.org%3Atechnosophia%3A25&amp;format=mods 200 Ok \t/unapi.php?uri=oai%3Alackoftalent.org%3Atechnosophia%3A25&amp;format=BADFORMAT 406 Not Acceptable \t/unapi.php?uri=BADURI&amp;format=mods 404 Not Found \t/unapi.php?format=mods 400 Bad Request  Woohoo?   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/07/unapi-revision-1-compliant/",
        "teaser": null
      },{
        "title": "\"Local Service\" Services Dying Repeatedly?",
        "excerpt":"One of my Windows 2003 servers began exhibiting very strange behavior a few months ago.Â  That a Windows server behaves badly isn’t strange, of course, but I’d never before encountered precisely this problem.Â  I searched and searched for solutions on Google, but could not findÂ one that worked for me (though I didÂ discover thatÂ a handful of others reported having very similar issues).Â  Since I’ve gotten so much help from the web (via Google) before, I figured I would pay back my debt by posting the solution I found this afternoon.Â  YourÂ kilometerage mayÂ vary.   Problem  Services which log on as LocalService*, as opposed to LocalSystem and NetworkService, die repeatedly and at regular intervals.Â  In my case, it was every 90 seconds give or take 5.Â  I believe Windows 2003 and XP use the LocalService and NetworkService accounts for runningÂ services,Â so this may not apply to Windows 2000 or other versions of Windows.Â  * Some of these services are Windows Time, TCP/IP NetBIOS Helper, Remote Registry, Application Layer Gateway Service, Alerter, Smart Card, SSDP Discovery Service, Universal PnP Device Host, WebClient, and Windows User Mode Driver Framework.Â  Solution  Disable WINEXIT.SCR screensaverÂ within any relevant user accounts, e.g., Default User, All Users, or any service account.Â  You can run a search in regedit for the string \"winexit.scr\" to turn up all such values and determine which are relevant.Â  The value I needed to delete was in HKEY_USERS/.DEFAULT/Control Panel/Desktop/SCRNSAVE.EXE. Explanation  The server in question is to be used for public terminal services, so I intended to use the winexit.scr screensaver as a relatively lightweight, easy way to ensure that users get warned about idle time and subsequently logged off after a preset period of time.Â  There are other ways of accomplishing this, to be sure, but I've always had good experiences with winexit.scr.Â  At any rate, while watching the list of processes in Task Manager, I noticed that about every minute-and-a-half, a winexit.scr process popped up and, more interestingly, was running in the LocalService context.Â  Lightbulb!  Services running under LocalService are all launched via the svchost.exe binary that is included with Windows, and it turns out that the winexit screensaver kept shutting down these services (as I unwittingly instructed it to do) when they took advantage of their ability to log on as a service.Â  Why every 90 seconds?Â  I had winexit configured to allow only 60 seconds of idle time, and it gives the user 30 seconds of warning before killing a session.Â  It’s almost as though computers are logical.Â  I just love when problems make sense.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/03/09/local-service-services-dying-repeatedly/",
        "teaser": null
      },{
        "title": "Managing Services: From Red Hat to Debian",
        "excerpt":"A note forÂ Linux sysadmins who’re scouringÂ Google for answers to this issue:Â   Having administered Red Hat servers for years at prior places of work, I’d gotten quite used to doing things the Red Hat Wayâ„¢, e.g., using the chkconfig and setup tools to manage services and the runlevels at which they are enabled.Â  These tools apparently aren’t used in Debian, which is the preferred distro at MPOW.Â  Instead, I’ve used update-rc.d and rcconf, respectively, to manage services on Debian.Â  They work similar to the aforementioned Red Hat tools, but I’m not yet that familiar with them; I’m inclined to say that the RH tools are more intuitive to use, but I may have some recall bias.   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/04/19/managing-services-from-red-hat-to-debian/",
        "teaser": null
      },{
        "title": "DSpace.war, what is it good for?",
        "excerpt":"Absolutely nothin’!   Â (Feeling both punchy and worn out by DSpace today.)  ","categories": [],
        "tags": [],
        "url": "/blog/2006/04/20/dspacewar-what-is-it-good-for/",
        "teaser": null
      },{
        "title": "Putting a face to a name",
        "excerpt":"During a recent visit to the area, my mother took a photograph of my (soon-to-be) wife and myself which turned out halfway decent, so I figured I would post it to aid those who like putting faces to names.  If you run into me at a conference, you might recognize my mug and we can discuss library technology, preservation, repositories, indie rock, video games, comic books, or beer itself over a pint or two.       Addendum: a slightly more recent, and formal, picture is available in another post.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/04/25/putting-a-face-to-a-name/",
        "teaser": null
      },{
        "title": "The Jester's Case for Fedora",
        "excerpt":"Peter Murray has written a series of pieces about the Fedora digital repository systemÂ over at the Disruptive Library Technology Jester blog.   In the first piece, On the Need for a General Purpose Digital Object Repository, it is argued that having a unified repository simplifies management of information systems or “silos.”Â  For instance,Â there needn’t be duplication of workflows or synchronization of content if a numberÂ of an organization’s repositories, digital libraries, electronic journals, course management systems and so on are all built atop a robust institutional repository.Â  A unified repository is useful if one desires a search across previously disparate digital projects or collections, if one wishes to eliminate redundancies in coding, if one intends to have a particular object, collection of objects, or part of an object shared across different systems – e.g., a journal article repurposed in a course management system and deposited into an open archive.Â  With an open,Â flexible repository,Â like Fedora, such a configuration is possible assuming your organization, unit, or consortiumÂ has someone to devote to managing and customizing the repository.Â   An advantage of using the Fedora system, as outlined in Why Fedora? Because You Don’t Need Fedora, is that due to modular design and adherence to more or less open standards, one is not necessarily wedded to Fedora for the foreseeable future.Â  Items inÂ a Fedora repository are serialized as XML objects, either in the Fedora-METS or FOXML format.Â  While some of this information is copied into aÂ relational database system and anÂ RDF triplestore for speed and convenience, it is all intact within the serialized XML objects which reside in a predictable directory hierarchy on the local filesystem.Â  There are at least two advantages to this design:   \tShould Fedora experience a catastrophic system glitch, one may rebuild the entire system via a built-in utility (cleverly named \"fedora-rebuild\") that goes through the objects on the filesystem and restocks the database and triplestore.Â  And assuming that the administrator of the system is worth his salt, there should be regular full backups of the filesystem, so the entire repository should be rebuildable.Â  As Peter notes, a simple copy of the filesystem on which the XML objects reside is a fine practice in a larger digital preservation strategy. \tIf one decides to move away from Fedora to the Next Best Thingâ„¢, it should be relatively simple to migrate content from Fedora into the new system because of Fedora's storage of all objects (and associated metadata, files, and disseminators) to the filesystem as serialized XML.Â  All one needs, perhaps,Â is a set of funky XSLT scripts to massage the objects into a format that works with the new system and voila.Â  (That is a gross oversimplification, but the point remains that open standards, simple file operations, and XML markup do make for more orderly migrations than black boxes, complex datastores, and loose coupling of information.) \tHaving one's objects stored as XML on the filesystem also opens up opportunities to see how tools which act thereupon might be glued into the repository infrastructure.Â  One such example might be for an XML-aware search engine (such as amberfish, Lucene, or Zebra).Â  Since you've got low-level access to these files, it would be fairly simple to tack on a search &amp; indexingÂ system that is independent of your choice of repository.  The third piece, Thinking about Our Fedora Disseminators, highlights Fedora as a repository system that’s put real emphasis on digital preservation.Â  While other repository systems allow for preservation of an object and its metadata, Fedora grants one the ability to preserve the behavior of digital objects and the datastreams thereof,Â a potentialÂ approach to the issue of format migration/emulation.Â Â Through a dissemination abstraction (the “behavior definition”) one might apply the same abstract behaviors to items in different formats, saving one the time ofÂ defining redundant behaviors.Â  My explanation is rather vague and incomplete, so I would encourage you to read Peter’s third piece in detail.Â  The point is that “for each record, the application simply asked the repository to deliver a thumbnail of the object. And the repository, regardless of media type, delivered one.”Â   Taken together, Peter makes a strong case for Fedora as a fine back-end for a unified, multi-purposeÂ repository.Â  Unlike other repository systems that focus more on the front-end, Fedora focuses on being the plumbing, the “digital library operating system” as Ron Jantz calls it.Â Â Â  Were I not already a Fedora enthusiast, I would find it quite difficult not to consider Fedora (or something like it, such asÂ LANL’s aDORe Archive)Â at MPOW.Â  Now if someone can send me some hints on drumming up institutional support…   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/05/02/the-jesters-case-for-fedora/",
        "teaser": null
      },{
        "title": "Sharing is caring",
        "excerpt":"So I'm finally on del.icio.us. If anyone wants to recommend bookmarks for me (or vice versa), here I am:  http://del.icio.us/Technosophia Â   ","categories": [],
        "tags": [],
        "url": "/blog/2006/05/09/sharing-is-caring/",
        "teaser": null
      },{
        "title": "unAPI-rev3 compliance",
        "excerpt":"Technosophia is now compliant with unAPI-revision 3.Â  Cruise around and let me know if you turn up any bugs.   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/05/18/unapi-rev3-compliance/",
        "teaser": null
      },{
        "title": "unAPI revision 3 plug-in for WordPress",
        "excerpt":"The unAPI plug-in for WordPress has moved to the following location: https://mike.giarlo.name/blog/unapi-wordpress-plug-in/  ","categories": [],
        "tags": [],
        "url": "/blog/2006/05/19/unapi-revision-3-plug-in-for-wordpress/",
        "teaser": null
      },{
        "title": "OCLC report on students' perceptions of libraries",
        "excerpt":"From http://www.oclc.org/reports/perceptionscollege.htmÂ –  College Studentsâ€™ Perceptions of Libraries and Information Resources examines the information-seeking habits and preferences of international college students. This report is a companion piece to the December 2005 OCLC Perceptions of Libraries and Information Resources report.  The 396 college students who participated in the survey range in age from 15 to 57 and are either undergraduate or graduate students. The college students were from all of the six countries included in the survey (Australia, Canada, India, Singapore, United Kingdom and the United States). Responses from U.S. 14- to 17-year-old participants have also been included to provide contrast and comparison with the college students, as these young people are potential college attendees.  With all-new graphs and additional analysis of how college student data compare to that of total respondents, this report is a subset of the original Perceptions report and provides findings from the online survey in an effort to learn more about:  \tLibrary use \tAwareness and use of library electronic resources \tThe Internet search engine, the library and the librarian \tFree vs. for-fee information \tThe â€œLibraryâ€ brand  This report looks at these questions from the point-of-view of college students and 14- to 17-year-olds. In the original study, we found that college students are more aware of and use librariesâ€™ information resources more than other survey respondents. In addition, the more educated the respondents, the more they continue to use libraries after graduation. Awareness does not always translate into high usage.  Overall, respondents have positive, if outdated, views of the â€œLibrary.â€ Younger respondentsâ€”teenagers and young adultsâ€”do not express positive associations as frequently. These findings, and more, are valuable insights for anyone seeking to know more about the library usage and perceptions of college students and young people.  This subset of the original Perceptions report is appropriate for provosts, deans and academic library administration. Read the report online or order a print copy using the links at the right, then use our feedback form to tell us what you think. I’m eager to print this puppy out and read through it.Â  It may be eye-opening to read about what users actually think rather than the typicallyÂ confidentÂ proclamations aboutÂ users needs in the off-the-cuff, hand-wavy,Â evidence-bankrupt way thatÂ many (most?)Â tend toward.Â  I’m as guilty as the next guy or gal, but that doesn’t make it any more acceptable.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/05/31/oclc-report-on-students-perceptions-of-libraries/",
        "teaser": null
      },{
        "title": "All Things Must Come To An End, or, Shameless Sentimentality",
        "excerpt":"One such thing, a just recently ended venture, is my bachelorhood.  I’ve been away for much of the past couple weeks preparing for, celebrating, and recovering from Elizabeth’s and my wedding on June 11th.   And because sometimes jabbering about library technology, and thinking about the many and challenging issues facing our profession, can just plum tucker a guy out, I’m posting tonight to share a photograph my beloved mother took of Elizabeth and myself on our 0th anniversary.  There’ll be plenty of time in the future to spend on the aforementioned jabbering and thinking; for now, I’m just enjoying life.      (And some folks thought a neckbeard couldn’t go with a three-piece suit.)  ","categories": [],
        "tags": [],
        "url": "/blog/2006/06/20/all-things-must-come-to-an-end-or-shameless-sentimentality/",
        "teaser": null
      },{
        "title": "unAPI 1.0 Compliance and a belated haiku!",
        "excerpt":"The page for the unAPI WordPressÂ plug-in has moved to the following location: https://mike.giarlo.name/blog/unapi-wordpress-plug-in/   Technosophia is now compliant with the brandy new unAPI version 1.0 specificationÂ (which bears an uncanny resemblance to revision 3). I’ve also added support for the MARCXML and SRW_DC formats to the WordPress plug-in, and updated Xiaoming’s unAPI Wiki page on existing formats.   Download the unAPI-1.0Â plug-in for WordPressÂ   I’d welcome any feedback on the goodness of my various formats. A cataloger I am not, so there are bound to be mess-upsÂ with my *_DC, MODS, and MARCXML records. Especially the MARCXML, eugh.  unAPI belâ€“ lows,Â \"veni, vidi, vici,\" most triumphantly! I never claimed to be a poet.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/06/26/unapi-10-compliance-and-a-belated-haiku/",
        "teaser": null
      },{
        "title": "Home of unAPI WP plug-in",
        "excerpt":"The WordPress plug-in for unAPI now lives here: https://mike.giarlo.name/blog/unapi-wordpress-plug-in/  ","categories": [],
        "tags": [],
        "url": "/blog/2006/06/28/home-of-unapi-wp-plug-in/",
        "teaser": null
      },{
        "title": "Screenscraped RSS Feeds",
        "excerpt":"I confess; I’m a fan of graphic novels, many of which are published by DC imprints Vertigo and WildStorm.  Wanting to keep up with the newest titles without having to make weekly stops to the local comic shop – which is a nifty shop, but why get up off your butt to do something if you don’t have to? – I went a-searchin’ for some RSS feeds and found naught.  There may be some out there, and I’d be happy to hear of them.   In the meantime, however, I’ve thrown together a quick and dirty screen-scraper (36 lines of PHP, partly inspired by Screen Scraping Your Way Into RSS) that grabs an appropriate “new comics” page on the DC website, and generates an RSS feed from relevant discovered information.  To keep abreast of new Vertigo or WildStorm comics, subscribe to the following feeds (which should be updated on a monthly basis):  http://feeds.feedburner.com/WildstormComics (Sub with Bloglines) http://feeds.feedburner.com/VertigoComics (Sub with Bloglines) Enjoy, and spread the goodness if you wish.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/07/12/screenscraped-rss-feeds/",
        "teaser": null
      },{
        "title": "Innovation v. Sustainability",
        "excerpt":"As a community of librarians and technologists, we like to talk about innovation.Â Â A lot:Â We like it; we support it; we do it; we reward it; we expect it; we value it; and weÂ need it.Â   The obvious and crucial question to my mind is how an organization might support innovationÂ without sacrificing sustainability, or alternatively value sustainability without hampering innovation.Â   This harks back to an earlier discussion at MPOW about using native XML databases as a storage back-end for XML web services - technologically, it’s a trivial decision, but how many different databases are we supposed to support?Â  The same question can be asked for programming languages, metadata schemas, operating systems, repositories, file formats, and so forth.Â   Here are my assumptions:   \tInnovationÂ requires flexibility and agnosticism so that developers and sysadmins can nimbly explore options without artificial limitations; \tSuch flexibility and agnosticismÂ are relatively easy to attain in development shops, but are untenable in production shops where sustainability isÂ of theÂ utmost concern (and a corollary: that needing to support an indefinite number of technologies andÂ standards at any given time is not sustainable); \tWithout harmony between production shops and development shops, seeing projects through their complete lifecycle becomes difficult, at best, and impossible at worst.Â  If the development shop wants to use innovative language X for its Project Of The Month and the production shop only supports languages A, B, and C, does the dev shop need to refactor or does the prod shop need to loosen its support requirements?Â  Alternatively, does the dev shop get fed up with the prod shop's policies and simply start housing their own production applications, despite the fact that they lack the resources to effectively manage them (per best practices)?  How then does an organization position itself both to support innovation and to ensure sustainability?Â  Change of personnel?Â  Policies?Â  Procedures?Â  Organization structure?Â  Or is it more of a cultural shift?   It’s one thing to talk innovation, but a whole ‘nother to actually do it.Â  And a third ‘nother to do it in a manner that’s sustainable, not to mention in accord withÂ all the otherÂ organizational values.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/07/13/innovation-v-sustainability/",
        "teaser": null
      },{
        "title": "Going to Princeton, code4lib++",
        "excerpt":"Though I will miss Seattle and my colleagues at the University of Washington, I am leaving to work at Princeton University Libraries as a digital library developer.Â  I’m looking very forward to helping design their repository infrastructure and building repository tools.Â  This is a fabulous opportunity to continue my work on digital libraries, and for my wife and I toÂ return home to the Garden State.   Still, itÂ will beÂ difficult to leave an area we’ve grown so fond of, and especially to leave the family and friends we have up here.Â  There’s a certain bittersweetnessÂ I experience with every move, and this is perhaps both the most bitter and the sweetest I’ve felt about a move yet;Â I can think of no other cities I’d rather liveÂ than Seattle.Â  ButÂ that ol’ Dorothy was right: there is no place like home.Â  And home, in this case, happens to be the place where our closest family members and most of our friends reside, and also an area with perhaps the greatest number of career opportunities for a digital librarian-up-and-comer like me.   I’d also like toÂ expressÂ appreciation forÂ the code4lib community, and especially the #code4lib IRC channel.Â  Without them, I likely would never have learned of this position.Â  I’ve been meaning to post about how code4lib has changed my thinking about the profession of librarianship; but suffice it to say that I feel indebted to this wonderfully resourceful and visionary community for broadening, to a great extent,Â my perspective on library technology.Â  I’ve benefitted greatly from their knowledge and diverse points of view, and have felt welcome in the community from the outset.Â   code4lib++  ","categories": [],
        "tags": [],
        "url": "/blog/2006/08/02/going-to-princeton-code4lib/",
        "teaser": null
      },{
        "title": "Durable URLs: Case in Point",
        "excerpt":"The title makes this post sound rather more interesting than it is.   In light of my recent announcement, I figured I would make sure that the three of you who are reading this – assuming my mother and wife are still following along – have bookmarked the durable URLs.Â  (Durable, at least, to the extent that they will live on after my 9/20 resignation date at UW.)Â  The persistent URL for Technosophia is http://purl.org/net/leftwing/blog.Â  The URL for Technosophia’s feed is http://feeds.feedburner.com/technosophia.   I’ll be moving Technosophia and my CV (which is of great and obviousÂ value to the internet tubes) to a new server in the coming weeks, hopefully with very little disruption (especially for those who’ve bookmarked the durable URLs).   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2006/08/11/durable-urls-case-in-point/",
        "teaser": null
      },{
        "title": "Object-oriented ontology: conceptual claptrap",
        "excerpt":"I have idly been considering the ontologies proposed by Gottlob Frege in The Thought, and revisited in Karl Popper’s Objective Knowledge, in which there are three distinct realms of existence.  Moreover, I wonder how envisioning such ontologies from an object-oriented perspective might provide a structured way of thinking about existence without need of dualistic theories.   A good question to raise is “what particular reason might you have for thinking that an object-oriented perspective is at all relevant here?”  I have no answer for that other than “if we can mash-up web services, why not theoretical frameworks?”  A glib and dorky answer, I admit, but I challenge the reader to name but one discipline where this sort of haphazard duct-taping of theory does not occur.  But I digress.   In both of these ontologies, the first realm is that of physical objects: things one can see, smell, taste, touch, or hear.  (This is an oversimplification, of course.  One risk is suggesting that subjective perceptions, such as hallucinations, are objective phenomena perceptible by others.  If I see a flying pink elephant and no one else does, is it “real?”  I also run the risk of hinting that imperceptible, physical entities do not exist.)   The second realm consists of mental entities, though Popper and Frege diverge at this point.  Whereas Popper thinks all non-physical entities live in the second realm, Frege confines the second realm to only those non-physical entities which consist in private or subjective ideas.  Take the example of the concept of pi and my belief that “beer is good.”  Popper would say that both of these entities belong in the second realm of his ontology: Frege would say that only the second is a second-realm entity.   The latter notion makes more sense in light of Frege’s third realm, that of objective, non-physical entities.  A good example is the Pythagorean Theorem which exists independently of the mind of Pythagoras; although Pythagoras discovered the Theorem, it was not a private or subjective idea of his invention; it was merely his expression of an otherwise objective property of the physical world (i.e., first realm), which anyone of a certain intellect might have discovered upon due thought and experimentation.   Popper lumps the Pythagorean Theorem, which Frege considers to be an objective entity, in with other subjective entities such as my idea that beer is a tasty beverage.  Popper sees the third realm as “the totality of all human thought embodied in human artefacts,” a realm of objective knowledge.  However, does he mean to suggest that a subjective, second-realm idea committed to a physical, first-realm object becomes something ontologically different upon their combination?  Do these entities warrant their own realm of existence?  Popper’s vision of third realm sounds more like an interface between the first and second realms than like a realm all its own.  An example of such an interface is human perception, which allows us to form second-realm ideas about first-realm entities by virtue of our innate sensory and cognitive abilities.   Frege’s and Popper’s views may be simplified by thinking about ontological realms in terms of object orientation as this perspective defines clear and structured relationships that are applicable to such thought.  Issues such as the nature and number of interfaces between the realms and the consumption and immensity of space in the realms, for instance, may be addressed with conceptual apparatus that already exist in the object-oriented perspective.  An ontology consisting of one object-oriented realm, rather than a number of realms, also has the benefit of emerging from the murky depths of dualism, for those who are inclined to reject dualistic theories.   An object-oriented ontology based on Frege’s conceptual divisions, summarized very briefly, would view: the physical realm as the main container object, that which contains all other objects, consisting of electro-magnetic radiation, matter, and physical forces; mental realms as distinct member objects of the physical realm, particular and distinct configurations of energy and matter resulting in concepts that are physically imperceptible; a “realm” of public, objective, non-physical entities as properties of first-realm objects, which are perceptible in the first realm and deducible (a la Pythagoras) in the second realm.   For a better thought out alternative object-oriented ontology, see Object Orientation Redefined.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/08/12/object-oriented-ontology-conceptual-claptrap/",
        "teaser": null
      },{
        "title": "Technosophia has moved",
        "excerpt":"Since my staff.washington.edu account will soon go away, I’veÂ movedÂ the site to my personal domain for the time being.Â  If you have bookmarked the old hard-coded URLs, you’ll need to change your bookmarks.Â  The persistent URL for Technosophia is http://purl.org/net/leftwing/blog, and the feed link is http://feeds.feedburner.com/technosophia.Â   It’s likely that I’ve missed some links and broken some others.Â  If you notice any problems, drop me a line.Â  Hopefully this shift will not affect my small andÂ strange audience.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/08/14/technosophia-has-moved/",
        "teaser": null
      },{
        "title": "Introducing unAPI",
        "excerpt":"What is unAPI?  Why should you care about it?   Read Introducing unAPI, in Ariadne issue 48, for answers to those questions… and more!  Here’s the obligatory snippet:  Common Web tools and techniques cannot easily manipulate library resources. While photo sharing, link logging, and Web logging sites make it easy to use and reuse content, barriers still exist that limit the reuse of library resources within new Web services.  To support the reuse of library information in Web 2.0-style services, we need to allow many types of applications to connect with our information resources more easily. One such connection is a universal method to copy any resource of interest. Because the copy-and-paste paradigm resonates with both users and Web developers, it makes sense that users should be able to copy items they see online and paste them into desktop applications or other Web applications. Recent developments proposed in weblogs and discussed at technical conferences suggest exactly this: extending the 'clipboard' copy-and-paste paradigm onto the Web. To fit this new, extended paradigm, we need to provide a uniform, simple method for copying rich digital objects out of any Web application. (Full disclosure: I helped a bit with this article.  Thanks to Dan et al. for giving me the chance to sully their otherwise well-thought out article.)  ","categories": [],
        "tags": [],
        "url": "/blog/2006/09/07/introducing-unapi/",
        "teaser": null
      },{
        "title": "Mission haiku, or, memetic conformity",
        "excerpt":"Public services: “Restrooms are over there, sir!” Cynical, ain’t I?   Repositories, institutional or not– we fill them with stuff.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/09/11/mission-haiku-or-memetic-conformity/",
        "teaser": null
      },{
        "title": "Why Fedora?  More answers to the Fedora users survey",
        "excerpt":"I noticed this response to the Fedora users survey on Peter Murray’s blog, and figured I’d post a response. Since my previous employer did not use Fedora, and I haven’t begun my new job yet, I’ll be posting about our use of Fedora at Rutgers, The State University of New Jersey.    How did you hear about Fedora? I heard about Fedora through Ron Jantz, Rutgers’ Digital Library Architect, in the summer of 2002. I was working at Rutgers then, actively seeking and tinkering with digital repository software. I installed the 0.9 version of Fedora in December of 2002, if memory serves.   Why did you choose Fedora? I’d already installed DSpace, Greenstone, and DLXS, and none of them seemed to suit our needs very well. Fedora was an easy choice to make for us since it seemed to be the only repository system that supported any metadata schemas we threw at it, which was a hard requirement, and also its ability to support behavior preservation via disseminators set it apart from the others. Additional advantages were that it exposed its methods through well-documented APIs, which other major repository systems still do not seem to do. In short, it was a clear decision.   The only drawback was the lack of an out-of-the-box interface, but we saw that as more of an opportunity to keep our back-end system separated from its interfaces, and our development team was eager for the chance to develop a customized set of interfaces.   Also, it’s free.   Were there economic advantages to your project/org. in selecting Fedora? I mentioned that Fedora is free, right? I know of no other economic advantages though perhaps one could argue that its inclusion in certain grants was beneficial.   What is Fedora’s unique role in your production system? Fedora serves as the back-end of RUcore: the digital institutional repository of Rutgers University, statewide digital initiatives such as the New Jersey Digital Highway, and electronic journals such as Pragmatic Case Studies in Psychotherapy. It is being used to preserve and provide access to metadata and data objects.   Is there one specific Fedora attribute that enables your project/organization to accomplish your overall goals. Flexibility with regard to metadata; extensibility of Fedora via its service framework; support for custom disseminators and behavior preservation; usage of the filesystem rather than complete dependence upon RDBMS; separation of repository methods and interfaces; and exposure of methods via web services APIs.   Do you see yourself as an active member of the Fedora community? And why? I see myself as an active consumer in the Fedora community. I haven’t worked with Fedora in over a year due to a job change, so I haven’t contributed anything in a while. I hope to remedy this in my new position, though I’m not yet convinced that Fedora is the right solution for them.   What would inspire you to become more involved? An employer that has committed to Fedora.   What should be the mission of an ongoing Fedora organization?  Continuing support and upgrades, outreach to Fedora users, and advocacy to ensure that folks know what the benefits of Fedora are.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/09/25/why-fedora-more-answers-to-the-fedora-users-survey/",
        "teaser": null
      },{
        "title": "Feed for First Monday",
        "excerpt":"I couldn’t find a good feed for First Monday, so I scraped one together:  http://feeds.feedburner.com/FirstMonday (Subscribe with Bloglines) Let me know if you notice any problems with the feed (other than character encoding issues).  ","categories": [],
        "tags": [],
        "url": "/blog/2006/09/30/feed-for-first-monday/",
        "teaser": null
      },{
        "title": "Access 2006 - Day One",
        "excerpt":"Day one of the Access 2006 conference is winding down.  Two groups of hackers (by trade or in spirit) gathered earlier today at Hackfest and Ad Hockfest, determined to work on a select few of the excellent project suggestions.  The experience was entirely new to me, but not uncomfortably so.  I was in a group (with pbinkley, ksclarke, tholbroo, and BigD) that worked on a Scriptaculous- and Cocoon-powered METS editor, a project similar to one I may be working on at Princeton.  The most rewarding aspect, though, was meeting fellow library technologists and hearing about the interesting, and sometimes interestingly uninteresting :), projects they’re working on.   Despite that, today was a bit of a bummer for me.  My wife’s and my recent move from Seattle to New Jersey was exhausting, and I was not looking forward to spending a potential day of rest, between relocation and starting my new job, sitting on a bus for twelve hours.  I woke up with a painful headache that wouldn’t go away, which rendered me basically useless until Hackfest was over and I took some pills back in my hotel room. My laptop was behaving badly when the day began, and I spent an hour or two trying to figure out why we Windows users couldn’t connect to the wireless network.  My single-minded persistence paid off when I finally got it working, but in retrospect I probably could have spent that time more wisely (and just jacked in to the ethernet).  Anyhoo, enough belly-aching for now.   I wound up skipping the opening reception tonight because of how crappy I felt, and instead found a cool Irish pub – the Aulde Dubliner Pour House – a couple blocks away from my hotel.  It was exactly how I would have pictured an Irish pub in Canada.  Hockey was on every television; the Tragically Hip was playing loudly; and folks were quick to offer a friendly “Cheers!”  It was quite relaxing, thanks in some part to the Kilkenny Cream Ale.   All in all, a good beginning to the conference in spite of my malaise.  I’m looking forward to reading other folks’ accounts of their Access 2006 experiences over at Planet Access.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/10/11/access-2006-day-one/",
        "teaser": null
      },{
        "title": "Access 2006 - ... and then what?",
        "excerpt":"I assure you that Access 2006 was longer than one day, I really do.Â  My (!*#@$ laptop had the nerve to go and die on me the second night of the conference, after I’d resolved to take copious notes during each talk, and had mostly kept my promise.Â  I’ll be posting some more notes when I get the machine fixed up, assuming my memory’s still relatively fresh (and my damn hard drive lives through the surgery).   Normally I might just give up and say “why bother?”, since the conference will likely be old news by then.Â  Then again, this is the Access conference we’re talking about; it’s not likely to be a passÃ© topic anytime soon.Â  For a different sort of account of how Access stays with a person, read Dan Chudnov’s post.Â   In the meantime, there is photographic evidence that I was at the conference (or at least out drinking in Ottawa around the same time).   More to come…  ","categories": [],
        "tags": [],
        "url": "/blog/2006/10/20/access-2006-and-then-what/",
        "teaser": null
      },{
        "title": "RSS Feed for RLG DigiNews",
        "excerpt":"http://feeds.feedburner.com/RlgDiginews The latest in my series of scraped-together feeds: RLG DigiNews  ","categories": [],
        "tags": [],
        "url": "/blog/2006/11/01/yasf-yet-another-scraped-feed/",
        "teaser": null
      },{
        "title": "Access 2006 Presentations",
        "excerpt":"Presentations from the Access 2006 conference have been posted.Â  Slides and audio are provided for many of the talks.Â  Check them out if you were not able to make it this year.   My laptop’s back like Lazarus thanks to the healing touch of HP customer service.Â  I hope to find some time soon to type up the rest of my notes from the conference, hopefully before Access 2007*Â rolls around.      To be hosted in Victoria, BC,Â a most charming city in my favorite corner of the world.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/11/03/access-2006-presentations/",
        "teaser": null
      },{
        "title": "Marxist ideals and NJLA 2007",
        "excerpt":"The magnanimous folks who are planning the 2007 NJLA conference have invited little ol’ me to give a presentation on unAPI in April.  I’m excited about the opportunity to evangelize about all of the interesting work and ideas out there in LibraryLand and beyond, and I have subsequently, and humbly, requested to expand the scope of my talk.  Fortunately, they have agreed, and I’m sharing the title and abstract here to gather impressions.   Thanks especially to Brian Hancock for encouraging me to present at TAG two months ago, to Mary Mallery for the invitation, and to Dan Chudnov for inspiring the lion’s share of the ideas I intend to ramble about.  A library revolution: Returning the means of production via service discovery, systems integration, and open standards  Imagine, if you will, a world where library services are automatically discovered; Library users retrieve information objects and metadata with a single click, never having to navigate the dark alleys of dead-ends that are full-text resolvers; Information sources and services are connected and remixed according to user preferences and needs, where and when they wish. What if we could leverage existing library and industry standards, applications, and protocols to make this a  reality?  And soon?  Technologies such as OpenURL, COinS, unAPI, OpenSearch, and DNS-SD are explained and their promise is examined in this context, alongside ideas such as service registries, auto-discovery, and integration of search and resolve systems.  Such evolutionary steps paired with bold, forward-thinking direction and a commitment to innovation may indeed lead us to this \"revolutionary\" scenario. ","categories": [],
        "tags": [],
        "url": "/blog/2006/12/03/marxist-ideals-and-njla-2007/",
        "teaser": null
      },{
        "title": "Fedora marches forward",
        "excerpt":"I was pleased to see the note that Sandy Payette sent to the fedora-users mailing list earlier today, updating the community on the Fedora 2.2 release date.  Version 2.2 is going to include a bunch of features, some of which have been long-awaited and are quite, well, sexy.  Some of the highlights:   \tDatabase support has been extended to include Postgres, which should make all the MySQL-haters happy \tFedora may now be deployed via a .war file in an existing servlet container, such as Tomcat, rather than requiring its very own Tomcat server \tA Lucene- or Zebra-backed search service has been included, which is more robust than the previous search service that used the built-in Dublin Core-populated database  These are but a few of the enhancements, and I can’t wait to put it through its paces when it’s released on January 19th.   For a more complete set of feature enhancements, click on the link above to read Sandy’s message.   Now if we can come together as a community and work on some more UIs, and get them used in some high-profile projects, many of the gripes against Fedora may be silenced.  It’s still not a perfect product, but what is?   That it uses XML as a storage format and exposes its functions via web-services APIs and allows use of any metadata schema, in my humble opinion, puts it head and shoulders above many other library repository solutions.  And for that, it’s at least worth consideration.  ","categories": [],
        "tags": [],
        "url": "/blog/2006/12/22/fedora-marches-forward/",
        "teaser": null
      },{
        "title": "Finally, unAPI Server for WordPress 1.0",
        "excerpt":"I’ve finally gotten around to updating the unAPI plugin for WordPress so that it fits into the WordPress plugin architecture, making it simple to install and maintain. I’m calling it version 1.0 since it’s the first substantial release of the plugin since I got involved.  Just unzip that sucker (or check the code out) to wp-content/plugins/unapi and do the rest via the administration pages, including activation and identifier configuration.   I updated the plugin page and imported it into the svn repo hosted at wp-plugins.org.   This was my first WP plugin, so I would appreciate any feedback.  All you folks that are using it should try stripping out the old version (which requires hacking some theme files) and plugging this in.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/03/finally-unapi-server-for-wordpress-10/",
        "teaser": null
      },{
        "title": "Camp for NJ Library Geeks?",
        "excerpt":"I just noticed Ed Summers posted a link to DemoCampDC1, a local BarCamp being organized in the Washington D.C. area, “to build an active community for people in the DC area to show up and informally share geeky stuff.”  I’ve heard of these BarCamps before, but I never really took the time to look into it.  Incidentally, I’ve been looking for good models for hosting and organizing such a group for a while now, inspired by the work Brian Hancock has done over the years with the Technology Awareness Group (TAG).  The way the D.C. group is going about it looks like a promising one: start out building a community with very informal meetings outside of work hours, to test the waters, so to speak, and build a rapport with interested parties.  And if there’s sufficient interest, hold a full BarCamp event (perhaps a one-day hands-on symposium / workshop).   I like this model quite a bit, and I’d be interested in trying to get something similar started up in New Jersey.  We have tons of talented library geeks in our many academic, public, and special libraries, not to mention general geeks whose interests and skill sets intersect with ours.   Why BarCamp?  Don’t we have enough meetings and symposiums?  Sure, we’re all spread thin.  Our niche would be to bring together the TAG-style dog &amp; pony shows with code sprints and other collaborative development a la the Access Hackfest.  We’re looking for a group that will be innovative, collegial, social, practical, and - not to be corny, but - fun-loving.   Here’s what I envision:   \tA number of very informal get-togethers spread throughout the state in order to foster inclusivity, with a \"point person\" in each area to report back about the level of interest \tRotating event venues, possibly one per \"point person\" above who would ideally receive institutional support in one form or another \tPartnerships with other in-state groups to take advantage of cross-pollination without crossing purposes, such as TAG, NJLA's IT section, NJ-ASIS&amp;T, and some student groups from Rutgers SCILS  Any of you New Jerseyans interested?  Think it’s a waste of time?  Drop a comment or trackback here, or send me e-mail.  I’m very interested in your feedback.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/05/barcamp-for-nj-library-geeks/",
        "teaser": null
      },{
        "title": "Five things you didn't know and are now worse off for hearing",
        "excerpt":"The “five things” virus has been going around a bunch during the past month or two.Â  While I wasn’t specifically tagged by anyone, I’m foisting my own five things upon the biblioblogosphere (and, oh,Â how I loathe that word).Â  I’m not aÂ terribly interesting person, so it was a struggle to come up with five things.Â  I tend not to share muchÂ personal information here, but I’d like to break that habit for now, since this approach is at loggerheads with my relatively open-bookish nature.Â  Disclaimers aside, voici!   \tAfter finishing up my bachelor's degree, and feeling oh-so-special for being aÂ Henry Rutgers ScholarÂ and a fairly successful slacker student, I moved to rural Oregon and spent the next nine months living in a barn.Â  (I'd realized that a degree in linguistics and philosophy was less than marketable.)Â  It marked the first time I lived beyond the boundaries of New Brunswick, NJ, and sparked my love affair with the Pacific Northwest.Â  After living in \"the barn,\" which towards the end of my stay became infested with some very nasty insects -- more details for those who inquire within -- I moved back to New Brunswick for seven years, and then wound up heading back to Seattle for a year.Â  And now I'm back east, yet again, living in Bucks Cty., PA. \tIÂ was diagnosed with anterior hypopituitarism as a child, which incidentally is the same conditionÂ Gary Coleman has.Â  Yes, Gary \"whaddyoo talkin' 'bout, Willis?\" Coleman.Â  I was fortunate to have a very good pediatrician who surmised very early on that my short stature might be due to such a condition, and I started taking growth hormone (and various other hormones) aroundÂ 1st grade.Â  While I'm still a bit on the short side, I would never have grown an inch above four feet tall had I not been treated.Â  Treatment involves a daily injection of growth hormone, which I still take as an adult.Â  The doctors aren't sure why I have this condition, but my father's best guess is that it's due to Agent Orange poisoning -- he served two tours as an infantryman and helicopter gunner in Vietnam. \tI am somewhat at risk for Creutzfeldt-Jakob Disease, having been treated for a few years with growth hormone that was harvested from cadaver brains, rather than synthesized which it has been since 1985 or so.Â  So far, only folks who were treated before 1978 have come down with CJD, but it can take upwards of 35 years before symptoms develop.Â  Or so says the Department of HHS report.Â  I'm probably safe, but it's something my wife and I will have to keep an eye on for a while yet. \tI met my wife on Hot or Not.Â  (I'm pretty sure I was the \"Not.\")Â  I paid $6 to be able to e-mail her, and it was the best $6 I ever spent. \tI used to be aÂ LARPer.Â  If you're curious --Â and who wouldn't be? --Â my characters were a barbarian fighter, a highlander ranger, an elven wizard, and an ascetic monk.Â  Yes, I have the costumes to prove it.Â  No, I won't show you pictures.Â  (Unless you piss me off.)  Aren’t you glad you read along?   Â  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/11/five-things-you-didnt-know-and-are-now-worse-off-for-hearing/",
        "teaser": null
      },{
        "title": "New theme",
        "excerpt":"New year, new theme. Technosophia is now comin’ atcha in day-glo green.   Drop me a comment if you notice any problems.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/14/new-theme/",
        "teaser": null
      },{
        "title": "Useful Trac reports?",
        "excerpt":"I had to create some Trac reports a while back, and figured I would share them with the world wide (time)waste.   The first selects all completed milestones:   SELECT name,     date(completed, 'unixepoch') as Completed,     date(due, 'unixepoch') as Due,     description FROM milestone WHERE completed &gt; 1 ORDER BY completed DESC  And the second lists all of your closed tickets:   SELECT p.value AS __color__,     (CASE status         WHEN 'closed' THEN 'color: #777; background: #ddd; border-color: #ccc;'         ELSE             (CASE owner WHEN '$USER' THEN 'font-weight: bold' END)     END) AS __style__,     id AS ticket, summary, component, version, milestone,     t.type AS type, severity, priority, time AS created,     changetime AS _changetime, description AS _description,     reporter AS _reporter FROM ticket t, enum p WHERE status IN ('closed') AND p.name = t.priority AND p.type = 'priority' AND owner = '$USER' ORDER BY (status = 'assigned') DESC, p.value, milestone, severity, time  They were pretty easy to whip up based on the other reports, but I figured I might save someone else a few minutes by sharing.  For all that I’ve plagiarized borrowed from the web, it’s time to pay that karma down a bit.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/16/useful-trac-reports/",
        "teaser": null
      },{
        "title": "Persistent URL Tools",
        "excerpt":"I’ve posted a couple new tools during the past couple days.  One is an update of Devon Smith’s LinkPURL extension for Firefox 2.0.   The other is an ultra-lightweight Wordpress plugin that embeds a linkpurl link tag for auto-discovery (so bookmarking agents can detect and grab the persistent URL rather than the impersistent URL up in the addressbar).   Based on a discussion in #code4lib earlier today, I realize that there are a lot of important questions, not to mention serious doubts, about persistent identifiers.  I flip-flop on their utility myself, so I found the discussion very useful.  (Thanks, edsu!)  Maybe I’ll write a post or two about persistent identifiers to flesh my thoughts out.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/17/persistent-url-tools/",
        "teaser": null
      },{
        "title": "OpenID plug-in for WordPress",
        "excerpt":"Sam Ruby posted a while back on how to embed LINK tags in your blogs (or other web resources) in order to enable OpenID auto-discovery.   Here’s a plug-in for WordPress that lets you accomplish this lickety-split.   Oh yeah: my OpenID is http://mjgiarlo.myopenid.com/.  Envy my creativity!  ","categories": [],
        "tags": [],
        "url": "/blog/2007/01/31/openid-plug-in-for-wordpress/",
        "teaser": null
      },{
        "title": "Princeton, meet Google",
        "excerpt":"Google and Princeton University went public twenty minutes ago in announcing their arrangement to digitize roughly one million of Princeton’s public domain works as part of the ongoing Google Books initiative.  Princeton is one of a growing number of academic institutions in the United States – including private institutions like Harvard and Stanford and state universities in Virginia, Texas, and California – that have worked with Google on this.  For more information, see the post on the Google Books blog or the official announcement.   Many of the details need to be worked out yet, but I am hopeful this will bode well for our users here at Princeton and for the broader community.   The need for digital preservation just grows and grows.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/02/05/princeton-meet-google/",
        "teaser": null
      },{
        "title": "Do what now?",
        "excerpt":"I usually travel by ground-based transportation – train when I can, bus when I must – because I hate flying.  There is something about this sort of travel that makes folks more sociable in my experience.  Frequently a question that I have come to dread is asked: “So, what do you do?”  Mumbling about libraries and technology is typically enough to bring about the initial stages of Ophthalmus Vitriatus: yes, the dreaded eye glaze (in faux Latin for pretentiousness points).  The best reaction I can hope for is “oh, yeah, seems like a really useful job nowadays what with all that information going online.”  Not a bad reaction, all in all.  True for sure.  But I wish I could succinctly express what I do without sounding overly general (“I’m a programmer and librarian”), or grandiose (“I am attempting to organize, disseminate, and preserve all the information in the world”).   I’m sure this is a problem with my own attempts at expression rather than a problem with our vocation.  So I’m putting this question out there:&lt;blockquote&gt;Assume you’re talking to someone who knows almost nothing of libraries and technology, how do you explain, in a quick-and-appropriate-on-a-bus-or-train sort of way, what you do for a living?&lt;/blockquote&gt;I’m guessing that snappy Rails app I’ve been working on for management of persistent identifiers is not very interesting to these folks.   What brought this about was my recent trek down to the Code4Lib conference in Athens, GA, which I must say is beautiful this time of year.  Now if I can only convince myself that there won’t be snow to shovel at home, maybe I’ll work up the courage to step back on the train.  By any road, I’m really looking forward to the next three days.  The conference program looks awesome, and last year’s really set the bar high.   Oh yes, Anjanette Young (Systems Librarian at the University of Washington) and I found a cafe/pub in the College Ave. / Broad St. area that has happy hour from 4pm-9pm.  This may very well be the death of me.   The title of this post has been brought to you by the Society for Meatwad Quotations™.  Those of you who get the reference may be mildly amused.  Those who don’t, clearly lead sad and empty lives.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/02/27/do-what-now/",
        "teaser": null
      },{
        "title": "Will Libraries Smell Like Teen Spirit?",
        "excerpt":"I followed a series of links[1] to find this article on the effect of Generation X values upon work culture.  The article cites the impending wave of Baby Boomer retirements, pointing out that a number of executive and upper management positions will open up and likely be filled by Generation Xers.  Will workplace culture change when GenXers take the reins?  If so, to what extent?  And how will it affect libraries which, unlike the corporate world, are saturnine[2]?  If organizational culture is in part derived from the values and culture of those who are at the helm, it is bound to change when Generation X takes over.  Baby Boomers are described as follows:&lt;blockquote&gt;“Leadership for them has been characterized by workaholic tendencies and materialism. Baby Boomers have had a minimum number of careers or a single career path, are impressed by authority, are optimistic and are driven to achieve.”&lt;/blockquote&gt;GenXers, on the other hand,&lt;blockquote&gt;“…question authority, seek bigger meaning in life and work, are technologically savvy, live in the present, are skeptical, see career as a key to happiness, are open to multi-careers, consider challenge and variety as being more important than job security and constantly aim to achieve work-life balance.”&lt;/blockquote&gt;An obvious criticism of the differences between the generations is that they are only generalizations and that they apply only to those who fit the stereotypes.  However, it does seem naÃ¯ve to assume that none of the generalizations apply, that cultures and skill sets do not vary generationally; stereotypes are rooted in the truth.   The article links to a study showing that&lt;blockquote&gt;“Baby Boomers received higher ratings from managers in 10 out of 18 competencies, particularly in their ability to coach and develop people and to manage execution. Generation X managers received higher ratings in self-development, work commitment and analyzing issues”&lt;/blockquote&gt;GenXers will need to work on their ability to mentor and on expanding upon their analysis skills with the ability to synthesize as well; it is important to see both the forest and the trees.  Whereas Baby Boomers are more comfortable in standard hierarchies, GenXers take more of a team-building approach, valuing independence and creativity to get the job done, whatever it may be.  GenXers are also reported to thrive on change which is a key attribute in dealing successfully with disruptive technologies and technological discontinuities, clearly an area where improvement is needed.   As a GenXer, I am cynical about the likelihood of my generation’s general disdain for bureacratic structure having any noticeable effect upon the bureaucracy, so common in academia, that seems to cripple decision-making processes and any semblance of agility.  I would love to see a new crop of administrators and managers come in and abolish the old ways of “analysis paralysis.”  Why not take a heuristic-based approach to management?   Instead of discussing something ad nauseum in three months’ worth of meetings, why not try it out as a pilot project?   Still, I am hopeful.  If the coming wave of retirements brings in a batch of administrators that value agility, tear down process-tangling walls of bureaucracy, encourage and reward creativity, embrace innovation and skunk-works as being essential to our mission, and generally stir the pot in some productive fashion, the future holds great things.   Besides, I can’t wait for flannel shirts and wool hats to become de rigueur for Academic Library Fashion 2.0.  (Or will the meme have evolved to 3.0 by that time?)   –   1. I’ve been following Mark Leggott’s Slow Library blog the past few months.  Read the “Ahhh…the Beauty of Slow” post for an explanation of what the movement entails.  The latest post contains a link to the Slow Leadership blog, which is where I found the Generation X article.   2. This is a base generalization; I admit that libraries might even be more agile than the corporate world in some areas, but do tend to believe that we are slow-moving, change-resistant beasts in general.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/03/12/will-libraries-smell-like-teen-spirit/",
        "teaser": null
      },{
        "title": "Digital librarians: Need a J.O.B.?",
        "excerpt":"Peter Binkley wrote a while back about the crop of neat digital librarian-y jobs that’d been popping up.  There’ve been a bunch more lately:            Head, Library Technology (Oregon State University)  \tCoordinator for Digital Library and Metadata Services (University of Colorado)  \tDigital Collections Coordinator (U. of Oregon)   \tDigital Projects and Catalog Management Librarian (U. of Oregon)   \tDigital Initiatives Coordinator (Clemson U.)    \tDigital Library Initiatives Manager (Temple U.) [Direct link could not be obtained]   \tDigital Library Program Manager (UC-San Diego)     It’s great to see academic libraries diving head-first into digital collections / library initiatives, and doing so with dedicated staff.   Some of these positions look like great options for folks like myself who’ve been bouncing around the digital libraries world for a few years now and are starting to think about taking on greater responsibility within an organization.  I know you’re out there, folks.  Consider applying!  ","categories": [],
        "tags": [],
        "url": "/blog/2007/03/14/digital-librarians-need-a-job/",
        "teaser": null
      },{
        "title": "L'informatique est morte.  Vive l'informatique!",
        "excerpt":"Is the discipline of computer science on its last legs?  Neil McBride, a lecturer in the School of Computing at De Montfort University, advocates for great change in The death of computing.  Citing greatly reduced CS enrollment figures in the UK, US, and Australia, and the growing disconnect between industry and academia, McBride argues that computer scientists need to reform the field from within or risk further marginalization and ultimate irrelevance.  Though I can sympathize with his desire to revitalize the discipline, and I understand how perception of computer science might have suffered from the dot-com bust, his message amounts to more than mere doomsaying and pointless nostalgia.   If it is true that computer scientists “look to games programming for [their] salvation,” there is a great opportunity being missed.  There are other options – exciting, pragmatic, and revolutionary options – computer scientists might investigate if they believe a wholesale rededication of their skills is needed for the betterment of their field.  (To be sure, some already have begun this great work.)  As a former student of information science and computational linguistics, I’m here as an interested observer to say that your skills are needed if we are to accomplish some of our loftiest goals.  I humbly submit the following areas that could use your help:    \tInformation retrieval: Build smarter, faster algorithms for finding and organizing information.  Instead of building a better bubble sort, figure out better ways to access and relate disparate bits of information.  The Google guys made a couple bucks at this; why not cite their success, and point at the meteoric rise of Google, as evidence of the continuing and growing sexiness of computer science? \tSemantic web: Bring your knowledge to bear upon the growing semantic web discussion.  If you could think up distributed computing, perhaps the challenge of distributed networks of semantically encoded data is ready for your insight. \tNatural language processing: Be the Google-killer by being the first to market with a usable natural language search tool.  There is much research in NLP, but very little of it seems ready for end-users.  Help make the keyword a thing of the past.  Computational linguists would love to cultivate interdisciplinary connections with you folks.   Although McBride’s article may fade into the background of the very frequent, if strident, cries that CS is dead, I am hopeful that interdisciplinary ties between computer science, information science, library studies, and linguistics will bring about practical innovation, not to mention a renewed sense of relevance and excitement for computer scientists.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/03/19/linformatique-est-morte-vive-linformatique/",
        "teaser": null
      },{
        "title": "Five extrabiblioblogospheric blogs",
        "excerpt":"A number of folks have responded to the Liminal Librarian’s original meme asking for a sampling of five non-library blogs folks read.   Here are some of mine:    \tdmiessler's \"grep understanding knowledge\" - He writes about society, programming, UNIX administration, and his personal life.  I enjoy both his writing style and the diversity of topics he covers.  His recent piece about the passing of his grandfather resonated strongly with me, as I've been struggling with having lost my own, one and only, grandfather in December. \tSlow Leadership - Gosh, this is sort of a dirty secret, but I am ridiculously interested in management.  It mystifies me, and it's a skill I'd like to gain and hone someday.  Effective management and leadership are tasks I never expected to have an interest in, but I've been drawn more and more to them the past few years.  Slow Leadership contains a number of insights that I have found quite useful in making sense of how (good) administrators do what they do, and I try to take some of their advice to heart. \tThe Rails Way - I'm a programmer by day, and most of my web application work is in the Rails framework.  The Rails Way is written by two Rails committers who know the conventions and good patterns inside and out.  Here's how it works: folks submit Rails applications they are working on, and these two rip them apart (in a very constructive and nice way), giving code samples along the way.  It's very instructive.  To wit, I've taken more notes on their suggestions than I can shake a stick at. \tThe Seattle Times - Okay, it's not really a blog, but I do read it in my aggregator.  And what can I say?  I can't let go.  I still feel like Seattle is my home. \tSlog (NSFW) - The Slog is the blog of Seattle's alternative weekly newspaper, The Stranger.  It's an odd combination of gutter humor, satire, social commentary, political rants, and philosophical bombast.  Where else can you read about deep-fried, beer-battered, bacon-wrapped, spray-cheese-filled hotdogs, analysis of the Alaskan Way viaduct vote, the recent Garrison Keillor flap, and the latest eating establishments in Capitol Hill (Seattle, not D.C.)?  ","categories": [],
        "tags": [],
        "url": "/blog/2007/03/20/five-extrabiblioblogospheric-blogs/",
        "teaser": null
      },{
        "title": "Greetings from NJLA!",
        "excerpt":"   I’m at the NJLA 2007 conference today talking about technology.  I’m using this post to demonstrate RSS, OpenSearch, unAPI, and auto-discovery.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/04/25/greetings-from-njla/",
        "teaser": null
      },{
        "title": "Library Camp NYC",
        "excerpt":"I kicked around the idea of having a Library Camp NJ a few months ago.  Response wasn’t fantastic but, then again, I didn’t try very hard to spread the word, and perhaps I am not the best champion for the cause.  In the meantime, Baruch College in Manhattan will be sponsoring Library Camp NYC this August.   I certainly plan on attending if I can get approval to take that day off and may even be talking about this or that if I can come up with an appropriate topic – Ruby on Rails might be too narrow or technical.  Those of you who are relatively local to NYC should read more about it and consider coming.   What is a \"Library Camp?\"  The first Library Camp was held at the Ann Arbor District Library in Ann Arbor, Michigan in April 2006 as an \"unconference\" to talk about opportunities and challenges regarding Library 2.0. It was organized by John Blyberg, and went over so well a Library Camp East was held in September 2006 at the Darien Public Library (CT) with about 50 attendees (organized by Alan Gray).  So what's an unconference? Unlike a traditional conference, where people have to pay registration fees to sit in an auditorium and listen to someone read off PowerPoint slides, there are no registration fees and everyone is a active participant. Whoever shows up are the right people for an unconference. The discussion topics are decided by the participants when they arrive, as well as topics that are brought to the forefront during discussions that are deemed as particularly important and are thought to be worthy enough by members of the discussion groups to have these topics break off into their own core discussion groups. The format has gained popularity in the technology field in recent years, because it promotes collaboration and engagement.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/15/library-camp-nyc/",
        "teaser": null
      },{
        "title": "A farewell to Falwell",
        "excerpt":"I decided a while back that I would use this space exclusively for library- and technology-related bits, that I should not clutter it with more personal or political matters.  That decision has probably saved me a lot of embarrassment.  But it’s also kept me from updating more often than I would like, and forced filters upon me that give a one-dimensional view of what I’m about.  Underlying the decision was a certain trepidation and strange sense of professional propriety – what would colleagues and potential employers think of me if I just spilled my guts out here?   I’m starting to question that decision.  Other well-known biblio-bloggers do it – I’m looking at Dorothea and Karen – though I’m in the league below the bush league compared to them. But sometimes, dammit, I just want to be me.   I’d like to add to the left-leaning echo chamber by reminding folks that Jerry Falwell was a lunatic.  People come out of the woodwork to spout ebullient praise about the recently deceased, whether they were actually good people or not, and the posthumous Falwell spin has begun with numerous smiley glad-hands lauding his life and deeds.  But let’s not forget what kind of guy ol’ Jerry was.  &lt;blockquote&gt;I really believe that the pagans, and the abortionists, and the feminists, and the gays and the lesbians who are actively trying to make that an alternative lifestyle, the ACLU, People For the American Way, all of them who have tried to secularize America. I point the finger in their face and say ‘you helped [the terrorist attacks on 9/11] happen’.&lt;/blockquote&gt;   I’m not so crass as to celebrate the death of someone who has arguably done some good during his long lifetime, but I couldn’t help but think “good riddance to bad rubbish” when I heard of his passing.   I am sure there will be dozens of doomsayers scrambling to take his place in the Pantheon of Evangelical Kooks.  I can see it now: And lo, I woke up this morning and my toilet was running.  SURELY THIS IS A SIGN OF THE SECOND COMING.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/15/a-farewell-to-falwell/",
        "teaser": null
      },{
        "title": "[0 of 10] Why Ruby on Rails?",
        "excerpt":"It would be an understatement to say I’ve been enthusiastic about Ruby on Rails for a while now.  Okay, I am downright fanatic – just look at that shrine to DHH in my closet; it’s plain to see that I’ve drunk the (ruby red) Kool-Aid.   This is the first post in a series in which I hope to share my enthusiasm and explain just what it is that I love about Ruby on Rails.  Not many folks in the library world seem to be using RoR – with a few notable exceptions – and so I thought it might be of interest to folks unfamiliar with or curious about Ruby and Rails to hear some impressions from a bright-eyed newbie.   Stay tuned, folks.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/21/0-of-10-why-ruby-on-rails/",
        "teaser": null
      },{
        "title": "Edumacation and a call for ILS abstraction",
        "excerpt":"First, I’m stoked to be getting copies of the following books over the course of the next few weeks:   \tInformation Architecture for the World Wide Web: Designing Large-Scale Web Sites \tAmbient Findability: What We Find Changes Who We Become \tA Semantic Web Primer (Cooperative Information Systems) \tRESTful Web Services  They reflect pretty clearly the sorts of things that have been on my mind of late.   Also, I saw a very interesting post on Peter Brantley’s blog, Shimenawa, and I thought it was important enough to share verbatim:   There is considerable ferment these days in the library community about the shortcomings of the current generation of OPAC systems. A number of libraries are investigating replacement discovery systems divorced from their Integrated Library Systems (ILS), and a few have already implemented alternatives. Replacing an integrated OPAC with an external generalized discovery system raises some difficulties, however, as OPACs provide functions beyond simple bibliographic discovery. Among the areas of difficulty are:      * Handling/displaying complex non-bibliographic library data, such as detailed serial holdings;     * Providing access to highly volatile ILS transaction data, such as circulation status;     * Supporting ILS-specific patron functions, such as renewing charged-out books;     * Supporting institution-specific functions, such as requesting the delivery of books from a remote storage facility.   The inability to provide such popular functions will inhibit the freedom of libraries to use alternative (frequently more modern) discovery platforms for their catalogs.  From the standpoint of libraries it would be ideal to be able to mix-and-match ILS and discovery platforms to suit local needs. To create such a rich environment the library and vendor community will need agreement on the specific technical details of how discovery and ILS systems are to integrate.  DLF is planning to establish a Task Group to analyze the issues involved in integrating ILS and discovery systems, and to create a technical proposal for how such integration should be accomplished. We are now seeking nominations of knowledgeable people to serve on the Task Group. Candidates should have analytic skills, be familiar with ILS and discovery systems, and, ideally, have some experience in systems interoperation. We hope to create a group that has a balance of experience with a variety of specific ILS and discovery systems. We encourage both self-nomination and the nomination of people from the community you believe can contribute to this important effort.  Because we believe the need to regularize interoperation is immediate, we are hoping to name this Task Group within the next several weeks, and that its work can be completed by the end of the calendar year. We expect most of the work of the group to be carried out through telephone and electronic means, although one or two face-to-face meetings may be needed. I have also asked a few experienced people to serve as advisors for the Task Group, to offer help and advice if needed.  Note that although this call went out originally to DLF members, this effort will not be limited to DLFies.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/23/edumacation-and-a-call-for-ils-abstraction/",
        "teaser": null
      },{
        "title": "Visionary visualization... from Microsoft?",
        "excerpt":"Maybe I’m late to the party – nods to John Blyberg and Rob Styles – but damn(!), does Microsoft have some exciting visualization projects or what?   John and Rob wrote about Microsoft Surface, a hardware/software combination that allows for tactile manipulation of data.  In Microsoft’s own words:   Microsoft Surface represents a fundamental change in the way we interact with digital content.  With Surface, we can actually grab data with our hands, and move information between objects with natural gestures and touch.  Surface features a 30-inch tabletop display whose unique abilities allow for several people to work independently or simultaneously. All with out using a mouse or a keyboard.  Don’t take my word for it; go watch a demonstration video and be amazed.   But wait, open the video in another window, and keep reading before you lose interest in my uninteresting prose.   I noticed among the recently released TED talks a brilliant short presentation by Microsoft’s Blaise Aguera y Arcas on their Photosynth project.  My first impression was just the visual joy of the photo browse / pan / zoom interface.  It’s impressive in its own right.  But what really tickles me about Photosynth is that it “takes a large collection of photos of a place or an object, analyzes them for similarities, and displays them in a reconstructed three-dimensional space.”   For example, throw it at http://www.flickr.com/photos/tags/seattlepubliclibrary/ and it will construct a collage-like view of the Seattle Public Library.  Zoom in and you’ll see images at that level of zoom.  Pan in three dimensions and (assuming there are enough photos to support the various views) you can virtually be in front of SPL.  See for yourself:    &lt;/param&gt;&lt;/param&gt;&lt;/param&gt;&lt;/param&gt;&lt;/param&gt;&lt;/param&gt;&lt;/embed&gt;&lt;/param&gt;  Imagine browsing the stacks with this thing.  Imagine finding a book on the stacks and being able to hook directly into its full-text.  Super cool!   Microsoft Surface is tentatively planned for a November release, which will be targeted for “retail and entertainment settings”.  It could be available to the public in a few years’ time though it will likely cost a few times more than an average PC. [Gleaned from a Seattle PI article.]  Photosynth is also not available yet, but you can track its progress on the Photosynth team blog.  (Yes, there’s a feed.)   [Disclosure: I lived in the shadow of Redmond not long ago, though I was not employed by Microsoft.  I probably invest in Microsoft indirectly, but I haven’t scoured my portfolio distributions in a while.]  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/30/visionary-visualization-from-microsoft/",
        "teaser": null
      },{
        "title": "Want to work at Princeton?",
        "excerpt":"I was stoked to see our Digital Initiatives Coordinator position posted earlier today.  We have been without a full-time field officer for nearly eight months, though Kevin Clarke (lead programmer) has served with distinction – and with nary a complaint!  (Well, okay, there may have been some complaints, but he always had a smile on his face.)   Here are some reasons you might consider this position:    \tA chance to get in on the ground floor -- our team is still undecided on many important issues, so you would have the opportunity to shape the organization. \tWork with a broad range of technologies, such as Java, Ruby, XQuery, XForms, native XML databases, Solr, and more. \tAssist in the ongoing effort to select repositories for various use cases  -- we're evaluating DSpace, several Fedora front-ends, and the X-Hive/DB native XML database \tMany rare and beautiful library collections. \tGreat location -- it hardly seems like New Jersey! \tLead the digital collections team, currently consisting of four librarian programmers and five digitization gurus.         Forge ties with one of the finest faculties in academia. \tComprehensive benefits package -- 24 days vacation, the whole insurance kit and kaboodle, and so on. \tUncommon amount of support (financial and otherwise) for professional development. \t... and you get to work with me!  But seriously, the great team we have assembled has been a big part of why I've thoroughly enjoyed my time at Princeton.   You can get a sense of the work we’ve been doing by browsing our Digital Collections, though that’s only the tip of the iceberg, and the site will be overhauled in the coming months.   Have some digital initiatives experience?  Ready for leadership responsibilities and an environment in which you will hone your vision?  Then by all means, check it out.  An MLS is not required.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/05/31/want-to-work-at-princeton/",
        "teaser": null
      },{
        "title": "Identifier Persistence: Fundamentals",
        "excerpt":"A friend and former colleague asked if I would comment on a chapter in her upcoming book on digital rights management and I agreed.  The chapter is about identification and authenticity of web resources.  Throughout my review of the chapter, I kept coming back to a couple of very basic notions that underlie any effort to provide persistent identifiers for web resources.  These notions are, to my mind, central to identifier persistence, and any other concerns rely upon this foundation:    \tIdentifier persistence requires an organizational commitment.  Persistence cannot be ensured by a few renegades in the skunk-works, nor can it be mandated from on high without the support of those who manage the identifiers or produce web resources.  All individuals involved in the life-cycle of web resources must be committed to persistence in perpetuity if true persistence of identifiers is to be achieved. \tNo technology, no standard, no identifier scheme, no information architecture will get you persistence.  Whether you choose native URIs, Handles, DOIs, PURLs, ARKs, UUIDs, or XRIs, you will never achieve identifier persistence without active management of your identifiers and web resources.  This requires the aforementioned organizational commitment since such management cannot occur without sufficient resources.  Management of web resources and identifiers requires time and due diligence and those don't come for free.   And, at the risk of being reductive, that’s about it.  Once you’ve got an organizational commitment and a person or team to manage your identifiers and web resources, the rest of the decisions are secondary.  If you like semantically meaningful URLs that redirect, choose Handles; if you prefer opaque identifiers, go with ARKs; if you don’t want to run your own software, consider PURLs.  At that point, it really doesn’t matter which scheme you choose, as long as its characteristics match your organization’s values.  You’ve already done the heavy lifting; rest easy.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/05/identifier-persistence-fundamentals/",
        "teaser": null
      },{
        "title": "NJLA 2007 Talk",
        "excerpt":"This is a slightly modified (read: rough) transcription of the talk I gave at this year’s NJLA conference, called “Library Revolution.”     The abstract described an idealistic scenario:  Imagine, if you will, a world where library services are automatically discovered; Library users retrieve information objects and metadata with a single click, never having to navigate the dark alleys of dead-ends that are full-text resolvers; Information sources and services are connected and remixed according to user preferences and needs, where and when they wish. What if we could leverage existing library and industry standards, applications, and protocols to make this a reality? And soon?  In this scenario, a potential library revolution could be fomented – in which the goal would be to return the means of production to users, to hand over the reins, to re-envision ourselves as tool- and service-building artisans, as Karen G. Schneider described in her keynote at the Code4Lib 2007 conference, rather than gatekeepers and information proxies – and I’m going to suggest some ways this might be achieved.  For now I’ll assume it’s self-evident why it is desirable to, generally speaking, get “our stuff” “out there” and meet users at their points of need.   Rather than get into nitty-gritty details, I’d like to describe a higher-level vision which has been put forward by a host of library technologists that have come before me (especially Daniel Chudnov).  Some aspects of my vision may indeed be pie-in-the-sky, but consider this:   \tIsn't pie delicious? \tShouldn't we reach for it?  So, what’s the problem?  Why a revolution?  Here are some (arguably trite) observations:   \tFull-text resolvers do not work well.  You should not have to click through two, three, or four windows to get at full-text -- assuming it's actually there and not a complete dead end!  Don't get me wrong -- it's better to have access to full-text through a resolver than not to.  I'd like to see more resolver systems that implement look-ahead resolution like Oregon State University's new, and freely available, metasearch tool, LibraryFind.  LF uses what Jeremy Frumkin, the Chair of Innovative Library Services at OSU, likes to call \"two-click workflow\": one click to find, one click to get. \tInformation splatter.  We've accumulated too many silos and need to figure out better ways to access all of that information via a single interface, whether the method is federation, aggregation, or something else.  Users should not have to go to multiple sites to search our collections for resources of interest. \tSandboxing.  Our content and services are, generally speaking, tightly coupled to our websites, so we are generally unable to meet users at their points of need. \tService usage -- reference desk visits, OPAC searches -- appears to be dwindling. \tGrowing popularity of Google, Amazon, and \"web 2.0\" or social networking sites -- del.icio.us, flickr, twitter, myspace, facebook, ning, librarything.  These sites are great -- especially MySpace, where I get all sorts of offers for new prescription drugs and live adult webcams.  But these sites really -are- great.  They empower users to connect with one another, to describe their own resources, to share with others, to remix information.  And most of all?  They're incredibly easy to use.  Are our tools as easy to use?  Are we similarly empowering users?  An aside on “2.0”: Although I cringe at the viral “2.0” meme – web 2.0, library 2.0, business 2.0, identity 2.0, enterprise 2.0, learning 2.0, travel 2.0… – it is interesting to note that there is something to “2.0”.  Something revolutionary.  And it’s not folksonomies, it’s not tagging, it’s not tag clouds, it’s not sharing, it’s not any particular site or idea.  It is the very fabric of “2.0”, and that is a re-envisioning of the web from connecting people with data to connecting people with people.  The web has evolved from a network of interlinked documents to an extension of the social fabric connecting us all.   As you can see, a revolution of sorts has already begun.  Time magazine selected “You” as their 2006 person of the year.  When MSNBC covered the story, the headline read “From blogs to YouTube, user-generated content transforms the Internet”.  I’m personally not that interested in the social networking aspect, and it already receives a lot of coverage from the library 2.0 gang.  Library 2.0 is a popular topic now, and much has been said of wikis, blogs, and RSS.  These are important topics but others are already covering them quite well.  The point to take away from 2.0, in my view, is that it’s empowering and inspiring users to do things with information they previously were not able or willing to do.  Ask tens of millions of people to help us catalog MARC records?  Right.  But ask them to tag videos on YouTube, bands on Last.FM, images on Flickr, links on del.icio.us, and so forth?  There you go.  My areas of interest with regard to library revolution are unifying our content and services, getting them outside the library sandbox, and returning the means of production in this very “2.0” way.   Let’s step through some technologies and technological concepts that may play a role in reaching this outcome.   \tSystems integration: We have accumulated a wealth of resources over the years and have purchased, or built, or licensed, numerous systems to access these resources that have traditionally been disparate.  This is a great accomplishment; the more information we can get into the hands of our users, the better.  The process doesn't scale, though, and has resulted in a proliferation of information silos.  Because of thorny issues of interoperability, not to mention licensing issues, technological incompatibilities, and lack of resources, we have thus far struggled to bridge the gaps between these silos.  The result?  A number of different search interfaces, with different result sets, in different formats, supporting different depths of coverage. &nbsp; &nbsp;How can we reconcile in our users' minds this information environment with the \"simple, single search box\" mentality of the Google age?   What if we built bridges between our systems?  Pull together metasearch with the link resolver, the link resolver with the catalog, the catalog with institutional repositories.  Easy, right?  Well, no.  But at the very least, if you can get XML out of these systems -- whether through OAI-PMH, or SRU, or a database export -- you can bring it together.  Index it with a tool like Solr, and you've got your Google-ish library search tool.  \tAuto-discovery: Auto-discovery is used by a number of technologies, though perhaps its usage to announce syndication (RSS) feeds is the most well-known.  The mechanism for syndication auto-discovery is actually quite simple.  Got a feed for your site?  Add a single line of HTML code to any page you'd like to announce it on, and modern web browsers will pick it up and clue you in. &nbsp; &nbsp;In HTML, there is a LINK tag, not to be confused with the A tag (which stands for anchor) commonly used for hyperlinks.  The anchor and link tags differ in the following ways:  \tAnchor tags may have text content and show up as labels for links.  For instance, you might link to FoxNews.com and label it \"Fair and balanced?  Yeah right.\"  LINK tags do not have text content. \tActionability: Anchor tags are clickable.  They take you someplace.  LINK tags are not clickable. \tContext: Anchor tags appear in the body of a document.  LINK tags appear in the HEAD. \tSemantics: Anchor tags may represent any number of things.  It might be a link to content further down in the current page, it might link to another page entirely, or it might even be used to activate some javascript or launch a popup window.  LINK tags are used solely to describe document relationships, more semantic information.  For instance, a LINK tag might describe a link to the next and previous chapters in an e-book, a LINK tag might be used to link to alternative representations of a document, such as versions in other languages, or versions formatted in RSS or the Atom syndication format.  The LINK tag is a great way to leverage the existing web architecture to handle the problem of \"one resource, many representations\", and I wouldn't be surprised if the OAI-Object Reuse and Exchange initiative took a hard look at it.  The LINK tag sort of auto-discovery, such as for syndication feeds, is common, but is not the only implementation of auto-discovery.  There are more sophisticated ways, such as Zero Configuration Networking. \tSyndication: You've probably heard a lot about RSS, or Really Simple Syndication, and I wouldn't be surprised if most of you are already using it.  It's a great technology, simple to use and implement, and I know it saves me a great deal of time on a daily basis.  Instead of having to click through and browse the 50 or so websites I track regularly, I read updated content from each site in my feed aggregator in a unified interface. A lot of attention is already paid to RSS, especially in library 2.0 circles, so I won't say much more about it.  Syndication allows content to be syndicated into feeds that folks can subscribe to and unsubscribe from willy-nilly. &nbsp; &nbsp;But I thought it was important to include an explicit mention of syndication since a couple of the other topics relate to it, and since it is a great example of getting stuff out there.  Rather than requiring your audience to come to your website, syndication enables them to read your content in an environment of their choosing.  It's worth noting that my wife is not a fan of syndication.  She likes the experience of going to different websites, enjoying their different takes on web design, and compartmentalizing her web surfing.  And that's great; no one, to the best of my knowledge, has advocated an \"RSS-only\" interface.  Content available in the RSS format is also available otherwise, so it is a convenient option for people like myself. &nbsp; &nbsp;One more point about RSS, despite saying I wouldn't talk much about it.  It's kind of an academic point, but I feel it warrants some clarification.  The term RSS has quickly become the Band-Aid, or the Kleenex, of syndication feeds.  RSS is one of a number of formats used for marking up syndication feeds.  Another is the Atom Syndication Format.  Most browser and feed aggregators are fully aware of both feed types -- for instance, Bloglines has supported both formats since June of 2006 -- and they generally should render the same, and that's why you don't hear about Atom much; it's a detail that is, for the most part, behind the scenes.  \tOpenSearch: Does anyone here have a website or a catalog?  Do they have search interfaces?  Perfect, you're about a third of the way there.  OpenSearch is a specification for some simple formats that allow you to share search results.  Just as syndication allows you to decouple your content from your website, OpenSearch allows you to decouple your search engines from your websites.  Here's how it works:  \tGo to your search page and look at the URL after you run a search \tWrite an OpenSearch description document \tEmbed a LINK tag linking to the OpenSearch description document, for auto-discovery \tReturn search results in RSS or Atom  You might ask \"why bother?\"  Firstly, the newest browsers -- FF2 and IE7, among others -- support auto-discovery of OpenSearch targets.  So folks can search Google, Wikipedia, Amazon, eBay ... and your websites or catalogs directly from their browser.  Secondly, it allows for fairly simple federation of searches across OpenSearch targets.  Since each target contains a description document that is machine-readable, I can point my OpenSearch client at a number of targets, find their descriptions, and learn how to search them.  Results are, by convention, returned in RSS or Atom, which are easily crosswalkable, so aggregating result sets is fairly trivial (though how to sort or rank them is tricky).  Thirdly, since results are returned as RSS or Atom, one can in effect subscribe to search results.  For example, you could subscribe to a search on Wikipedia for \"Anarcho-Syndicalism\", and your feed aggregator will be alerted whenever that search returns new results.  Or, a Linguistics professor could subscribe to your catalog's OpenSearch target, hoping to be alerted when new materials about Germanic syntax are cataloged -- and it's worth noting, this is as easy as just two or three clicks in a web browser. \tunAPI: Numerous tools and protocols exist for integrating library resources into other information systems, library or otherwise.  OAI-PMH and OpenURL are two great examples of successful and widely deployed technologies.  Unfortunately, few developers outside the relatively small world of library technology know anything about library standards, and this is seen as a significant integration barrier.  Dan Chudnov, a librarian programmer at LC, reflected on this: 'we librarians and those of us librarians who write standards tend, in writing our standards, to \"make complex things possible, and make simple things complex.\"' &nbsp; &nbsp;To address this issue, a number of librarians and technologists came together to develop a new standard called unAPI.  unAPI is a tiny web-based specification designed to solve the problem of identifying, copying, and pasting discrete content objects to and from web applications (including catalogs, bibliographic databases, repositories, link resolvers, and so forth), making it simpler for developers outside the library world to get at our vast intellectual resources.  The objective of unAPI, then, is to enable web sites with HTML interfaces to information-rich objects to simultaneously publish richly structured metadata for those objects, or those objects themselves, in a predictable and consistent way for machine processing. &nbsp; &nbsp;unAPI consists of three parts: A microformat for embedding object identifiers in HTML, an HTML LINK tag for unAPI service auto-discovery (as used for RSS, Atom, and OpenSearch), and a web service consisting of three functions--get formats , get formats for x identifier , get format y of identifier x -- two of which have a standardized response format, returning XML.  \tZeroConf: I want to acknowledge Dan Chudnov again, for suggesting that Zero Configuration Networking might have a place in library services.  The general question here is \"why can't library tools be as cool as iTunes is?\"  Just waltz into Starbucks, connect to the wi-fi, and you can see everyone else's playlist.  You can listen to their music, even.  What sort of magic made this sort of auto-discovery \"just work?\" &nbsp; &nbsp;The technology is called Zero Configuration Networking, or ZeroConf, though you may see older mentions under the names Rendezvous and Bonjour.  Without getting into the hairy details, ZeroConf is a small stack of fairly low-level technologies that piggy-back on the ubiquitous domain name system (or DNS), which enables us to type identifiers like \"google.com\" and \"nytimes.com\" into a web browser and rest assured that our computers will take care of the rest for us -- looking up the domain name, finding the network address of the server, connecting to the server on an appropriate port, and so forth.  ZeroConf allows machines to connect to networks without any knowledge of what's already on the network, without regard for the type of network topology or infrastructure, and both register the services it provides and query the services already provided by other nodes on the network.  It sounds complicated, but everytime you walk into Starbucks and start up iTunes, it seems pretty trivial.  It \"just works.\" &nbsp; &nbsp;Wouldn't it be great if users could discover our services and resources that easily?  What if we went through with systems integration and announced that unified service via ZeroConf?  A visiting scholar could enter our library, connect her machine to the network -- and let's forget about authentication and authorization for now -- and immediately find our new service, from which she could run a simple search against all of our bibliographic databases, all of our catalog records, all of our full-text holdings, and all of our repository objects.  What if library services \"just worked\"?Significant work needs to be done in this area before it becomes viable as I've just described, but I find it to be a compelling vision.  The technology as is widely acknowledged is the easy part.  So how do we get there?  How do we re-envision ourselves as library artisans?  How do we craft services that “just work?”  How can we investigate all these technologies that let us unleash our considerable assets?   \tCommitment to innovation.  If you can afford to have skunkworks in your organization, even if it's only one employee, or allowing a couple of creative individuals to devote 10% of their time to innovative pursuits, it's worth it. \tBold direction. \tThink outside the orgchart -- leverage collaborative development, forge communities, make the most of your consortial ties. \t... You tell me.  We can yield revolutionary results via small steps and a bold, forward-thinking direction.  Pie in the sky?  Maybe, but isn’t pie delicious?  (Yes, that was a glib and abrupt ending, but I’m tired of editing.)  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/05/njla-2007-talk/",
        "teaser": null
      },{
        "title": "Digital preservation for archivists",
        "excerpt":"At long last, the paper that Ron Jantz and I wrote for the Journal of Archival Organization has been published in a special double issue.  It’s titled “Digital Archiving and Preservation: Technologies and Processes for a Trusted Repository” and is intended to be a fairly nitty-gritty piece on digital preservation (in the context of trusted repositories) for archivists.  The abstract:&lt;blockquote&gt;This article examines what is implied by the term “trusted” in the phrase “trusted digital repositories.” Digital repositories should be able to preserve electronic materials for periods at least comparable to existing preservation methods. Our collective lack of experience with preserving digital objects and consensus about the reliability of our technological infrastructure raises questions about how we should proceed with digital-based preservation practices, an emerging role for academic libraries and archival institutions. This article reviews issues relating to building a trusted digital repository, highlighting some of the issues raised and possible solutions proposed by the authors in their work of implementing and acculturating a digital repository at Rutgers University Libraries.&lt;/blockquote&gt; This special double-issue of JAO will also be released in the manuscript, “Archives and the Digital Library.”   Thanks to editors Bill Landis, Robin Chandler, Tom Frusciano, and Caryn Radick for seeing this through.  And of course to Ron Jantz for getting me interested in this crazy stuff at a time when I had no direction or interest in my career.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/11/digital-preservation-for-archivists/",
        "teaser": null
      },{
        "title": "Yawn",
        "excerpt":"There is a series of blog posts that is now getting a lot of press in the biblioblogosphere.  I won’t link to it.  I won’t refer to it by name.  And I will not name its author.   Goodness, why give the author the satisfaction of a response?  It lends his or her points a certain credence that they would otherwise lack.  This, my friends, is little more than the sound  of irrelevance.  It will pass in time.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/13/yawn/",
        "teaser": null
      },{
        "title": "RESTful Fedora?",
        "excerpt":"Matt Zumwalt of MediaShelf, LLC has been hard at work thinking about how to make Fedora RESTful.  There is now a proposal on the Fedora wiki based on a PDF he sent to the fedora-commons-developers list.   It’s an interesting proposal.  I’ve read over the PDF version quickly but it does bear a closer read.   Whether SOAP or REST is more appropriate for a Fedora API is something I’m not sure about, though I don’t mean to imply it’s an either/or situation.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/19/restful-fedora/",
        "teaser": null
      },{
        "title": "Open source in libraries: Marching on",
        "excerpt":"A couple of interesting stories regarding open source library projects have come out during the past few days.   First, Carl Grant, the former CEO of VTLS, is forming a new company devoted to providing and building services for open source software. The name of the company is CARe Affiliates, and they have already struck a deal with open source software provider Index Data (creators of Zebra, YAZ, YAZ Proxy, Metaproxy, Keystone, and so forth).  I have worked just a bit with Carl and he seems to be a stand-up guy.  Best of luck, Carl.   Second, the Mellon Foundation has approached the GPLS with great interest in the open source ILS, Evergreen. Where this is going is yet to be seen, but it’s something to keep an eye on. It could be a fantastic opportunity for libraries that are frustrated with their current ILS and have the resources to commit development time.  (With an assumption that the former set is vastly larger than the latter set.)  This could be very exciting.   Those who still doubt that open source in libraries is a legitimate movement must find it more and more difficult to justify their arguments.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/06/21/open-source-in-libraries-marching-on/",
        "teaser": null
      },{
        "title": "Independence Day",
        "excerpt":"I woke up this morning wanting to do something special, something patriotic.  Until I come up with something better, I’m blogging.  Yes, that might itself be a sad commentary, but there you go.   Amidst all the fireworks, barbecues, and (at times mechanical) flag-waving, I like to put the significance of today into perspective.  How do I do that?  I read the Declaration of Independence.   We have seen these words a million times before, but I read them closely trying to avoid the clichÃ©d meanings that soundbite culture has ascribed to them.  It helps to put the document into context; I think about the courage and vision of those who wrote these words.  I think about the thousands who embraced the upstart revolution and forsook the old order to take up arms against old allies.  I think about the millions who have sacrificed their lives and livelihood throughout the history of our nation, despite any feelings about the justifications and conditions of the wars and conflicts we have fought.  I think about the American Revolution which continues to this very day as we struggle against our own weaknesses and challenges from abroad to keep the dream alive.  This paragraph, perhaps more than any, encapsulates that dream:   We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States.   It’s a powerful document and though it is not legally binding, it is the very spirit of our nation, this grand social experiment.  It is also a beautiful piece of prose and a landmark exposition of the principles of classical liberalism.  It’s as close to a Bible as I have, and I revere it.   Happy Independence Day, folks.  (And enjoy the fireworks, barbecues, and flag-waving.)   P.S. Cat macro representations of feeds should not be this funny.  My favorites: Yawn, Want to work at Princeton?, and RESTful Fedora?  ","categories": [],
        "tags": [],
        "url": "/blog/2007/07/04/independence-day/",
        "teaser": null
      },{
        "title": "The MLS and library technology",
        "excerpt":"Karen Coombs responds to Ross Singer, re: requiring an MLS degree in library technology positions.   I’m torn on the issue, as an erstwhile systems analyst who went through library school primarily for letters after my name.  I’ve seen both sides of the divide and have seen IT positions that might have benefited from the MLS –  a particular non-MLS comes to mind who had no sense of which battles were winnable and ultimately wound up leaving – and also seen jobs with a totally unnecessary MLS requirement that accomplished little other than watering down the candidate pool.   If a position will need to interact with librarians or act in public services capacities, the MLS is very useful; folks from IT (or elsewhere in the extrabibliosphere) do not necessarily know our culture or our values – hell, they might not even speak our language.  And there is something to be said for this sort of familiarity.  Librarians hold the reins in libraries and if you can’t speak to their values in a language they understand, you are likely to spend much of your career tilting against windmills (and getting nowhere fast).   On the other hand, we librarian folk think that libraries are a hell of a lot more special than we actually are.  Our needs are sometimes esoteric, but many times they are not, and so an MLS requirement probably does more harm than good.  And too often “web librarian” and “systems librarian” are euphemisms for “underpaid IT workers.”  Earning a master’s degree should not reduce your earning potential, and yet that is precisely what happens.   In summary, I guess my views align with Karen’s  but I appreciate what Ross is adding to the dialogue (having experienced much of the same nonsense).  We need more Rosses in library-land, and ought to treat them very well in order to keep them around.  Library technologists are (read: “should be”) first-class citizens in libraries and will play an active if not vital role in our future, whether they have the MLS or not.  We’d do well to keep that in mind.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/07/22/the-mls-and-library-technology/",
        "teaser": null
      },{
        "title": "[1 of 10] Why Ruby on Rails?",
        "excerpt":"1. Ruby   I gave a little talk on Ruby at today’s VALE-NJ Technology Awareness Group meeting, and so I’m using my slides to finally kick off this vaporseries.  [Slides are embedded as Flash here if you’re reading in an aggregator.]   It seemed to be taken pretty well, though it could easily have been dismissed as the fanboyish ravings of a neophyte Rubyist.  The folks from Rutgers were interested in having me come talk to their software architecture group, so it couldn’t have gone too badly, I suppose.   I need to stop cramming so much into presentations.  Rambling about reflection and OO at a speed the Micro-Machines Guy would envy isn’t the best way to share enthusiasm about Ruby.   The rest of the meeting was interesting: Ron Jantz, the Digital Library Architect at Rutgers, posed a number of challenging questions about trust, authenticity, and reliability in the digital realm; Terry Catapano, Special Collections Analyst at Columbia, exposed the lack of data model inherent in many current metadata schemas and suggested ontologies as a potential direction; and Jeffery Triggs, Applications Programmer at Rutgers, demonstrated the Java DjVu Viewer Applet for displaying DjVu-based images outside the DjVu browser plug-in.  Many good questions were asked, and new faces were in the crowd, so the future of TAG seems to be bright.   I’m hoping to follow up my Ruby talk with another on Rails.  That was my initial goal for today, but there was far too much to cover, knowing the audience was relatively new to both technologies.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/08/16/1-of-10-why-ruby-on-rails/",
        "teaser": null
      },{
        "title": "Library Camp NYC 2007",
        "excerpt":"I proposed an NJ Library BarCamp some months ago, not realizing that efforts were already under way to do the same in NYC.  In retrospect, I’m glad I didn’t do anything to get things moving; I wouldn’t have pulled things together nearly as well as the NYC folks did.  The event was excellent.  It was my first camp, and I’d definitely try another.  A big thanks to Stephen Francoeur et al.   Here are the three sessions I attended, with links to the “official” wiki pages for summaries:   \tSolr and Lucene (session moderated by AIP's Mark Matienzo and NYU's Jason Casden) seem to be gaining momentum in the library world.  Having gone to the last Code4Lib conference, my head was already chock full of relevant tidbits, but the moderators did a great job of showing examples, evangelizing, and keeping the discussion going. \tGrid Services (session moderated by OCLC Openly Informatics' Eric Hellman) might have been very interesting if I hadn't kept receiving phone calls from an insurance company.  I had to take the calls, and so this session was difficult to follow.  The basic idea was to think of networked library services like the power grid.  What would libraries want from the grid?  What would they be willing to contribute back? \tSemantic Web (session moderated by NYU's Corey Harper and CUNY's Sunny Yoon) was the most widely attended session I went to: standing room only!  When I first added the topic to the wiki, I had no idea it would draw this many people.  Odd that I would suggest this topic since I had little to offer on the topic, so I gleaned an awful lot.  The discussion was spirited and, as you might expect, the RDF vs. microformats arguments flew fast and furious across the room.  I'm left wondering if the RDFa/GRRDL approach might not be a good middle-road between the \"everything must be represented as RDF in a triplestore\" camp and the \"just embed microformats in xhtml\" people.   And now, the requisite name-dropping.  I got to reconnect with a bunch of people I hadn’t seen in a while, like Terry Catapano, Jay Datema, Nicole Engard, Valerie Forrestal, Kevin Reiss, and Sunny Yoon.  And I got to meet LibLime’s Chris Cormack, NYPL’s Josh Greenberg, Corey Harper, Mark Matienzo, Jenkins Law’s RayAna Min Park, and Steven’s Tech’s Linda Scanlon, among other people.   It was about as good as any camp without kayaks and archery can be.  Check out some more summaries.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/08/18/library-camp-nyc-2007/",
        "teaser": null
      },{
        "title": "Code4Lib 2007 Review",
        "excerpt":"Antonio reports that our review of the 2007 Code4Lib conference has been published in volume 27, issue 6 of Library Hi Tech News.   Though these articles have very low impact, the more press code4lib gets, the better.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/09/11/code4lib-2007-review/",
        "teaser": null
      },{
        "title": "Library degrees a mixed bag?",
        "excerpt":"Nicole Engard has posted the results of her library school survey.  She writes,  Why arenâ€™t we all required to learn a bit of the basics from each area of the library? All schools should require an intro to reference/research, intro to cataloging, collection development, library automation, management, and systems class - that gives students a feel for each area of the library allowing them to decide where they want to go. Then after giving a grounding throw in some practical experience.  I agree with her in principle that a core curriculum is helpful in preparing library school students for librarianship.  After all, what’s wrong with subjecting a future cataloger (or reference librarian, or subject selector, or systems librarian…) to learn about other aspects of what we do?  It puts their work in context within the library.   I can’t speak authoritatively about other library schools, only having been to one, but I would surmise that the answer is simple: that the library degree tries to be too many things to too many people.  We need clearer vision.   Case in point: the MLIS degree at Rutgers combines library studies, information science, and school media studies.  If these disciplines are collapsed into the one degree, how is it possible to have a core curriculum?  A good strategy is to have multiple tracks with set requirements, but students are, for the most part, left to mix and match their courses.   I can appreciate the freedom afforded to students by this.  They can craft the degree that they want with very few restrictions.  And that works for some students.   One is left to wonder, though, what a library degree from, say, Rutgers says about a candidates’ qualifications.  One conclusion to draw is that selection committees need to look beyond the degree towards specific courses and especially towards experience in the workplace, whether it be in a library or elsewhere.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/09/18/library-degrees-a-mixed-bag/",
        "teaser": null
      },{
        "title": "OAI-PMH in XQuery",
        "excerpt":"Thanks for the nod, Winona.  Hopefully you folks will get some good use out of the XQuery-based OAI-PMH data provider I’ve been working on.   I just want to clarify that only one small bit of the code is specific to X-Hive, and that’s a call to an extension that gets last-modified dates from the X-Hive service.  We do not reliably store this information in the metadata itself, and so I needed to go this route.  Some folks do store this in MODS or elsewhere in descriptive or administrative metadata.  It should be a two-line change to short-circuit this behavior (xhive-exts:last-update() is only invoked in two places, I believe).   I’m currently working on adding EAD support, modularizing things a bit more, and streamlining configuration.  resumptionTokens will come after that, I hope.   I’ll be interested to hear more of UVM’s implementation and how I can make this thing more useful to others.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/09/25/oai-pmh-in-xquery/",
        "teaser": null
      },{
        "title": "October",
        "excerpt":"October has been a month of transition for the past several years.   I moved into my first apartment in New Brunswick, NJ (my hometown) in 2001, shortly after the attacks of 9/11.  2003 saw me preparing for the MLIS program at Rutgers.  Elizabeth and I moved from New Jersey to Seattle in 2005 so that I could study computational linguistics at the University of Washington.  (I burned out on school very quickly and decided, after the Code4Lib 2006 conference, that I could in fact make a career in library technology.)  We moved back to the New Jersey area in 2006 for my current position working on the digital library at Princeton.   It’s been a great year.  My time at Princeton has been highly rewarding and we’ve taken full advantage of living closer to our family and friends.   October is also my favorite month of the year (even when the Yankees don’t win the World Series): the leaves turn pretty colors; the air takes on that early autumn crispness; and groves of apples are ripe for the picking on lazy weekend afternoons.  And this October, we have yet more changes on the horizon.  Elizabeth and I are moving to Arlington, VA and I’ll be starting at the Library of Congress on the 29th.  I’ll be working with folks like Dan and Ed on the repository development program, which supports a number of projects including Chronicling America (National Digital Newspaper Program).   It will be difficult to say goodbye to folks around here, but it’s the right move for us at this time.  Ah, October.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/09/27/october/",
        "teaser": null
      },{
        "title": "WordPress upgrades and the crossing of fingers",
        "excerpt":"On Monday I woke up with a very mild and very annoying bronchial infection.  Doctor Me prescribed two days of rest, relaxation, and chicken soup.  Where “chicken soup” is “finally dropping the unreasonably expensive and embarrassingly outdated web hosting package at Speakeasy and transferring all of my domains and content to Dreamhost,” that is.  I am now paying less than a third of what I had been for a hell of a lot more features.  And, I must say, administering DNS records, transferring files, and upgrading long-neglected software is rather amusing when you’re loopy and feverish.   My experiences thus far with Dreamhost are very promising.  I’m impressed but perhaps that’s because I’ve been in the web hosting ghetto for so long.  I understand there will very likely be downtime and sluggishness – that I can deal with.  Being shackled to 1999 technologies for $30/mth, while my e-mails go unanswered, not so much.   I upgraded both Technosophia and my wife’s blog to the latest WordPress release (2.3) from something ridiculous like 2.0.3.  In doing so, I also switched to the svn upgrade configuration Ryan Eby detailed a while back.   I crossed my fingers and it turns out the unAPI server plug-in still works in WP2.3.  Huzzah!  Not sure if it works in the 2.1 or 2.2 branches, but I suspect it does.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/10/04/wordpress-upgrades-and-the-crossing-of-fingers/",
        "teaser": null
      },{
        "title": "Use cases for Handle identifiers?",
        "excerpt":"Reading Adam Smith’s D-Lib article has got me thinking about identifiers again.  I don’t agree with some of the assertions in the section titled “A Persistent Identifier Primer” – URIs are in fact persistent; we just break them through poor management – and so I’m led to a fundamental question: what are the good use cases for Handle (or ARK, or PURL) identifiers?   I get the need for persistent and globally unique identifiers; I’m just wondering why one needs special software with a separate URI namespace to gain persistence.   One potential use case might be resources that are outside of the organization’s control – i.e., licensed content from vendors – but surely folks are using Handles for many resources that are created and managed within the organization.  And I’m curious why they have decided that Handles are more durable than native URIs (the URIs to which Handles redirect), and how they deal with the problem of downstream (post-redirection) citation and bookmarking.  How useful is this sort of identifier scheme if your users never even see the supposedly more persistent URI for a resource?   As a former proponent of Handles and ARKs, this may seem like a hypocritical question to pose.  If I had to answer my own question, I would say that Handles seem like a good option because they save you some work and headaches in the short-term; you don’t need to get together with your web team and come up with a scalable and sustainable URI policy; just assign native URIs in the usual haphazard way and generate Handles to compensate for a lack of identifier policies.   But if you’re already making an organizational commitment to identifier persistence – and if you’re rolling out Handles, I’d wager that’s likely – why not do so by minting carefully-considered cool URIs?  Less management and technology overhead and less confusion for your users are two good reasons to consider it.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/10/04/use-cases-for-handle-identifiers/",
        "teaser": null
      },{
        "title": "Self-archiving",
        "excerpt":"Dorothea left a comment on a post announcing the publication of a little conference review some colleagues and I splurted out.  In the announcement I lamented a bit about impact and she wisely suggested I consider depositing the review in a subject repository such as E-LIS.   We looked into our agreement with the publisher and it was actually quite permissive.  (Way to go, Emerald.)  And here’s the review in all its open access glory.   Thanks, Dorothea!  ","categories": [],
        "tags": [],
        "url": "/blog/2007/10/10/self-archiving/",
        "teaser": null
      },{
        "title": "Using Linux to fix Windows",
        "excerpt":"The hard drive on my laptop is slowly failing and a combination of being busy, lazy, and cheap is preventing me from replacing it.  About once every two weeks over the past couple months, one of the Windows registry files becomes corrupted and the XP disk is unable to repair it.  And the HD fails basic manufacturer-provided diagnostics.  But I’m stubborn.  So I’ve been routinely resuscitating this box and I decided to post the process I use.   If you boot and see a message like  Windows could not start because the following file is missing or corrupt C:\\windows\\system32\\config\\system then you may be interested in this.  I should note that there are more Windows-y ways to fix this.  But I have an Ubuntu Edgy disc on hand, and so these instructions are for using Ubuntu Edgy as a band-aid an adhesive bandage for a dying hard drive or a screwy Windows installation.  The ntfs-3g and ntfs-config packages are necessary to mount your local hard drive in read-write mode, otherwise these instructions would be much shorter.   I’m assuming you have a good backup from which to restore the missing or corrupt files that are preventing you from booting Windows off your hard drive.  My backups are available via a networked Samba mount.  If yours are on a secondary internal hard drive, an external hard drive, a CD or floppy or thumb drive, some of these steps won’t apply to you.   \tGet a copy of Ubuntu and burn it to a bootable medium.  I'm using a Ubuntu 6.10 (Edgy) on a CD-R. \tInsert your bootable medium into the crappy computer.  Boot said crappy computer from the medium.  You may need to fiddle with your BIOS to get it to boot from removable media, but hopefully not. \tAt the Ubuntu boot menu, chose \"Start or Install Ubuntu.\"  Don't worry: it won't write over your hard drive or touch any of your data. \tWhen the operating system is done loading, click on the Applications menu, and choose Accessories, then Terminal. \tSince my backups are available on a Samba mount, and the Edgy CD does not have Samba installed, I first must install the Samba FS and its dependencies:  sudo apt-get install smbfs \tCreate a directory for mounting the Samba share: sudo mkdir /mnt/remote \tMount the share, substituting appropriate values for the IP address and the share name: sudo smbmount //192.168.1.5/backups /mnt/remote \tIf not running Edgy, see this guide to mounting local Windows drives in read-write mode.  Otherwise, first open the aptitude sources list: sudo nano -wc /etc/apt/sources.list and add the following three lines at the bottom of the file: deb http://flomertens.free.fr/ubuntu/ edgy main main-all deb http://ntfs-3g.sitesweetsite.info/ubuntu/ edgy main main-all deb http://flomertens.keo.in/ubuntu/ edgy main main-all Save and close the file. \tLet aptitude update its list of sources: sudo apt-get update \tInstall necessary packages: sudo apt-get install ntfs-3g ntfs-config libfuse2 \tLaunch the ntfs-config utility: sudo ntfs-config Call the mount point \"windows\", which will live at /media/windows.  Click the Add checkbox next to the device corresponding to your local drive (/dev/hda1 for me).  Click Apply.  Make sure the checkbox next to \"Enable write support for internal drive\" is clicked on the next dialog, and click Ok. \tCopy over backed up versions of missing or corrupt files:  sudo cp /mnt/remote/repair/system* /media/windows/WINDOWS/system32/config/ is an example I've used a couple times. \tShutdown, remove the Ubuntu CD when prompted, and boot off your hard drive.  Cross fingers.  And you should be back in action.  Voila?  ","categories": [],
        "tags": [],
        "url": "/blog/2007/10/13/using-linux-to-fix-windows/",
        "teaser": null
      },{
        "title": "My Left Arm for a Teleporter",
        "excerpt":"Moving Day has come.  In approximately six-and-one-half hours, movers will show up at my front door to take my crap – which seems to have multiplied, making me wonder if dark closets are particularly romantic environments for inanimate objects – and haul it down to Virginia.  How did we accumulate so much crap?  Why can we put a man on the moon, and a monkey into space, and yet we still can’t teleport matter?  But I digress.   As excited as I am about working at LC, deciding to leave Princeton and the New Jersey area was not easy.  New Jersey’s my home state, where most of my friends and family still live, and though I keep moving away, I’ve wound up coming back each time.  (Insert obligatory Godfather “pull me back in” impression.)  I was completely comfortable and very happy working at Princeton, where we were making real headway on building out the digital library.  I’ll miss my colleagues greatly – the people I met at Princeton were all friendly and created a laidback workplace.  I will miss our semi-regular lunches at Triumph brewery.   I have to thank Kevin Clarke specifically for making my time a real joy.  He sheltered me and my colleague, Parmit, from a lot of administrivia and committees and meetings (oh my!), allowing us to concentrate on the job at hand, and was always supportive of our technology choices – I’m glad I’ve left a legacy at Princeton, namely saddling my replacement with a bunch of Ruby code and a couple Rails apps.  I was especially thankful that Kevin and our administrators were fully behind my decision to release much of the software I wrote as open source (MIT/X11 license).  They made the procedure an absolute snap, allowed me to retain copyright, and were flexible with regard to the choice of license.  As an open source neophyte, I appreciated the flexibility.   Time to hit the hay, and hope that tomorrow’s move goes smoothly.  We’ll have ten days to unpack and settle in, and I start at LC on the 29th of this month.  I noticed today that one of the big projects my team has been working on got a write-up in the NY Times today (hat tip: Ed).  I can’t wait to jump on board and start wrapping my mind around these projects.  I fully expect to be overwhelmed and challenged.  I’m really looking forward to it and I’ve got a great team to work with.  But for now, I’ve got to rest.  The movers will be here before I know it and we have a long day/week ahead of us.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/10/18/my-left-arm-for-a-teleporter/",
        "teaser": null
      },{
        "title": "OAI-ORE ResourceMap for WordPress",
        "excerpt":"This is very rough, but here’s a WordPress plugin that provides a resource map for the aggregation of all posts within an installation of WordPress.  I’ll be working on this some more, but for now, it does appear to work and validate (as Atom).  Useful?  If so, I’ll zip it up and commit it to the wp-plugins svn.   Note:Ed reminds me that xsltproc can be used to transform the Atom-based resource map into RDF via GRDDL:   xsltproc http://www.openarchives.org/ore/atom-grddl.xsl https://mike.giarlo.name/blog/wp-content/plugins/oai-ore/rem.php   Update: The plugin has its own page.  ","categories": [],
        "tags": [],
        "url": "/blog/2007/12/14/oai-ore-resourcemap-for-wordpress/",
        "teaser": null
      },{
        "title": "\"Harvard and the Making of the Unabomber\"",
        "excerpt":"Oh frabjous day!  I noted with great excitement the following post from “Digital Eccentric,” Leslie Johnston:   I was initially very excited by the announcement on BoingBoing that The Atlantic had opened its archive.  I read the Editor's Note describing the decision.  I followed the link to start my exploration.  It's a little misleading. The _site_ is now open to all. They have \"Unbound\" (web only) content and full issues back to 1995 open. But their other free content seems to be selected material. Back in 2000, after having survived the terrible and great Y2K (!@#), I read a fascinating article by Alston Chase on the childhood and college experiences of one Theodore “Ted” Kaczynski: the Unabomber.  It humanizes him in a way that may have you screaming “darned bleeding-heart apologist, how dare you!” and puts his actions in a context where they actually, gulp, make some sense.  This is not to excuse his actions; rather, it is a glimpse into how an otherwise rational and normal (if brilliant) man transformed into a monster:   In the fall of 1958 Theodore Kaczynski, a brilliant but vulnerable boy of sixteen, entered Harvard College. There he encountered a prevailing intellectual atmosphere of anti-technological despair. There, also, he was deceived into subjecting himself to a series of purposely brutalizing psychological experiments -- experiments that may have confirmed his still-forming belief in the evil of science. Was the Unabomber born at Harvard? From “Harvard and the Making of the Unabomber”.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/01/25/harvard-and-the-making-of-the-unabomber/",
        "teaser": null
      },{
        "title": "Wherein I sort of admit to being a sunshine patriot",
        "excerpt":"THESE are the times that try men's souls. The summer soldier and the sunshine patriot will, in this crisis, shrink from the service of their country; but he that stands it now, deserves the love and thanks of man and woman. Tyranny, like hell, is not easily conquered; yet we have this consolation with us, that the harder the conflict, the more glorious the triumph. What we obtain too cheap, we esteem too lightly: it is dearness only that gives every thing its value. Heaven knows how to put a proper price upon its goods; and it would be strange indeed if so celestial an article as FREEDOM should not be highly rated. -- Thomas Payne, The American Crisis Rest in peace, GOP.  You had a good run, but in the end, loyalty to a broken system weighed down the time-tested principles of the party that Lincoln built.  Now you are grand only in name.   I remain hopeful that the enthusiasm of the Ron Paul Revolutionaries sustains their efforts to transform the Republican party, from the ground up, back to the party of non-interventionism and small government (even while my own energy to stay involved has waned).  I do believe there is a place for (small l) libertarian ideals in American political debate and that place is not out along the fringe.  And though I am a lifelong progressive, I will continue to play whatever small part I might in nudging the GOP away from neo-conservatism, which I see as a highly dangerous ideology.   It is looking more and more likely that I will have no better option than to vote for Barack Obama (EDIT: or Hillary since she did not bomb in last night’s primaries) in the 2008 election.   The Democrats are no less the party of American Empire than the Republicans and I am troubled that the only two viable options in our electoral system  are both agents of big government and imperialism.  I have long supported the Democratic party despite their being a ship of fools, more concerned with the fringe than with the core, despite rampant political correctness, despite turning their backs on the anti-federalist principles upon which they were founded.  I won’t feel at home among their ranks, but I will very likely be supporting their candidate against the war-mongering John McCain.   Cynicism and idealism are battling within me, and I fear it’s only a matter of time before cynicism once again wins the day.   Yes, I’m being dramatic.  Maybe I’m overreacting a bit.  But damn it, am I bummed.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/03/04/wherein-i-sort-of-admit-to-being-a-sunshine-patriot/",
        "teaser": null
      },{
        "title": "Not quite mint juleps on the veranda",
        "excerpt":"That I very nearly called this post “Southern comfort” reveals me as a long-time yankee from the urban northeast.Â  No, I suppose Arlington, Virginia isn’t quite the south – certainly not culturally – but you can see why I’d say so if you consider that I once thought any place south of 195 may as well have been Deliverance country.Â  Ah, the old provincialisms.Â   And to further deconstruct this ridiculous metaphor, my entire apartment is probably smaller than a veranda.   But, boy howdy, did I have a relaxing and refreshing day: sitting on the couch with the windows open, reviewing a chapter of a friend’s upcoming book, while the wind rustled the blooming saucer magnolia right outside the windows.Â  It was a beautiful, sunny day, and the pair of mourning doves that have taken up residence on the neighbor’s window sill were soaking up the rays, singing their “woo-woo-oo-oo-oo” song on occasion.   Almost any sort of writing is a real chore for me and yet every now and then I commit to writing or editing something or other.Â  I enjoy it but it does take the sort of concentration that I’ve found so difficult of late.Â Â  I do not regret agreeing to review the chapter in question (on DRM technologies, for the curious), and I feel somewhat validated in my decision after the elements all aligned today and made for a very pleasant time.Â  (I did not get as much done as I would have liked, but what else is new?Â  Time management remains a challenging task, especially when the television and the internets are so near. )   I’m beginning to ramble and I don’t really have a point.Â  It was a good day – an entire good weekend in fact – and I felt it worth committing to bits.   … Hey, are those banjos I hear?  ","categories": [],
        "tags": [],
        "url": "/blog/2008/03/16/not-quite-mint-juleps-on-the-veranda/",
        "teaser": null
      },{
        "title": "Code4Lib Journal",
        "excerpt":"… meanwhile, the Code4Lib Journal has published its second issue and boy is it packed with articles; Eric Lease Morgan, Coordinating Editor of the issue, does a bang-up job on the introduction (though the title has effectively Bostonroll‘d me).  Each article in this issue has a little bit of something for all who call themselves a librarian or work in a library. Each identifies some sort of library problem to be addressed, and offers one or more solutions. Many are complete with code snippets. After all, this is Code4Lib.  For example, people in public service may be interested in Edward M. Corrado and Kathryn A. Frederickâ€™s review of database-driven subject guide applications. Kenneth Furuta and Michele Potter describe a simple help system that brings librarians running to the reference desk. Margaret Mellinger and Kim Griggs explain how library resources can be organized into course pages without the need of HTML knowledge and yet sport Web 2.0 features. Nancy Fried Foster, Nora Dimmock, and Alison Bersani shed light on participatory design.  For those of us who enjoy cataloging and metadata issues, Jonathan Gorman outlines how he modified VUFind to exploit Wikipedia and cataloging authority records to enhance information about authors in a library catalog. Chris Freeland, Martin Kalfatovic, Jay Paige, and Marc Crozier illustrate a different use of Library of Congress Subject Headings by integrating place names with Google Maps. Carol Jean Godby, Devon Smith and Eric Childress describe a technique for crosswalking just about any metadata format into just about any other metadata format.  For the systems librarian in you, Dan Scott and Kevin Beswick share how they used Linux live CDs customized as kiosk browsers to provide laptops as â€˜quick lookupâ€™ stations at their library. Andrew Darby takes advantage of the Google Calendar API to easily manage the display of library hours. Jody DeRidder exploits Google sitemap technology and static HTML pages to make content in the â€œdeep Webâ€ more accessible. We hope you find these articles useful, stimulating, and relevant to your daily working lives. I  am ashamed to admit that I have not yet finished the first issue, so I now have pages upon pages to read.  Ordinarily when I am behind on my reading I wind up letting bits fall by the wayside but the material largely looks too good to ignore.   Congratulations to Editorial Committees past and present and to the community on the whole!  ","categories": [],
        "tags": [],
        "url": "/blog/2008/03/25/code4lib-journal/",
        "teaser": null
      },{
        "title": "Rails Deployment",
        "excerpt":"Deploying Rails (to Apache servers) is about to get much easier.Â  Hopefully.   Â http://www.modrails.com/Â    Deployment has long been the bugaboo with Rails, so this should bode well for the framework.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/03/26/rails-deployment/",
        "teaser": null
      },{
        "title": "Jythons and Javas and bears, oh my!",
        "excerpt":"It’s hard to believe but I’ve been at the new job for six months already, a full half-year come the 29th.  Some days it seems like I’ve been here forever; others like I’m still a rank newb.   I haven’t written terribly much about what I’ve been up to (but I assure you I’ve been busy).  Let me rectify that.  The Transfer Problem  Two of the projects I’ve been working on relate to a fairly general problem that we like to call “transfer,” which revolves around, well, transferring files to and fro.  Sounds simple.  Is simple.  That is, until you start thinking about preservation and accounting for a highly heterogeneous network with idiosyncratic nodes, esoteric storage software, and differential firewall rules.  And that’s where it gets interesting (and problematic). The transferring itself, or copying of files from one location to another which we call “transport,” is the easiest part.  We like to use common tools in our environment.  It makes life easy.  And so good ol’ scp seems like an obvious choice to handle the job.   Since preservation is a core aspect of our “repository,” a term which I use loosely, we must build certain other functionalities into the transfer process: validation, verification, inventory, backup, ingest, and so forth.  Every time a file is copied to a non-transient location, we verify the file against a (SHA1) checksum and record an event for auditing purposes.  Repository Workflows  Some steps in the transfer process are routine and best handled by machines. Thus we automate them with scripts and code.  Others require human intervention.  This introduces another key aspect of our repository needs: workflows.  No two projects will have the same workflow and yet all will have some steps in common.  We’re using JBoss’s jBPM library for workflow management and it is more than capable of handling our workflow needs.  It allows us to model complex and varied flows in a robust and not ad hoc way; it does seem preferable to me to model our workflows via jBPM’s graphical editor (serialized to JPDL XML) rather than copying around blocks of code and otherwise modeling the workflow procedurally in business logic.   One of my coworkers (the author of this) designed a complete workflow system in jBPM last summer and I’ve taken on implementing, tweaking, and testing this system, which has required bootstrapping my sorry-arse Java skills and learning jBPM.  Though I find it difficult to think in Java patterns and generally find it a burdensome environment, I’m quite impressed by jBPM.  I’ve been working on various updates of and unit test coverage for the workflow system, which has been a crash course in a number of Java technologies and a perfect first task at LC, as it gets me into the guts of things.  The Java stack we use for our workflow is highly abstracted and componentized, which is conveniently modular… but it’s also Java: fairly heavy and arguably not as agile as dynamic languages such as Python.  Two Great Tastes?  So recently we’ve begun to think about implementing some transfer and workflow components in Jython (Python written in Java).  Why?  The value of Jython is as follows:   \tEase of deployment - Deploying jar files to existing JVMs in production environments (which we do not control) is a simple task, or at least simpler than some other options. \tInteroperability - Our stack is primarily Java-based and so interoperating with existing Java components means not having to rewrite functionality in other languages.  Jython allows Python to talk to Java and vice versa. \tFamiliarity - It's Python, and we like Python.  It's the closest my team has to a lingua franca and so it increases the chances of sharing code, maintaining code, and so forth.  It does not come without its drawbacks:   \tCurrency - The Jython project went moribund for a few years or so and the latest stable version is now 2.2.1.  Compare that to the latest version of Python: 2.5.2.  I don't begrudge the Jython developers, though.  I'm glad some folks picked the project up, dusted it off, and breathed new life into it.   I am also glad that they're skipping 2.3 and 2.4 releases and plowing right into a 2.5 release.  Because of this currency issue, some Python libraries won't work with the latest Jython and that means you're stuck looking for outdated, potentially vulnerable Python libraries, or hooking into Java libraries for the same functionality (which inevitably means more lines of code).  I'm no lines of code fetishist but it does militate against the goal of agility somewhat.One does have the option of living on the edge and trying out the 2.5 branch, but that seems out of step with an infrastructure that is supposed to preserve terabytes upon terabytes of our nation's, and the world's, intellectual property.  It's a responsibility I do not take lightly, as much as I'd like to be on the bleeding edge. \tInteroperability difficulties - Talking to Java from Jython is a snap: just import java at the top of your script, and, assuming your classpath is copacetic, voila: you have access to Java libraries in your Python code!  Talking to your Jython modules from Java code is, well, a little more complicated. Read on.  Despite the caveats it does seem a sane, reasonable, and potentially productive path to go down. Right?  I am specifically looking to implement two workflow components in Jython: one for transport (wrapping Ant’s JSch library, which provides a slick scp API) and the other for automation of ZFS filesystem/volume creation on the staging server.  Nothing arcane, nothing tricky, nothing fancy.  So it must be easy!  Right? …  Lessons Learned?  I’m beginning to wonder about the feasibility of using Jython to make bits of our Java stack more agile. Specifically, there are three ways to get at Jython code from Java:   \tCompile to bytecode/jar via the Jython compiler (jythonc) and reference your Jython objects and methods as though they were POJOs \tEmbed a Jython interpreter \tInstantiate a (JSR-223) script engine  Option 1 is nice because you get object- and method-level interop.  However, jythonc is unsupported and will disappear.  This does not seem sustainable though I might be able to limp along a while.  And there are signs of hope:  Though jythonc is going away, all of the capabilities it provides will be present in 2.5 in other forms.  We're adding functionality to expose Python classes as Java classes using decorators to replace the docstring class creation that jythonc provided, and we're adding static compilation of proxy classes so regular jython can run in applets and other environments with restrictive classloaders.  We're definitely doing something about jythonc. That doesn’t help much now, of course, but just because jythonc goes away does not mean my jars will stop working.   The suggested methods for option 2 [1, 2] seem to be more trouble than they’re worth.  If the goal is more agile development for certain components, the reliance upon multiple, separate Java classes – an interface class and an object factory, in the examples listed – to get a ten-line Jython script working, this seems suboptimal, both inefficient and not straightforwardly maintainable; it seems, to me and my Java-dumb ways, rather baroque.   Option 3 [1, 2] is more appealing than option 2 as it does not rely upon these other classes specifically for Jython code.  But the number of lines of Java code that must be wrapped around the Jython to get it working looks like overkill for the drop-dead simple scripts I’m writing – it might be easier, for instance, to just write the darn things in Java and be done with it.  (Did I just say that?)  Conclusions  Options 2 and 3 are similar as both involve embedding Jython code, or referencing files with Jython code, and interpreting the code within Java.  Generally, I worry that either option would obviate the benefit of agile Jython scripting because you wind up wrapping the code in so much Java.  I offer two disclaimers to counter my objections:   \tCleverer Java coders than myself could, I am almost certain, find ways to build abstractions (or abstractions of abstractions) to eliminate the \"lines of code\" and \"many separate classes per Jython script\" issues \tThe value of Jython in our Java environment increases proportionately with the complexity of the component -- given the overhead, a short Java class seems easier and more straightforward to implement than a short embedded Jython script.  On the other hand, there's value in embedding a Jython script that'd be an order of magnitude simpler than its Java analog.  At least, that’s the state of my head right now re: getting Jython and Java to play nice.  If I make any breakthroughs or give up entirely, I’ll post follow-ups.   I am but a Java philistine and a Jython neophyte, so I remain humbly open-minded.  I would greatly appreciate comments, questions, corrections, smackdowns, sagacious advice, and so on.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/04/11/jythons-and-javas-and-bears-oh-my/",
        "teaser": null
      },{
        "title": "Jython scp",
        "excerpt":"In spite of some open questions, I’ve been making some progress on my Jython-based transport tool.  Right now it’s pretty dumb and simple: it copies files to and fro via scp.   Being a newb at both Java and Jython made finding the right libraries a bit of a challenge, and so I’m posting some code here for folks in the same boat.  It’s not particularly pretty due to 1) wanting to get something working very quickly, and 2) weird errors when I try to make things prettier (such as getting rid of the hard-coded bits), but I’ll resolve these soon.  The jython code:   # Biter.py import java from org.apache.tools.ant import Project from org.apache.tools.ant.taskdefs.optional import ssh  class Biter(java.lang.Object):     def __init__(self):         self.keyfile = '/home/user/.ssh/id_rsa'      def transport(self, from_uri, to_uri):         \"@sig public void transport(String from_uri, String to_uri)\"         scp = ssh.Scp()         scp.setKeyfile(self.keyfile)         scp.setPassphrase('')         scp.setTrust(True)         scp.setProject(Project())         scp.setFile(from_uri)         scp.setTodir(to_uri)         scp.execute()         print \"%s -&gt; %s\" % (from_uri, to_uri)  I set my CLASSPATH to include /usr/share/jython2.2.1/jython.jar, /usr/share/java/jsch-0.1.28.jar, /usr/share/ant/lib/ant-jsch.jar, /usr/share/ant/lib/ant.jar, and ‘.’, and then compile to bytecode via jythonc -idp gov.loc.repository.transport Biter.py.  Once it’s compiled, I can use the Biter class in Java code thusly:   # BiterClient.java import gov.loc.repository.transport.Biter;  public class BiterClient {     public static void main(String[] args) {         Biter biter = new Biter();         biter.transport(\"/home/user/tmp/test1\", \"user@example.org:tmp/test2\");         biter.transport(\"user@example.org:tmp/test2\", \"/home/user/tmp/test3\");     } }  I should note that the passphrase and keyfile are not necessary if you’re passing valid username and password credentials in the remote to_ or from_uri (and the server in question supports password auth).  But pubkey auth is useful for our purposes and that’s why it’s in there.   Hopefully someone somewhere finds this helpful. :)  ","categories": [],
        "tags": [],
        "url": "/blog/2008/04/17/jython-scp/",
        "teaser": null
      },{
        "title": "Tweet tweet",
        "excerpt":"Guess who is finally on Twitter?  ","categories": [],
        "tags": [],
        "url": "/blog/2008/04/22/tweet-tweet/",
        "teaser": null
      },{
        "title": "Hiccup-y Hardy Heron",
        "excerpt":"In spite of how irksome I find “oh hai i upgrayded!” posts, I’m about to be guilty of same.   I upgraded my Optiplex GX620 from Gutsy to Hardy yesterday afternoon and it seemed to go as smoothly as it did on my HP box at home.Â  All looked a-okay this morning until, upon returning from a meeting, my display was all funky and jerky and laggy.Â  The right edges of my windows were uniformly screwy – I would have to click about an inch to the left of whatever I wanted to click on – and the right and left edges of the screen caused visual trails when I dragged windows around.Â  (And this has nothing to do with my usual breakfast of bacon and psychedelics.)Â  This wasn’t the first time I’ve run into problems with compiz/beryl and Ubuntu and so I was hopeful that things could be easily remedied.   I was still able to get around a bit and I found a Hardy installation guide that fixed me all up (I hope).   I should probably note that the Optiplex in question has an ATI Radeon X600 series video card.   Pining for the visual trails,   Mike from Arlington   P.S. Ubuntu, I still ♥ you.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/04/29/hiccup-y-hardy-heron/",
        "teaser": null
      },{
        "title": "From a midnight call to self.rand()",
        "excerpt":"I lament the greatest/crappiest dorkcore band (n)ever to have existed, Illegal Operation, with the stellar line-up of Major Crash on drums, General P. Fault on bass, and Colonel Dump on guitar.   It is rumored that there is some intersection between Illegal Operation, Lack of Talent, and Sausagebot.   P.S. Yes, I was (am) the (un-)esteemed Col. Dump. P.P.S. I am woefully sleep-deprived this week.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/05/07/from-a-midnight-call-to-selfrand/",
        "teaser": null
      },{
        "title": "Stupid terminal tricks",
        "excerpt":"Sometimes I find it useful to keep long-running processes in a session of screen.Â  And sometimes I launch one of said processes outside of screen, and then I yell something like “doh!” or an expletive, because, as I said, I do find screen useful.Â  Depending on how far the process has gotten, whether it was the sort of operation that would not run happily again, or how much cleanup a second run would require, I either kill the process and restart it or I suspend it with Ctrl+z and send it to the background with bg % so that it doesn’t die when I log off.Â  The latter is a decent option.Â  But, darn it, I like screen.   Well, perhaps I’m the last to know, but there’s this neat little tool called retty that allows you to attach running processes to your terminal.Â  I installed it in Ubuntu Hardy the typical way (sudo apt-get install retty).Â  So, the next time I screw up, I’ll Ctrl+z, bg it, and then screen retty {PID}.Â  Voila!  ","categories": [],
        "tags": [],
        "url": "/blog/2008/06/07/stupid-terminal-tricks/",
        "teaser": null
      },{
        "title": "Microsoft has the Power(set)",
        "excerpt":"Powerset’s Sr. Product Manager writes:  Weâ€™re excited to announce officially that Microsoft has signed an agreement to acquire Powerset. ... With any startup, the challenge is to take the seeds of an idea and grow it into a viable company. At Powerset, we transformed our idea into a world-class semantic search platform, demonstrating the future of search with our Wikipedia search experience. But building a large-scale semantic search engine is expensive, requiring an engineering effort and computing resources beyond what most start-ups could ever imagine. Because our goals around improving search align so well, Powerset has decided to team up with Microsoft. We believe that this is the fastest way to bring our technology to market at a large scale. Read more on Microsoft’s Live Search blog.   It’s not surprising to see Microsoft gobble up a company that has strived to be a Google-killer from its inception.Â  It will be interesting to watch Microsoft continue battling Google and to see how this latest acquisition comes into play.   (Maybe I gave up on compling too soon, eh?)  ","categories": [],
        "tags": [],
        "url": "/blog/2008/07/01/microsoft-has-the-powerset/",
        "teaser": null
      },{
        "title": "A founding father on the party system",
        "excerpt":"20 I have already intimated to you the danger of parties in the state, with particular reference to the founding of them on geographical discriminations. Let me now take a more comprehensive view, and warn you in the most solemn manner against the baneful effects of the spirit of party, generally.  21 This spirit, unfortunately, is inseparable from our nature, having its root in the strongest passions of the human mind. It exists under different shapes in all governments, more or less stifled, controlled, or repressed; but, in those of the popular form, it is seen in its greatest rankness, and is truly their worst enemy.  22 The alternate domination of one faction over another, sharpened by the spirit of revenge, natural to party dissension, which in different ages and countries has perpetrated the most horrid enormities, is itself a frightful despotism. But this leads at length to a more formal and permanent despotism. The disorders and miseries, which result, gradually incline the minds of men to seek security and repose in the absolute power of an individual; and sooner or later the chief of some prevailing faction, more able or more fortunate than his competitors, turns this disposition to the purposes of his own elevation, on the ruins of Public Liberty.  23 Without looking forward to an extremity of this kind, (which nevertheless ought not to be entirely out of sight,) the common and continual mischiefs of the spirit of party are sufficient to make it the interest and duty of a wise people to discourage and restrain it. George Washington, our first president, whom his peers wished to elevate to king, warns of the dangers of a party system in his farewell address.Â  It is widely known that Washington opposed the creation of parties – and that he was the last president not to be affiliated with one – but his words here are nonetheless powerful.   Seeing the stranglehold that the Democrats and Republicans have on power in the union makes me wonder if we haven’t failed in this grand experiment by ignoring the wisdom of its founders and gradually abdicating our responsibility for its care, and our own liberty, like sheep who would ask wolves to babysit their lambs.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/07/10/a-founding-father-on-the-party-system/",
        "teaser": null
      },{
        "title": "Justice and Moral Rectitude",
        "excerpt":"I have been meaning to write up some of my thoughts from the Revolution March and Rally and more generally on my evolving impression of the phenomenon that is the “Ron Paul Revolution,” with which I have been involved to some small extent and fascinated to a larger extent.Â  I don’t have the time or clarity for that just this moment.Â  But one of the things on my mind, spurred in part by Tom Woods’s speech at the Rally and his new book, “Who Killed the Constitution?”, is the tension that sometimes exists between “doing the right thing” and following the law as it was meant to be interpreted.    Reflecting on the constitutional transgressions of the executive and the judicial and the legislative branches, of the Democrats and the Republicans and the Whigs, I wonder what is the right action to take when the aims of justice are counter to those of moral rectitude.Â  Contrary to public opinion, the United States of America is not a democracy; we are a democratic federal republic, a constitutional republic, the operative word being “republic.”Â  We ought not to bow to the whims of the masses, as in democracy – which, in the words of Benjamin Franklin, may be defined as “two wolves and a lamb voting on what to have for lunch.”Â  Rather, we are subject to the rule of law, and the Constitution is the supreme law of the United States.Â  Whereas the Declaration of Independence breathed life into the union, the Constitution (and Bill of Rights) provided its skeleton and its life-blood.   What recourse do we have, then, when the Constitution prevents legislators, the judiciary, and the executive from doing what they, or the masses, deem “the right thing?”Â  Does the end, some morally sound outcome, justify the means even when the means involves sidestepping constitutional restraints?   We have a number of philosophical frameworks available to us to evaluate this issue – various theories of rights, justice, and morality – and I flit from one to the next with regularity.Â  If nothing else, I hope it enables me to see the many sides and nuances of the argument.Â  For instance, I might think that ending slavery was a moral necessity, that Brown v. Board of Ed. was a net win, that putting an end to the Nazi regime and liberating the concentration camps was the right thing to do.   But I’m also uncomfortable with the federal government’s repeated stepping on the Constitution, its disregard for states’ rights, and increasingly activist roles in excessively powerful executive and judicial branches. There are numerous examples,Â  many of which are in Woods’s book: Adams’s Alien and Sedition Acts, Lincoln’s war against the secessionists, Wilson’s Espionage and Sedition Acts, Truman’s grab of the steel industry, the SCOTUS interpretation of the Equal Protection clause in favor of Brown v. Board, the examples go on and on.   When the framework for our very government is the Constitution, that which the government it was meant to restrain so openly flouts, I am taken to believe that we flirt with tyranny the more we side with rectitude over justice.Â  (I am playing a bit fast and loose as my time to write draws to a close by referring to the strict Constitutionalist perspective as that of “justice.”)Â  I don’t meant to hint here that the government ought not to have ended slavery, or kept the union together, and so forth, but that there were other, perhaps more difficult, ways of achieving these same ends within the bounds of the law as it was written.Â  When the government acts as though it is above the law, it establishes a very dangerous precedent.Â  The greater the amount of power in the government’s hands, the less liberty in the people’s – isn’t this the tyranny our Constitution was supposed to protect us against?   It is often said that America is a grand social experiment, and I find myself agreeing.Â  Is the experiment predicated on America being a nation that strives to do right at all costs?Â  Or is it more about the lofty principles enshrined in our Declaration of Independence and codified in our Constitution, and how well our republic stands up to the natural progression towards empire, and towards tyranny?Â  I believe very strongly that the American Revolution is not bound in time but that it continues to this very day, and that the Constitution, and adherence thereto, is the very best chance we have to protect us from the base instincts of humanity and sustain a system of government that instead appeals to “the better angels of our nature,” as Honest Abe would have put it.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/07/16/justice-and-moral-rectitude/",
        "teaser": null
      },{
        "title": "Sustaining digital libraries",
        "excerpt":"About a month ago, I read on my colleague’s blog that the Emory University Digital Library published a new book on sustaining digital libraries.Â  I’ve finally started reading it and figured I would post a note here.  The articles of this monograph provide resources for digital library stakeholders who seek to better understand how to effectively evolve such efforts from short-term projects to long-term sustainable programs. The monograph includes contributions from leaders in major digital libraries that have made such transitions or which are systematically considering the question of programmatic sustainability, including representatives from the National Digital Infrastructure and Information Preservation Program (NDIIPP) and the National Science Digital Library (NSDL). I might also note that the book is available for free as a PDF.   So far I’ve read the introduction by the editors and the abstract from Leslie’s paper, and the book looks like a high-quality read from cover to cover, with articles based on actual digital library experience.Â  It’s a pragmatic approach for how to sustain digital library initiatives, looking beyond technical concerns towards the more challenging social and economic ones.Â  To some extent, we are getting pretty good at preserving bits and relationships between collections of bits – it is yet to be seen how good we will be at preserving the preservation systems themselves.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/07/18/sustaining-digital-libraries/",
        "teaser": null
      },{
        "title": "ORE plugin updated",
        "excerpt":"I’ve been using my time at RepoCamp today to get the OAI-ORE plugin for WordPress validating again.Â  I’m having some trouble using the validator so I say that with some diffidence.Â  But the latest code which is now checked in to the WordPress plugins svn repo ought to be close, if not fully conformant, to the 0.9 version of the ORE spec.   I’m not sure the plugin is really useful; it’s just an Atom feed of all posts and pages in a WP instance.Â  I can think of some ways to make this more useful, by allowing blog authors to create their own aggregations, pulling in content outside of the particular instance.Â  I am certain that others can come up with even better uses.Â  I’m open to suggestions.   Thanks to Jay Datema for prodding me a bit, if indirectly.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/07/25/ore-plugin-updated/",
        "teaser": null
      },{
        "title": "Unescaping HTML in Python",
        "excerpt":"Dear Future Me,   You’ve forgotten how to decode (or unescape) HTML or XML in Python again, haven’t you?Â  My, my, that old age does catch up with you.   Well, it turns out that ï»¿xml.sax.saxutils.unescape() works like a charm.Â  I’m certain that edge cases lurk here and there, so caveat, um, coder.   UPDATE: Edge case found.  Note that unescape() will not work on &amp;apos; or &amp;quot;, and so there is:  xml.sax.saxutils.unescape(\"&lt;p&gt;This is &amp;quot;markup&amp;quot;&lt;/p&gt;\", {\"&amp;apos;\": \"'\", \"&amp;quot;\": '\"'})  ","categories": [],
        "tags": [],
        "url": "/blog/2008/08/01/unescaping-html-in-python/",
        "teaser": null
      },{
        "title": "Some shots from Alaska",
        "excerpt":"I don’t ordinarily post pictures around here but I am making an exception.Â  Elizabeth and I recently spent a week in Anchorage, AK, where my in-laws were gathered for a family reunion.Â  Eliz took a bunch of pictures and has uploaded a few to Flickr.   Boy, but it is gorgeous up there.Â  A fellow sure could get used to all those mountain vistas and free-range zucchinis (?!).  ","categories": [],
        "tags": [],
        "url": "/blog/2008/08/22/some-shots-from-alaska/",
        "teaser": null
      },{
        "title": "DRM for Librarians",
        "excerpt":"I know precious little about rights management.Â  And what I do know I have gleaned from the occasional Slashdot post or Wired article.Â  Former colleague Grace Agnew, Associate University Librarian for Digital Library Systems at Rutgers University, has put the wraps on a book about digital rights management targeted at librarians, Digital Rights Management: A Librarian’s Guide to Technology and Practice (also available in paperback):  This book provides an overview of the current landscape in digital rights management (DRM), including: an overview of terminology and issues facing libraries, plus an overview of the technology (including standards and off-the-shelf products). It discusses the role and implications of DRM for existing library services, such as integrated library management systems, electronic reserves, commercial database licenses, digital asset management systems and digital library repositories. It also discusses the impact that DRM â€˜trusted systemâ€™ technologies, already in use in complementary areas, such as course management systems and web-based digital media distribution, may have on libraries. It also discusses strategies for implementing DRM in libraries and archives for safeguarding intellectual property in the web environment. If you’re a librarian or information professional looking for an introduction to DRM, an underpinning for rights management strategy, or a refresher on rights management technologies, you might consider checking it out.   For full disclosure, I was one of several reviewers of this book.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/09/08/drm-for-librarians/",
        "teaser": null
      },{
        "title": "The so-called \"bailout bill\"",
        "excerpt":"Dear elected representatives,   I write you as a voting constituent outraged by the possibility of Congress passing the so-called “bailout bill” put forward by Messrs. Bush, Bernanke, and Paulson.Â  I implore you, as my representative, to weigh carefully the options before you.   A vote to pass this bill is a vote of confidence in the George W. Bush administration and future administrations, whomever they may be.   It is a vote that abdicates Congress’s constitutional duty to oversee the acts of the executive branch and provide checks against imbalances and abuses of power.   One cannot honestly decry the actions of the Bush administration, as Democrats have for nearly eight years, and then hand them the keys to the economy – not to mention $700B of hard-earned taxpayer money – merely because it is politically expedient.Â  There are greater ills than inaction.   Passages such as the following are reason enough to reject this plan outright.  Decisions by the Secretary pursuant to the authority of this Act are non-reviewable and committed to agency discretion, and may not be reviewed by any court of law or any administrative agency. This is a constitutional travesty.   The plan itself is a shot in the dark; Messrs. Paulson and Bernanke themselves testified that it may or may not work, and that the $700B amount is just an estimate.Â  They may in fact need to appropriate yet more taxpayer money to bail out corrupt and incompetent investors.Â  With language like the above, Congress may be powerless to stop them, and by their own hand no less.Â  Most worrying is that when Congress lacks the power, so does the citizenry.   If Congress passes this bill and grants the executive branch the powers described within, the American people will have no legal recourse to stop the Department of the Treasury and the Federal Reserve from squandering their wealth.   I urge you, sirs and madams, to vote against any plan that would strip Congress of its oversight responsibility.Â  For this is your constitutional duty and is a key mechanism by which our republic functions.Â  When this duty is removed from the legislative branch, members of whom are elected directly by the people and serve at our pleasure, the government ceases to function as it was intended.   I cannot in good conscience support any member of Congress who would break his oath to support and defend the Constitution, and I, like most Americans, do vote my conscience.   P.S. Sorry for all the bold.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/09/25/the-so-called-bailout-bill/",
        "teaser": null
      },{
        "title": "Bash, For loop, Files with spaces",
        "excerpt":"Dear Future Me,   Are you trying to iterate over filenames with spaces in them using a bash ‘for’ loop?Â  And instead of iterating over the filenames, you wind up seeing a list of filename parts split by said spaces?Â  Use case: you want to print out a list of unique extensions for all files in the current directory and below:       for file in `find . -type f`; do echo ${file##*.}; done | sort | uniq   If any filenames have spaces in them, you may see odd results.Â   The answer?Â  Set the input field separator (IFS) environment variable to the newline character (rather than the default space character):       export IFS=$'\\n'   And voila.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/09/27/bash-for-loop-files-with-spaces/",
        "teaser": null
      },{
        "title": "Not to be dramatic",
        "excerpt":"   That to secure these rights [of Life, Liberty, and the Pursuit of Happiness], Governments are instituted among Men, deriving their just powers from the consent of the governed, … That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.    Just saying1.                  Okay, yes, totally dramatic. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2008/10/02/not-to-be-dramatic/",
        "teaser": null
      },{
        "title": "Call for Proposals - code4lib 2009 conference",
        "excerpt":"Hear ye, hear ye, techie librarians and library techies:   Birkin James Diana, representing the host, Brown University, announces a call for proposals for talks at the code4lib 2009 conference:   The code4lib conference is wonderful in large part because of what you folk share.  Head over to the Call For Proposals page and submit your idea for a prepared talk. Information about the time-frame, scope, and evaluation process -- as well as the link to the submission form -- can be found at:  http://library.brown.edu/code4libcon09/proposals/  Some important notes: - Proposals can be submitted through Sunday November 23 2008. - Just like when you vote, use your code4lib username and password. - You initially won't be able to edit your proposal(s), but I'll have that implemented in the near future.   More information on prepared talks from the proposal page:    Prepared talks are 20 minutes, and must focus on one or more of the following areas:      * \"tools\" (some cool new software, software library or integration platform)     * \"specs\" (how to get the most out of some protocols, or proposals for new ones)     * \"challenges\" (one or more big problems we should collectively address)  The community will vote on proposals using the criteria of:      * usefulness     * newness     * geekiness     * diversity of topics  We cannot accept every prepared talk proposal, but multiple lightning talk sessions should provide everyone who wishes to present with an opportunity to do so.  Do consider submitting a talk if any of that sounds interesting to you.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/10/23/call-for-proposals-code4lib-2009-conference/",
        "teaser": null
      },{
        "title": "Is John McCain a socialist?",
        "excerpt":"Central to the McCain/Palin campaign’s rhetoric lately has been the allegation that Barack Obama is a socialist (which, sadly, is something of a four-letter word).  Their evidence: Obama’s encounter with the now famous “Joe the Plumber,” wherein Obama explained to Joe that the point of his economic plan, and by extension the progressive tax and the liberal welfare state, was to help bring up those “behind” Joe.  Obama’s misstep was using the (honest) phrase, “spread the wealth around.”  McCain has used this soundbite to justify labeling Obama with the scarlet letter ‘S.’   Yet John McCain embraces the same “socialist” principle:   &lt;/param&gt;&lt;/param&gt;&lt;/embed&gt;  Here is the relevant soundbite:&lt;blockquote&gt;When you reach a certain level of comfort, thereâ€™s nothing wrong with paying somewhat more.&lt;/blockquote&gt;   If John McCain believes Barack Obama is a socialist, then he too is a socialist – it’s the very same principle.  (For the record, I don’t believe either is a socialist.  And I believe soundbite politics insults our intelligence.)   I tip my hat to Daniel Miessler for posting about this.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/10/24/is-john-mccain-a-socialist/",
        "teaser": null
      },{
        "title": "Convert Windows shortcuts into Ubuntu shortcuts",
        "excerpt":"[Update: Feel free to grab the code via bzr with bzr branch http://lackoftalent.org/bzr/shortcut_converter.]   Here’s another entry in the “dumb little scripts that work for me and may or may not be helpful to other folks” department…   I use both Windows and Ubuntu at home, gradually transitioning from the former to the latter.  I’ve accumulated a bunch of Windows URL shortcuts, mostly things I wanted to read once so instead of bookmarking them, I dragged their links to my desktop.  This creates .URL files which are simple little plain-text two-liners.  It turns out that on Ubuntu, and probably similar *nix systems, web shortcuts are also simple little plain-text files.  These files have the .desktop extension (though you won’t see the extension by looking at the desktop).   I wanted a way to convert my .URL files to .desktop files so that I can just toss them on my Ubuntu desktop and double-click them the same way I would if I were on Windows.  This cruddy little Python script does the trick.    #!/usr/bin/env python # shortcut_converter.py  from __future__ import with_statement import os.path import sys  TEMPLATE = \"\"\"[Desktop Entry] Version=1.0 Encoding=UTF-8 Name=%(basename)s Type=Link URL=%(url)s Icon=gnome-fs-bookmark \"\"\"  def convert(f):     \"\"\" Takes a full filepath to a .URL file, converts it to a .desktop file         in the same directory \"\"\"     print \"Converting %s\" % f     (filepath, filename) = os.path.split(f)     (basename, extension) = os.path.splitext(filename)     with open(f) as urlfile:         lines = [line.strip() for line in urlfile.readlines()]     url = lines[1].split('URL=')[1]     dtfname = os.path.join(filepath, '%s.desktop' % basename)     with open(dtfname, 'w') as dtfile:         print \"Writing %s\" % dtfile.name         dtfile.write(TEMPLATE % locals())  if __name__ == '__main__':     for arg in sys.argv[1:]:         if os.path.isfile(arg) and arg[-3:].lower() == 'url':             convert(arg)         else:             print \"*** %s is not a URL file\" % arg   I used scp to pull over all my .URL files and then invoked the script thusly:   python shortcut_converter.py *.URL   worksforme!  ","categories": [],
        "tags": [],
        "url": "/blog/2008/10/29/convert-windows-shortcuts-into-ubuntu-shortcuts/",
        "teaser": null
      },{
        "title": "Plugin updates",
        "excerpt":"I finally pushed out some embarrassingly outdated WordPress plugin updates a few moments ago.    \tUpdated unAPI plugin with a patch contributed by Jay Luker that removes the hard-coded \"wp_\" table prefix.  The updated version of the plugin has been tagged as 1.4.1. \tUpdated LinkPURL plugin with a patch contributed by Mark Matienzo that enables partial redirects.  I made some additional tweaks to the plugin to make this feature configurable via the WordPress management UI.  This has been tagged as 1.1. \tCreated a new unAPI plugin branch for Mark Matienzo's Scriblio-oriented tweaks.  The branch is called 1.4.1-anarchivist-scriblio and it contains the scriblio.diff file.  I have yet to integrate the diffs, as the file that was patched has changed since the patch was issued.  If anyone is interested in working on unAPI/Scriblio integration, please get in touch with me.   And here is my to-do list which I hope will keep me honest.    \tUpdate OAI-ORE plugin to support version 1.0 of the ORE specification. \tAdd per-post (and per-page?) resource maps that wrap all embedded images and links. \tEnable \"cool URIs\" for all resource maps.   It is my hope that I’ll get to those sometime before the summer begins.  :)  ","categories": [],
        "tags": [],
        "url": "/blog/2008/11/16/plugin-updates/",
        "teaser": null
      },{
        "title": "JSON and the Blarghonauts, or, Firefox and Pretty-Printing FAIL",
        "excerpt":"There must be a better way of viewing pretty-printed JSON from Firefox than this.  (EDIT: Hail, JSONovich!)    #!/usr/bin/env python # ~/bin/jsonhandler.py # Take some JSON from a file or stdin, format it, output to a tempfile, # open in EDITOR  from __future__ import with_statement import os import sys import simplejson import tempfile  EDITOR = \"/usr/bin/gedit\"  if __name__ == \"__main__\":     if len(sys.argv) == 2:         # if invoked as jsonhandler.py {FILE}         json = open(sys.argv[1])     else:         # if JSON is piped in (e.g., from Firefox,         # or cat {FILE} | jsonhandler.py)         json = sys.stdin     json = simplejson.load(json)     # the with_statement is kind of gratuitous but I like it     with open(tempfile.mktemp('.json'), 'w') as jsonfile:         simplejson.dump(json, jsonfile, indent=4)     # all of that and gedit doesn't even highlight JSON     # I have emacs highlighting JSON but this generates a \"stdin is not     # a tty\" error, so EDITOR is not set to emacs     # xemacs works a little better, but I need to click:     # \"Options &gt; Syntax Highlighting &gt; In this buffer\" every time, despite     # saving to custom.el, so EDITOR is not set to xemacs     # Very annoying!     os.system(\"%s %s\" % (EDITOR, jsonfile.name))                  And then I set ~/bin/jsonhandler.py as the action for application/json in Edit       Preferences       Applications.           Yuck.  Help?  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/02/json-and-the-blarghonauts-or-firefox-and-pretty-printing-fail/",
        "teaser": null
      },{
        "title": "Introducing JSONovich",
        "excerpt":"JSONovich is a Firefox extension that pretty-prints and colorizes JSON.   Feedback is welcome.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/12/introducing-jsonovich/",
        "teaser": null
      },{
        "title": "Burn the Walled Gardens",
        "excerpt":"Issue five of the code4lib journal is out.  This issue looks to be just as good as the past four issues, but I’d like to highlight one article in particular: the column by Kansas State’s Web Development Librarian, Dale Askey: We Love Open Source Software.  No, You Can’t Have Our Code.      Librarians are among the strongest proponents of open source software. Paradoxically, libraries are also among the least likely to actively contribute their code to open source projects. This article identifies and discusses six main reasons this dichotomy exists and offers ways to get around them.    If you’re at a library doing open source software, or if you think you’re doing open source software, or if you’re considering jumping into the fray, you would do yourself, your institution, your users, and the open source community (whatever the heck that is) a great, great service by reading this column.  Srsly.   I’ve worked in academic libraries where open source was given lip service as well, so this resonates with my experience.   If you’re doing open source, or think you’re doing open source, it is necessary that you release code to the public under an open source license.  The public includes your institution, your partner institutions, all of academia, and everyone else; if you make code available to partners only, that’s not open source; that’s multi-institutional closed source.  It’s a walled garden with teleportation devices leading to other walled gardens. While we love hearing about your code, it’d provide even more value if we could read and run your code—and contribute some back to you.   Releasing code is necessary to claim you’re doing open source, yes, but it is not sufficient.  There is some value in just throwing code over the wall.  Sure, once the code is out there you’ve satisfied a definition or two and you can go off and pat yourself on the back and do the happy Ewok dance and maybe some more grant funds will come your way.  But if you want to add value to your involvement in open source, and add value to user-facing services built upon open source software, and add value for the vast community of open source developers champing at the bit to get at your code and make it better and work with you towards crafting a shinier, happier world. Avoid the tempation to “just fork it because not invented here.”  Take commits from the world.  There’s even more value in open source.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/15/burn-the-walled-gardens/",
        "teaser": null
      },{
        "title": "HOWTO: Get Twhirl 0.8.7 working on Ubuntu",
        "excerpt":"I use the Adobe AIR-based Twhirl as a Twitter and identi.ca client on my Ubuntu box.  Twitter recently made some changes to their authentication API, apparently, which prevented Twhirl from connecting as of version 0.8.6.  The fine folks over at Twhirl pushed out 0.8.7 in a jiffy but it included some AIR 1.5 dependencies.  The problem was that Adobe AIR for Linux only comes in 1.0 and 1.1 versions.   I was a tad frustrated that such a seemingly minor Twitter API upgrade resulted in a fundamentally different (and broken) version of Twhirl, but I couldn’t fault them for trying to respond quickly.  I followed the twhirl user on Twitter when this went down, and I was pleased to find out that they’ve whipped up a special AIR 1.1 Twhirl client for us Linux users.   I ran into some problems trying to install the client.  The first problem I ran into was due to a corrupt download.  If the file is less than 997K or so, you should try to download it again.  If Firefox fails you, there’s always wget.  Want to make sure you’ve got a good file?  Run unzip against it (an .air file is a .zip file underneath its raincoat).  If it succeeds, you’re golden.   The other problem was an old version of Adobe AIR.  You want adobeair_linux_b1_091508.bin installed, not adobeair_linux_a1_033108.bin.  Here’s how you “upgrade” AIR from the alpha to the beta (1.1) and get Twhirl 0.8.7-air11 installed:   sudo adobeair_linux_a1_033108.bin -uninstall sudo apt-get remove adobeair-enu (This step was not necessary on another box I tested.  It could be that the original box I tried these steps on was munted up.) This may be optional!  Clean out /opt/Adobe Air/, ~/.adobe/AIR/, and /root/.adobe/AIR/.  Note that this step will wipe your settings for all your AIR applications. Download the AIR 1.1 beta for Linux adobeair_linux_b1_091508.bin  (Some report that Firefox and other browsers must be closed during this step, but I couldn't reproduce that.) Download the latest AIR 1.1 Twhirl for Linux Then navigate to the twhirl-0.8.7-air11.air via Nautilus and double-click it. &lt;/ol&gt; Worked for me, at least.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/16/howto-get-twhirl-087-working-on-ubuntu/",
        "teaser": null
      },{
        "title": "JSONovich in the sandbox",
        "excerpt":"JSONovich is now in the “sandbox” over at addons.mozilla.org, where it will remain until it’s been tested a bit more, and rated and reviewed by users.  Until that point, it will be marked as “experimental” and will require users to login before they can download it.  If any of you would like to give JSONovich a quick spin and rate/review it over at the Mozilla add-ons site, that would be solid.  Once it’s gotten a few reviews and I’m more comfortable about it working cross-platform and cross-version, I’ll nominate it to be promoted.   Here’s where it lives: https://addons.mozilla.org/en-US/firefox/addon/10122   Much obliged, folks.  And thanks to those of you who have already downloaded it, installed it, tested it, left comments, or some combination thereof.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/23/jsonovich-in-the-sandbox/",
        "teaser": null
      },{
        "title": "Molotovs away!",
        "excerpt":"Lest I be criticized for unfairly calling out former employers in my recent Burn the Walled Gardens rant, I share news that the Rutgers University Libraries have boldly ventured into the world of open source software: RUcore Open Source Development.  Huzzah!  Thanks to the molotov-hurling Shaun Ellis, a peacenik/code monkey/musician extraordinaire, for all of his work and for bringing this to my attention.   On the RUcore open source page you can get a list of ongoing projects, a release schedule, and a rationale for their licensing decisions (i.e., choosing GPL 3).   The first project to be released (as of 2008/12/19) is the METS-based bibliographic utility, OpenMIC:&lt;blockquote&gt; OpenMIC is an open source, web-based cataloging tool that can be used as a standalone application or integrated with other repository architectures by a wide range of organizations. It provides a complete metadata creation system for analog and digital materials, with services to export these metadata in standard formats.        Low overhead and infrastructure requirements       Events-based model for management and rights documentation       Mapping and import from standard and in-house formats       Unicode and CJK vernacular character support   OpenMIC is a core application for the Moving Image Collections (MIC) initiative developed at the Rutgers University Libraries with funding from the Library of Congress. &lt;/blockquote&gt;   I look forward to following along as Rutgers releases yet more of the tools they have developed as part of their impressive digital library infrastructure.  It will be even more interesting to hear what their model will be for taking patches / commits from the broader open source community.  These things do take time, even though I failed to show an appreciation for that in my original rant, but I am reminded (by Jonathan Rochkind) that it’s better to take the time and get it right.  I cringe a bit to say that, knowing full well how things tend to languish in committees and fall victim to analysis paralysis in academia; surely there is some middle ground?  There are some very talented and experienced folks at Rutgers, so I will be excited to see them take a leadership role in this space.   Go, Scarlet Knights!  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/23/molotovs-away/",
        "teaser": null
      },{
        "title": "JSONovich update",
        "excerpt":"JSONovich is now up to version 0.6.  Recent revisions have added the following functionality:     Reads in JSON and converts to UTF-8 for some naive Unicode handling   Wraps long lines at the right edge of the window   Adds a check to see if a native JSON parsing library is already loaded (as will be the case in Firefox 3.1). Uses that library if so, otherwise loads the module included in JSONovich.   Handles JSON syntax errors more gracefully. Used to eat bad data and display nothing, but syntax errors (from the JSON parser) are now surfaced.  I’ve also tossed the source up on code.google.com for version control.   In the meantime, those of you who are using JSONovich can help increase its exposure by heading over to its entry at addons.mozilla.org, logging in, downloading, rating, and reviewing the extension.  Reviews and ratings help get extensions “promoted” from the sandbox to the public site, which provides the ability for automatic updates when new versions of the extension are released.  ","categories": [],
        "tags": [],
        "url": "/blog/2008/12/29/jsonovich-update/",
        "teaser": null
      },{
        "title": "Lynx and HTTPS/SSL on Ubuntu (8.10)",
        "excerpt":"Dear Future Me,   It has been a while, hasn’t it?  Yes, it has1.   Did you try to view an HTTPS/SSL URL in Lynx2 again, only to be met, most cruelly, with the following error message?      $ lynx https://example.org/resource/3     Alert!: This client does not contain support for HTTPS URLs.    Well, have no fear!  The lynx package, at least within the aptitude repositories for Ubuntu 8.10 (Intrepid Ibex), has no SSL support as you have just witnessed.  The lynx-cur package, on the other hand4, does!  Support SSL, that is.  Fix yourself thusly:      $ sudo apt-get install lynx-cur    N.B. the new lynx looks for its configuration in a different place than the old lynx, so you may need to fiddle with things if you’ve tricked out lynx with bells and whistles and racing stripes and nitrous boosts.  Otherwise, huzzah!                  In the future you will have evolved beyond answering your own questions, no doubt, but here in the past, in this quaint and backwards era, it is quite common to hold conversations with yourself.  Or myself.  But I (i.e., you) digress!  (We digress in the past as well!  Quite the confusing state of affairs, conversationally speaking!) &#8617;                  Do they even have Lynx in that brave new world of the future?  Does the lynx species still exist?  Did the polar ice caps melt and wipe out all non-domesticated felines?  Inquiring, unevolved minds of the past want to know! &#8617;                  I am assuming that in the future example.org remains a reserved dummy domain. &#8617;                  I hear that in the future hands will be replaced by hooks and detachable chainsaws and the like? &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/01/12/lynx-and-httpsssl-on-ubuntu-810/",
        "teaser": null
      },{
        "title": "And what rough beast",
        "excerpt":"   Lack of Talent, its hour come round at last, slouches towards podcastdom to be born.   It’s late and I’m tired, so here’s the skinny: at the beginning of the year I ambitiously1 resolved to record one song per month.  Instead I’ve serendipitously turned up the LOT recording sessions from July 2005, which we call the Burlap Overseas, and which I never really went through.  Now that stuff is backed up six ways from Sunday2 and I owe it to my fellow Lack of Talenteers to go through all these hours of raw audio and pick out some interesting bits so we have something to build on the next time we get together3.   If you’re interested, feel free to subscribe to the podcast and listen along.  Updates will be sporadic, perhaps even spasmodic or spastic.  Now the disclaimers: you should know that LOT was never about songs or practice or technique or order or music, really; we are true to our name4; it’s about some friends gathering in my grandparents’ basement with lots of music gear5, even more alcohol, a box fan to keep us cool, a washing machine to clean Gramma’s clothes, and a microphone that runs the whole darned time.  This is booze-soaked sonic experimentation among close friends and even if you hear nothing else, you should hear hints (or squeals, or yawps) of joy amidst the cacophony.                  Read: foolishly &#8617;                  Phew.  I am a paranoid backup freak. &#8617;                  Est. 2013 &#8617;                  At least I am &#8617;                  We switched instruments often, sometimes even playing the ones we could “play.” &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/01/28/and-what-rough-beast/",
        "teaser": null
      },{
        "title": "Cataloging and institutional repositories",
        "excerpt":"While doing some reading for a little talk my colleague, Ed Summers, and I are giving at code4lib 2009, I came across a paragraph that sparked a crazy thought.  So crazy that it’s not crazy at all.  So not crazy that I am sure other people have thought of it.  But nonetheless, here I am writing about it just in case.   From Sarah Currier’s paper on SWORD (emphasis mine):&lt;blockquote&gt;One of the most frequently cited barriers to academics depositing their teaching materials into repositories is the keystroke-count involved in logging into a repository, uploading the resource, creating metadata, perhaps selecting a licence, and publishing the resource. It was a quick win, therefore, to create a drag-and-drop desktop tool to allow a single keystroke deposit of resources, including multiple resources in one action. For a repository that supports automatic metadata generation, administrative metadata can be created at the point of entry to the repository without the user needing to create any.&lt;/blockquote&gt;   And I wondered how many repositories supported automatic metadata generation.  I wondered how many repositories supported automatic generation of rich metadata.  And lastly I wondered, might this be a more or less natural role for catalogers: augmenting stub metadata records or doing original cataloging for institutional repository deposits?  Especially at a time when many of them are being reclassified as acquisitions specialists or digital projects managers?   Potential issues and questions:&lt;ul&gt; \t&lt;li&gt;Author ignorance: Maybe catalogers are already doing this and I’m a moron?&lt;/li&gt; \t&lt;li&gt;Scale: Is it realistic to expect to be able to “keep up” with repository deposits?&lt;/li&gt; \t&lt;li&gt;Granularity: Does cataloging at the level of articles, and perhaps at even finer granularities, introduce challenges?&lt;/li&gt; \t&lt;li&gt;Duplication: If pre-prints are cataloged in the IR, for instance, will they need to be cataloged again later?&lt;/li&gt; \t&lt;li&gt;… there are others I thought of on my commute this morning but have since forgotten them.  Feel free to add comments.&lt;/li&gt; &lt;/ul&gt;   I will admit here that I’ve been somewhat out of the (academic) institutional repository space a while, and cataloging is something I don’t share thoughts about very often because my exposure is limited to having taken one course a couple years ago.   I assume there’s a body of research about this out there somewhere but I figured I’d post this anyway.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/02/09/cataloging-and-institutional-repositories/",
        "teaser": null
      },{
        "title": "Rutgers SCILS: What's in a name?",
        "excerpt":"Former colleague Trevor Dawes has written a thorough piece about a name change proposed by the faculty of Rutgers’ School of Communication, Information and Library Studies (SCILS).  They have voted on and approved a new name, School of Communication and Information, and it is now awaiting approval from the Board of Governors.   Trevor received e-mail from a current SCILS faculty member after getting involved in a discussion of the name change on a listserv.  I find part of that e-mail1, specifically the rationale for the name change, absolutely puzzling:&lt;blockquote&gt;We just have so many programs now – we can’t possibly cover all of them in our school’s name. School of Communication and Information is something of a compromise name, but it does encompass all our departments and programs in the school. &lt;/blockquote&gt;   So in order to cover more programs, the name of the school ought to communicate less?  Does dropping “Library Studies” somehow represent Journalism, Media Studies, and Informatics students more?   I fail to see how removing “Library Studies” makes the name of the school more meaningful.  Why not follow this rationale to its logical conclusion, then, and shorten the name to School of Information?  Or iSchool?  Or how about “School?”  Yes, that’s it, “School!”  Then all the departments and programs are equally well-represented.  Huzzah, faculty!   I should be clear about my objection.  I don’t mind SCILS becoming an iSchool.  In fact, I think my education there could have benefited from a more iSchoolish curriculum.  But any problems with the school then were not related to the name, and I doubt they are now.  What I object to is the oddball rationale for the name change, and the notion that in order to affect change and improve the school, well, clearly a change in name will do the trick!  It’s putting the cart before the horse, especially when the MLIS program lacks a core curriculum2. This is change in name only and that is perhaps a missed opportunity.                  Taken out of context, true. &#8617;                  An opportunity for real change, though I will admit that there are good arguments against having one. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/02/12/rutgers-scils-whats-in-a-name/",
        "teaser": null
      },{
        "title": "Ada Lovelace Day",
        "excerpt":"I confess: prior to today, I had never heard of Ada Lovelace.  A number of bloggers whom I follow wrote about Ms. Lovelace today, which is apparently Ada Lovelace Day: “an international day of blogging to draw attention to women excelling in technology.”   Inspired by their words, I thought I would say my piece as well.  And so, this being the first Ada Lovelace Day, I’d like to celebrate the woman who is most responsible for my own love of libraries1 and technology: my mother, Diane.  My mother is neither a technologist nor a mathematician, and I’m pretty sure she’s not comfortable in front of a Python interpreter.  She was an employee at Rutgers University’s Alexander Library during their first automation efforts in the ’70s, partly while I was in utero.  I like to think that library automation entered my bloodstream through osmosis back in 1973 and I’ve been working at this, well, not quite since then, but long enough.  More than that, she got me hooked on libraries many years ago through frequent trips to neighborhood libraries and also by including me, in snot-nosed kid form, in her genealogical research that took us to some rural Maryland libraries and, yes, the Library of Congress.  This thirst for knowledge (not to mention her constant and unwavering support for me despite the wacky paths I’ve chosen over the years) is why I celebrate my mother today.                  I realize Ada Lovelace Day is about technology, not about libraries, but I hope you’ll give me some slack. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/03/24/ada-lovelace-day/",
        "teaser": null
      },{
        "title": "State of the Me",
        "excerpt":"Has it really been two months?  Why, yes, it has.  Oh me, oh my.  I have tried to stick somewhat loosely to a schedule of writing here once a month1, but alas, April came and went and I simply made no time to write.   That’s not entirely true; I did plenty of writing:   I wrote code.  After a year of working on the World Digital Library project at $MPOW, we went live on April 21st.  The last few weeks were very busy for the development team, but I did find a few moments to breathe and blink.   I wrote microblog updates.  After months of trying to figure out what microblogging is all about2, it found its way into my daily routine.  When time is short or thoughts arise fast and fuzzy, microblogging is a useful public scratchpad.   I wrote slides.  The kind folks over at the College and University Section of the New Jersey Library Association invited me to be a panelist at the 2009 NJLA conference.  The panel addressed recentish developments in open source integrated library systems.  I spoke about the Evergreen ILS3 and my co-panelists spoke about Koha and the Open Library Environment Project.   And, ever the dutiful technologist, I wrote documentation.  And that will be the subject of my next post.                  Here I extend my hand and then imagine you, whomever you may be, smacking it ever so gently &#8617;                  Wondered: Is it IM?  Status updates?  Blogging?  And how is it related to these?  Concluded: it’s a little of each, and somehow it fits my status/vanity/sharing needs perfectly. &#8617;                  Hat tip to Equinox Software Inc.’s Karen G. Schneider for her kind assistance. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/05/18/state-of-the-me/",
        "teaser": null
      },{
        "title": "I2: Background",
        "excerpt":"[Series]   This is the first in a series of posts about institutional identifiers1.   In my last post, I alluded to some documentation that I’ve written.  That was somewhat misleading, which will soon be apparent, but I liked the parallel construction I had going, and I am but a slave to orderliness.   For about the past six months, I have been working with a NISO group looking into how institutions are identified within information systems:      The I2 (Institutional Identifiers – pronounced “I 2”) working group will build on work from the Journal Supply Chain Efficiency Improvement Pilot (http://www.journalsupplychain.com/), which demonstrated the improved efficiencies of using an institutional identifier in the journal supply chain. The NISO working group will develop a standard for an institutional identifier that can be implemented in all library and publishing environments. The standard will include definition of the metadata required to be collected with the identifier and what uses can be made of that metadata. …    The I2 group is split into a few subgroups which have been charged with looking into how institutional identifiers are used in particular scenarios.  These scenarios are e-resources, repositories and e-learning systems, and library resource workflows.  The scenario names pain me a bit, but so be it; this is our industry, and there are bigger windmills to tilt at.   I am currently co-chairing the subgroup looking at repositories and e-learning, and apparently I am its “tech lead.”  I don’t want to get caught up on names and roles and titles, though; this series isn’t about those at all.  I’m just setting the scene and explaining why my head’s in this space and laying bare my stake in the issue.   The remainder of this series will provide a bit more detail on the issues around institutional identifiers, share how the repository subgroup is grappling with identifier issues and engaging the repository community to assess needs, propose an approach for an identifier system that may meet said needs, and explore what seems to be the thorniest issue2.                  I offer that very tentatively, knowing what a spectacular failure my last attempt at a series was. &#8617;                  Hint: management.  I know, “duh,” right? &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/05/19/i2-background/",
        "teaser": null
      },{
        "title": "I2: Requirements",
        "excerpt":"[Series]   The I2 IR scenario subgroup approached the issue of institutional identifiers in repositories by first brainstorming about the various issues, problems, and sticking points that make identifiers in this space (and elsewhere) such a complex topic.  Folks on the subgroup are repository managers or are otherwise involved with or knowledgeable about the repository space, so the brainstorming exercise yielded a good number of concerns.   The purpose of the exercise was to enumerate concerns and issues that could inform a draft survey to be administered to repository managers and experts around the globe in different organizational contexts: libraries, subject disciplines, archives, historical societies, etc.  The purpose of the survey is to get an idea of the use cases and constraints around institutional identifiers in these different repository contexts, the assumption being that we ought to have requirements grounded in real world usage before we go off building a standard.   I will note here that the subgroup has worked up a draft survey that has just recently been reviewed by a small group of folks who know about survey design, and we hope to administer the survey to the aforementioned Reporati this week1.  Which is to say that I don’t yet have a strong grasp of the use cases out there in the wild, and this series should be construed as my own premature cognitive fumblings.  But let’s assume for now that what we learn from the survey results matches our initial brainstorming exercise.   Here is a slightly modified and boiled down version of the concerns and issues the subgroup came up with for a potential institutional identifier standard, which resembles a set of minimum requirements:    \tShould be agnostic to type of institution, e.g., libraries, museums, personal collections, historical societies \tShould handle varying institutional granularity, e.g., institution-level, campus-level, division-level, unit-level \tShould handle linking among institutions and subordinate units \tShould express different sorts of relationships among these institutions and units \tShould relate to existing relevant identifiers and registries \tShould be globally unique \tShould be actionable \tShould enable retrieval of metadata sufficient to identify the institution, which may vary widely by institution \tShould accommodate changes as institutions come and go and re-organize and be able to relate defunct institutions to new ones   I doubt the list is exhaustive; I am almost certain we will uncover all sorts of tangly and esoteric use cases that add requirements.  I expect it.  Why else would we be gathering to discuss the need for an institutional identifier if it were a solved problem or a simple one?2   Nevertheless, looking at the above list, the task we’ve taken on starts to feel less onerous.  And thinking about identifier systems constrained by the list of concerns, the mind starts to cook up all sorts of possible solutions.  I’ll share one in the next post in this series, a strawman proposal of sorts, and how it addresses each of these requirements.                  We will also x-post to repo-related mailing lists as well, and some of us may blog or tweet about it.  My inclination is to cast as wide a net as possible so as not to miss important use cases.  We can always scope things out later on, but it’s useful to be inclusive at this point lest our own assumptions carry the group forward. &#8617;                  The cynical among you might have interesting answers to this question. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/06/07/i2-requirements/",
        "teaser": null
      },{
        "title": "I2: Strawman",
        "excerpt":"  [Series]  In the prior I2 post, I wrote about the requirements the repositories subgroup has come up with for an institutional identifier standard (with the hope that our findings re: repositories could be generalized to other scenarios).      Image by PhOtOnQuAnTiQuE via Flickr   My strawman proposal of sorts is to explore how well linked data patterns fit this problem space.  Linked data, briefly, is a way to expose and link data on the web in a more semantically meaningful way, and is often summarized using the four principles put forward by Tim Berners-Lee:     \tUse URIs as names for things \tUse HTTP URIs so that people can look up those names. \tWhen someone looks up a URI, provide useful information. \tInclude links to other URIs. so that they can discover more things.   That's the crux of it.Â  Linked data takes well-known patterns on the web (linking, dereferencing, etc.) and applies them to data, which in this case could be metadata for identifying institutions.  Let's examine each of the requirements and the applicability of linked data thereto.    \tShould be agnostic to type of institution, e.g., libraries, museums, personal collections, historical societies: The web is already agnostic to type of institution.Â  HTTP URIs do not favor one type of institution over another.  \tShould handle varying institutional granularity, e.g., institution-level, campus-level, division-level, unit-level: HTTP URIs are flexible in this regard.Â  Hierarchy, should one wish it to be surfaced in the identifier, may be encoded in either a DNS hostname or the path appended to the DNS name.Â  One can imagine a URI like \"http://department.division.institution.tld/unit/subunit\" or \"http://institution.tld/campus/office/individual\".   Hierarchy needn't be surfaced in the identifier if one favors opacity, in which case \"http://registry.tld/xnjsdasd\" would suffice as an identifier, and may instead be entirely reflected in the (RDF) representation returned by dereferencing the URI.  \tShould handle linking among institutions and subordinate units: Linked data handles linking via well-known HTTP mechanisms, referenced in the fourth principle of linked data.Â  Unlike the HTTP link, which has limited semantics, linked data links are semantically rich and extensible.  \tShould express different sorts of relationships among these institutions and units: The \"useful information\" in the third principle of linked data is typically provided by an RDF representation, which is itself a list of assertions.Â  These assertions, or triples, consist of subjects, predicates, and objects.Â  The ability to express the relationships in this requirement is limited only by the availability of vocabularies that contain sets of predicates and classes for subjects and objects.Â  Think of the predicates as elements defined within a metadata standard, e.g., Dublin Core \"creator\", MODS \"relatedItem\", and so forth.Â  Vocabularies that contain these predicates and classes are growing and evolving daily, and should there not be a vocabulary that contains the relationship one wishes to express, it is fairly easy to create a custom vocabulary.   The ability to mix and match vocabularies provides an expressiveness that is often not found in document-based metadata formats and the flexibility to express radically different relationships on a per-industry or per-institution basis.Â  This latter point is important as the I2 group has identified both core metadata elements for identifying institutions of different types and additional elements for specific types of institutions.Â  Why re-invent a new metadata format or schema when all one needs to express may already be contained in others?  \tShould relate to existing relevant identifiers and registries: Same as requirement#4.Â  Linked data is all about expressing relationships between things, e.g., institutions, identifiers, registries, etc.  \tShould be globally unique: HTTP URIs are guaranteed to be globally unique by virtue of the distributed DNS system and hierarchical naming within each HTTP service.  \tShould be actionable: HTTP URIs provide dereferenceability/actionability via the well-known HTTP protocol.  \tShould enable retrieval of metadata sufficient to identify the institution, which may vary widely by institution: HTTP URIs are actionable per requirement #7 and the metadata returned is flexible per requirement #4.  \tShould accommodate changes as institutions come and go and re-organize and be able to relate defunct institutions to new ones: Linked data patterns provide for redirecting from defunct representations (institutional identifiers) to new ones via HTTP redirects.Â  One may also add assertions to institutional metadata such as owl:sameAs, for instance, which says that the institution identified by the given URI is the same as another institution identified by another URI.   This seems like a compelling path to follow for the I2 standard.   The I2 repositories subgroup will be sending out its survey on identifier use cases in the coming week.Â  It will be interesting to see if the requirements we have thus far identified still obtain in light of the data we collect from the survey.Â  If so, I would like to explore the idea of linked data for institutional identifiers a bit more.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/06/13/i2-strawman/",
        "teaser": null
      },{
        "title": "I2: Survey ",
        "excerpt":"[Series]   Near the end of my strawman post, I wrote:&lt;blockquote&gt;The I2 repositories subgroup will be sending out its survey on identifier use cases in the coming week.  It will be interesting to see if the requirements we have thus far identified still obtain in light of the data we collect from the survey. &lt;/blockquote&gt;   We completed the survey late last week and began distributing it.  Here’s what we sent out:&lt;blockquote&gt;&lt;p&gt;The NISO I2 Working Group is surveying repository managers to determine the current practices and needs of the repository community regarding institutional identifiers.  We value your time and your input in the process to create a standard for a new institutional identifier.  We hope that you will complete the survey which should take less than 15 minutes.  The survey will remain open through Monday, July 6th.&lt;/p&gt;   Here is a link to the survey: http://www.surveymonkey.com/s.aspx?sm=RGQgZ3090DVrb3kFzr3P3Q_3d_3d   Please feel free to share this message with other interested parties.  &lt;/blockquote&gt;   First we used Survey Monkey to send the survey link to approximately one-hundred repository managers that the subgroup identified.   Our process for identifying repository managers involved pulling together a list of prominent repositories from subgroup members, and then gathering more from OpenDOAR, “an authoritative directory of academic open access repositories.”  Then subgroup members were encouraged to share the survey link with colleagues, and post it far and wide via blogs, listservs, and tweets.  The listservs we targeted were: JISC-REPOSITORIES, metadataLibrarians, digital-curation, SPARC-IR, ir-net, REPOMAN-L, PALINET-IR-L, dspace-general, fedora-commons-users, DC-IDENTIFIERS, and code4lib.   I’ve already received a few responses and have gotten useful feedback.  Two of the hardest questions to answer so far have been: “What is an institutional identifier?” and “What is a repository?”   Institutional identifier An institutional identifier is defined as a symbol or code that uniquely identifies an institution.  Domain-specific examples of existing identifiers include SAN, IPEDS, GLN, MARC Org Code, and ISIL.  Another example might be a Handle prefix or ARK name authority assigning number.  Repository Institutional repositories and subject repositories like arxiv.org are clearly 'repositories', but beyond that it is a somewhat ill-defined term.  One might look to the Kahn-Wilensky architecture, or the OAIS reference model (PDF), or even Wikipedia for definitions, but it's not clear that even the authorities agree on what constitutes a repository. It's a system.  It's network-accessible and typically has a web interface of some sort.  Files and groups of files sometimes known as objects tend to be deposited in them, perhaps for some combination of management, access, or preservation.  Many run Fedora, DSpace, and ePrints, and factor heavily in scholarly communication.  Some are document-centric.  Some will accept anything.  To some, a learning management system may be a repo.  To others, a content management system may fit.  My background is in academia so my own definition is somewhat based in that context, but I wouldn't say the term is necessarily limited to that context.  There are other NISO I2 scenarios for library workflows and electronic resources, so it's safe to assume that repository does not mean ILS or OPAC or ERP system.  My hope is that folks have their own working definitions of the term and can decide for themselves what it means.    We’ve given folks a little over two weeks to respond to the survey, so the constant I2 drum-beating will quiet down for a while around here.  I am very interested in what sorts of responses we get from the survey.  Fun times!   Oh, and perhaps it goes without saying, but if you’re a repository owner, manager, expert, developer, or stakeholder with an interest in identifiers, please feel free to take the survey!  ","categories": [],
        "tags": [],
        "url": "/blog/2009/06/20/i2-survey/",
        "teaser": null
      },{
        "title": "WDL metadata mapping, and, parsing TEI in Python",
        "excerpt":"Context   Early on in the effort to develop the first public version of the World Digital Library web application, we developed a (non-public) Django-based cataloging application where Library of Congress catalogers could manage metadata for WDL items.  Management in this sense includes creation of records, editing of records, versioning of edits, mapping of source records, and some light workflow for assignment of records to individual catalogers and for hooking into translation processes1.   I worked primarily on the source record mapping tools.  They take a number of formats as input and are called by the cataloging application to map metadata from these formats into the WDL domain model.  Several though not all of which are XML-based, and thus easily dealt with in Python, via the etree module in the lxml package.   Dan recently kicked off a new R&amp;D project for evaluating (any) metadata against any number of metadata profiles, mapping into a generic data dictionary, the goal being to determine how feasible it would be to develop a toolset for aiding remediation of metadata across any number of digital collections.  I have been working on this project with Dan, and got started by seeing how generalizable the WDL metadata mapping tools are.  Turns out they’re fairly generalizable once you tweak the various format-specific mapping rules to map into the generic data dictionary model rather than the WDL model (around 15 elements, and somewhere between Dublin Core and MODS in terms of specificity but flatly structured like DC).   Some of the test data I am working with now, that has nothing to do with WDL, is SGML-based TEI 2 markup.  The closest I worked with on WDL was TEI P5 for manuscript description which is serialized in XML.  Turns out my TEI mapping rules from before blew up on this TEI 2 stuff, as lxml.etree (naturally) wasn’t digging the non-XML input.  I googled around a bit for how best to parse TEI (or any SGML) in Python and then discovered it’s actually simple as pie.   Code   If you’ve got the BeautifulSoup module installed2:   &gt;&gt;&gt; from BeautifulSoup import BeautifulSoup &gt;&gt;&gt; tei = open('foo.sgm').read() &gt;&gt;&gt; BeautifulSoup(tei).findAll('title')[0].string u'[Memorandum to Dr. Botkin]: a machine readable transcription.'   If not, the lxml.html module works too:   &gt;&gt;&gt; from lxml import html &gt;&gt;&gt; h = html.parse(open('foo.sgm')) &gt;&gt;&gt; h.xpath('//title')[0].text '[Memorandum to Dr. Botkin]: a machine readable transcription.'   Data   And here’s what the sample data looks like:   &lt;!doctype tei2 public \"-//Library of Congress - Historical Collections (American Memory)//DTD ammem.dtd//EN\" [ &lt;!entity % images system \"07010101.ent\"&gt; %images; ]&gt;     wpa0-07010101 [Memorandum to Dr. Botkin]: a machine readable transcription. Life Histories from the Folklore Project, WPA Federal Writers&apos; Project, 1936-1940; American Memory, Library of Congress.   Selected and converted. American Memory, Library of Congress.   Washington, DC, 1994.  Preceding element provides place and date of transcription only.  For more information about this text and this American Memory collection, refer to accompanying matter.     U.S. Work Projects Administration, Federal Writers&apos; Project (Folklore Project, Life Histories, 1936-39); Manuscript Division, Library of Congress. Copyright status not determined; refer to accompanying matter.   The National Digital Library Program at the Library of Congress makes digitized historical materials available for education and scholarship.  This transcription is intended to have an accuracy of 99.95 percent or greater and is not intended to reproduce the appearance of the original work.  The accompanying images provide a facsimile of this work and represent the appearance of the original.  1994/03/15 2002/04/05       0001  Memorandum to Dr. Botkin from G. B. Roberts, May 26, 1941  Subject:  Alabama Material  This material has not yet been accessioned and has only beeen been roughly classified as life histories, folklore, and miscellaneous data and copy save in the case of the 2 ex-slave items and the essay on Jesse Owens, each of which was recommended.  Total no. of items recommended:  3 (14 pp.) In progress                   Catalogers cataloged stuff in the English language, but every metadata record needed to be translated into the other six U.N. languages: Spanish, Russian, French, Arabic, Chinese, and Portuguese. &#8617;                  And you are but one sudo easy_install BeautifulSoup` away from that. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/07/13/wdl-metadata-mapping-and-parsing-tei-in-python/",
        "teaser": null
      },{
        "title": "A Digital Object Defined",
        "excerpt":"What happens to a digital object defined?1   Does its identifier dry up like a raisin in the sun? Or its relationships fester like a sore– And then run? Do its bits rot like meat? Or become overwritten– like some throw-away sheet?   Maybe its metadata just sags like a heavy load.   Or does it fade into code?                  Inspired by Langston Hughes’s “A Dream Deferred” and a spirited conversation in the office today. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/07/15/a-digital-object-defined/",
        "teaser": null
      },{
        "title": "Validating ORE from the Command-line",
        "excerpt":"I’ve been periodically poking at getting Linked Data/RDF views hooked into the World Digital Library web application, following Ed Summers’ lead from his work on Chronicling America.  The RDF views also use the OAI-ORE vocabulary to express aggregations – in WDL, an item is an aggregation of its constituent files.  The goal is to provide a semantically rich and holistic representation of a WDL item (identifier, constituent files, metadata, translations, and so on).   The ORE format is a new one for me so it’s hard to say whether the output of my dev branch is valid ORE or not.  Plus I’m a sucker for validators.  Turns out Rob Sanderson has developed a Python library for validating ORE, and this little snippet is what I’ve been using to validate the ORE.  I didn’t put much effort into making it readable, so much as banging something functional out so I can meet deadlines, so mea culpa and all that.  But without further hemming and hawing, the code:    # validate.py import sys from foresite import *  rem = RdfLibParser().parse(ReMDocument(sys.argv[1])) aggr = rem.aggregation n3 = RdfLibSerializer('n3') rem2 = aggr.register_serialization(n3) print rem2.get_serialization(n3).data   Most of this code is naively copied and pasted from Rob’s excellent Foresite documentation.   I invoke it thusly: python validate.py {URL}   And the output:    @prefix _27: &lt;http://www.semanticdesktop.org/ontologies/nfo#&gt;. @prefix _28: &lt;http://localhost/en/item/1/id#&gt;. @prefix _29: &lt;http://localhost/en/item/1/&gt;. @prefix bibo: &lt;http://purl.org/ontology/bibo/&gt;. @prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt;. @prefix dcterms: &lt;http://purl.org/dc/terms/&gt;. @prefix ore: &lt;http://www.openarchives.org/ore/terms/&gt;. @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;. @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;. @prefix rdfs1: &lt;http://www.w3.org/2001/01/rdf-schema#&gt;.   _28:ResourceMap a ore:ResourceMap;      dc:format \"text/rdf+n3\";      dcterms:created \"2009-07-31T14:23:31Z\";      dcterms:modified \"2009-07-31T14:23:31Z\";      ore:describes _29:id.   _29:id a bibo:Image,          ore:Aggregation;      dcterms:DDC \"973\";      dcterms:alternative \"Antietam, Maryland. Allan Pinkerton, President Lincoln, and Major General John A. McClernand\"@en;      dcterms:created \"1862å¹´10æœˆ3æ—¥\"@zh,          \"3 de octubre de 1862\"@es,          \"3 de outubro de 1862\"@pt,          \"3 octobre 1862\"@fr,          \"3 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 1862 Ð³Ð¾Ð´Ð°\"@ru,          \"October 3, 1862\"@en,          \"Â Ù£ Ø¢ÙƒØªÙˆØ¨Ø±ØŒ Ù¡Ù¨Ù¦Ù¢\"@ar;      dcterms:creator \"Gardner, Alexander\"@en,          \"Gardner, Alexander\"@es,          \"Gardner, Alexander\"@fr,          \"Gardner, Alexander\"@pt,          \"Ð“Ð°Ñ€Ð´Ð½ÐµÑ€, ÐÐ»ÐµÐºÑÐ°Ð½Ð´Ñ€\"@ru,          \"Ø¬Ø§Ø±Ø¯Ù†Ø±, Ø£Ù„ÙŠÙƒØ³Ù†Ø¯Ø±\"@ar,          \"åŠ å¾·çº³, äºšåŽ†å±±å¤§\"@zh; ... (and so on and so forth)      dcterms:title \"Antietam, Maryland. Allan Pinkerton, President Lincoln, and Major General John A. McClernand: Another View\"@en,          \"Antietam, Maryland. Allan Pinkerton, el Presidente Lincoln y el GeneralÂ Principal John A. McClernand: Otra visiÃ³n\"@es,          \"Antietam, Maryland. Allan Pinkerton, le prÃ©sident Lincoln et le gÃ©nÃ©ral-major John A. McClernand: Autre vue\"@fr,          \"Antietam, Maryland. Allan Pinkerton, Â Presidente Lincoln e Major-General John A. McClernand: Outra Vista\"@pt,          \"ÐÐ½Ñ‚Ð¸Ñ‚ÑÐ¼, ÑˆÑ‚Ð°Ñ‚ ÐœÑÑ€Ð¸Ð»ÐµÐ½Ð´. ÐÐ»Ð»Ð°Ð½ ÐŸÐ¸Ð½ÐºÐµÑ€Ñ‚Ð¾Ð½, Ð¿Ñ€ÐµÐ·Ð¸Ð´ÐµÐ½Ñ‚ Ð›Ð¸Ð½ÐºÐ¾Ð»ÑŒÐ½ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ð»-Ð¼Ð°Ð¹Ð¾Ñ€ Ð”Ð¶Ð¾Ð½ Ð. ÐœÐ°ÐºÐºÐ»ÐµÑ€Ð½Ð°Ð½Ð´: Ð”Ñ€ÑƒÐ³Ð¾Ð¹ ÑÐ½Ð¸Ð¼Ð¾Ðº\"@ru,          \"Ø£Ù†ØªÙŠÙ†Ø§Ù…ØŒ Ù…ÙŠØ±ÙŠÙ„Ø§Ù†Ø¯ Ø£Ù„Ø§Ù† Ø¨ÙŠÙ†ÙƒØ±ØªÙˆÙ†ØŒ Ø§Ù„Ø±Ø¦ÙŠØ³ Ù„ÙŠÙ†ÙƒÙˆÙ„Ù†ØŒ ÙˆØ§Ù„Ù„ÙˆØ§Ø¡ Ø¬ÙˆÙ† Ø£. Ù…Ø§ÙƒÙ„ÙŠØ±Ù†Ø§Ù†Ø¯: Ù…Ù†Ø¸Ø± Ø¢Ø®Ø±\"@ar,          \"å®‰è’‚ç‰¹å§†ï¼Œé©¬é‡Œå…°å·ž è‰¾ä¼¦Â·å¹³å…‹é¡¿ã€æž—è‚¯æ€»ç»Ÿå’Œå°‘å°†çº¦ç¿°Â·A Â·é©¬å…‹å…‹æ‹‰å—: å¦ä¸€ä¸ªè§†è§’\"@zh;      ore:aggregates &lt;http://localhost/static/c/1/reference/04326u_thumb_item.gif&gt;,          &lt;http://localhost/static/c/1/service/04326u.tif&gt;;      ore:isDescribedBy &lt;http://localhost/en/item/1/item.rdf&gt;;      rdfs:seeAlso &lt;http://hdl.loc.gov/loc.wdl/dlc.1&gt;.   &lt;http://localhost/static/c/1/reference/04326u_thumb_item.gif&gt; a _27:FileDataObject;      dcterms:format \"image/gif\";      _27:fileSize \"34531\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt;.   &lt;http://localhost/static/c/1/service/04326u.tif&gt; a _27:FileDataObject;      dcterms:format \"image/tiff\";      _27:fileSize \"1301614\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt;.   ore:Aggregation rdfs1:isDefinedBy &lt;http://www.openarchives.org/ore/terms/&gt;;      rdfs1:label \"Aggregation\".   ore:ResourceMap rdfs1:isDefinedBy &lt;http://www.openarchives.org/ore/terms/&gt;;      rdfs1:label \"ResourceMap\".   You might pick up on some warts I have yet to fix, but there you go.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/07/31/validating-ore-from-the-command-line/",
        "teaser": null
      },{
        "title": "Is MARC a data model?",
        "excerpt":"I posted a status update to Twitter, identi.ca, and Facebook late last night hoping to suss out two questions:&lt;ol&gt;  Is MARC a data model? But really: what qualifies something as a data model? &lt;/ol&gt;   I’d poked around looking for clues to the latter and was left cold by the long Wikipedia entry.  Maybe I’ve been doing the micro-blog thing for too long and my ability to parse information that comes in greater-than-140-character chunks has been damaged.  Plus I like learning from examples, and what better example for the library geek than MARC?   The feedback I received was pretty impressive, and not all of it consistent with the rest.  I found it an interesting example of crowdsourcing, so to speak.  As each response came in, I would read it, cross-reference with, e.g., Wikipedia articles, for accuracy, and revise my own answers to the above questions.  I’m honing in on an answer to the former question.  The latter question is still a bit murky.   I thought I’d share the responses, too.  Responses from Twitter are included in full w/ links to the original.  Responses from quasi-public Facebook have been anonymized.  You can see my replies interspersed as well and watch the evolution of the (admittedly short) discussion.  After the jump:   @bangpound: @mjgiarlo MARC is a markup language. It makes no declarations about how data is stored only how it's formatted.&lt;/a&gt;  @ranginui: @mjgiarlo a piece of crap, cue neil young and crazy horse  @anarchivist: @mjgiarlo not a data model, it's a transmission format  @vphill: @mjgiarlo I've heard that said about MARC too, let me know if you get an answer  A container for a data model, such as AACR2  @mjgiarlo: @bangpound, @anarchivist, @vphill: So. let's see: MARC21 bib is a profile of a serialization/transmission format w/ AACR2 as the data model?   @anarchivist: @mjgiarlo wouldn't even assume AACR2 if I was you.  @mjgiarlo: @anarchivist: Okay. Something says \"authors go in 100; contributors go in 700,\" though, right? Is that not a data model? Sorry if dense.  MARC is not a data model (and neither is AACR2) in the sense that neither of them explicitly describes entities and relationships among entities. The relationships in these two non-relational frameworks are implicit, and the semantics of the model must be supplied in the end by the people who use these frameworks. RDA/FRBR is a move toward an actual data model -- it makes some relationships explicit and can properly be represented in an Entity-Relationship diagram (with all those relationship words that explicitly express the semantics -- words like, for example, \"is realized through\" or \"is embodied in\" or \"is exemplified by\"), but even RDA/FRBR does not fully express all of the relationships/semantics and must be translated into an actual data model in order to be implemented -- librarians have been irresponsible, in my opinion, in refusing to learn about relational database concepts, mostly because of their slavish adherence to the old flat-file style that MARC represents.  @gmcharlt: @mjgiarlo MARC is many things at once, which is part of the problem. Not just transmission standard; embodies current cataloging worldview  @edsu: @mjgiarlo i think there are aspects of data modeling in Z39.2 &amp; ISO 2709, and certainly in MARC21 ; that said, i think @gmcharlt is right.  So, based on all the responses I’ve gotten (on Facebook, on Twitter, around the office), here’s my current thinking:   MARC means more than one thing. One meaning of MARC is MARC the binary format. A format is not a data model. Another meaning of MARC is, e.g., MARC21 Bibliographic. MARC21 Bibliographic is a profile of MARC, which is serialized in the MARC binary format. MARC21 Bibliographic defines semantics for fields and subfields and indicators, which makes it feel like a data model.  This gets at some of the assumptions I've internalized about data models. The MARC21 Bibliographic data model thus has well-defined entities, but otherwise is a poor data model, primarily because: It does not have well-defined relationships between the entities; It conflates different conceptual models, such as the FRBR Group 1 entities and also mixes FRBR Group 1 entities with Group 2 and 3 entities.   I'm not sure where this leaves AACR2, but it feels like it just fell out of the discussion.   I’d be pleased if the discussion continued.  If nothing else, it really satisfies my curiosity and gets my brain going (which is useful on a Monday morning).  ","categories": [],
        "tags": [],
        "url": "/blog/2009/08/10/is-marc-a-data-model/",
        "teaser": null
      },{
        "title": "Linking World Digital Library Data",
        "excerpt":"As I mentioned earlier, I’ve been learning about linked data in the context of dropping it into the World Digital Library project.  I am hopeful we’ll be able to deploy the RDF views1 before too long.  In advance of that, I thought it might be helpful to share a sample of what our RDF would look like.  The RDF below represents the WDL item for the U.S. Constitution.  I appreciate constructive criticism.   A few things to note:   Mmm, Unicode. Item types are from the Bibliographic Ontology. Most of the properties are from the Dublin Core Metadata Element Set ontology, especially used where literals are objects rather than resources identified by URI.  Where possible I dug up or found URIs and used the Dublin Core Metadata Terms ontology. An item is modeled as an aggregation of its constituent files, as defined in OAI-ORE.  The notion here is that an ORE aggregation of an item, as expressed in a resource map which is discoverable via a link header in each item detail page, is a \"whole\" item, including all of its files[^2], metadata, and translations. I'm also making light use of the NEPOMUK File Ontology to express that constituent files are files, and to be explicit about file sizes so that folks know in advance of retrieving it how large files are. Links out to DDC (Decimalised Database of Concepts), Lingvoj, DBpedia, and Library of Congress Authorities &amp; Vocabularies (e.g., LC Subject Headings) are included where possible[^3]. I'd be especially stoked to hear of other vocabs I might link to.  The more linked the data, the better. The output below is Turtle for readability, but the application will offer up RDF/XML.   The data after the jump:    @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt; . @prefix dcterms: &lt;http://purl.org/dc/terms/&gt; . @prefix nfo: &lt;http://www.semanticdesktop.org/ontologies/nfo#&gt; . @prefix ore: &lt;http://www.openarchives.org/ore/terms/&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .  &lt;http://localhost/static/c/2708/service/00303_2003_001_pr.jpg&gt;     dc:format \"image/jpeg\" ;     nfo:fileSize \"259485\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;     a nfo:FileDataObject .  &lt;http://localhost/static/c/2708/service/00303_2003_003_pr.jpg&gt;     dc:format \"image/jpeg\" ;     nfo:fileSize \"267031\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;     a nfo:FileDataObject .  &lt;http://localhost/static/c/2708/reference/00303_2003_004_pr_thumb_item.gif&gt;     dc:format \"image/gif\" ;     nfo:fileSize \"56620\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;     a nfo:FileDataObject .  &lt;http://localhost/static/c/2708/service/00303_2003_004_pr.jpg&gt;     dc:format \"image/jpeg\" ;     nfo:fileSize \"233875\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;     a nfo:FileDataObject .  &lt;http://localhost/static/c/2708/service/00303_2003_002_pr.jpg&gt;     dc:format \"image/jpeg\" ;     nfo:fileSize \"245809\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;     a nfo:FileDataObject .  &lt;http://localhost/item/2708/about.rdf&gt;     dcterms:created \"2009-08-10T18:11:25-04:00\"^^dcterms:W3CDTF ;     dcterms:creator &lt;http://dbpedia.org/resource/World_Digital_Library&gt; ;     dcterms:modified \"2009-08-10T18:11:25-04:00\"^^dcterms:W3CDTF ;     ore:describes &lt;http://localhost/item/2708/about.rdf#item&gt; ;     a ore:ResourceMap .  &lt;http://localhost/item/2708/about.rdf#item&gt;     dc:created \"17 Septembre 1787\"@fr, \"17 de septiembre de 1787\"@es, \"17 de setembro de 1787\"@pt, \"17 ÑÐµÐ½Ñ‚ÑÐ±Ñ€Ñ 1787 Ð³.\"@ru, \"1787å¹´9æœˆ17æ—¥\"@zh, \"September 17, 1787\"@en, \"\"\"Ù¡Ù§ Ø§ÙŠÙ„ÙˆÙ„ Ù¡Ù§Ù¨Ù§ \"\"\"@ar ;     dc:creator \"Constitutional Convention, United States\"@en, \"ConvenciÃ³n Constituyente, Estados Unidos\"@es, \"Convention constitutionnelle, Ã‰tats-Unis\"@fr, \"ConvenÃ§Ã£o Constitucional, Estados Unidos\"@pt, \"ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ ÐšÐ¾Ð½Ð²ÐµÐ½Ñ†Ð¸Ñ, Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ðµ Ð¨Ñ‚Ð°Ñ‚Ñ‹\"@ru, \"Ø§Ù„Ø§ØªÙØ§Ù‚ÙŠØ© Ø§Ù„Ø¯Ø³ØªÙˆØ±ÙŠØ©ØŒ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø©\"@ar, \"åˆ¶å®ªä¼šè®®ï¼Œç¾Žå›½\"@zh ;     dc:extent \"Manuscript (4 pages of parchment)\"@en, \"Manuscrit (4 pages de parchemin)\"@fr, \"Manuscrito (4 pÃ¡ginas de pergamino)\"@es, \"Manuscrito (4 pÃ¡ginas em pergaminho)\"@pt, \"Ð ÑƒÐºÐ¾Ð¿Ð¸ÑÑŒÂ (4 Ð¿ÐµÑ€Ð³Ð°Ð¼ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹)\"@ru, \"Ù…Ø®Ø·ÙˆØ·Ø© (Ù¤ ØµÙØ­Ø§Øª Ù…Ù† Ø§Ù„ÙˆØ±Ù‚ Ø§Ù„Ù†ÙÙŠØ³)\"@ar, \"æ‰‹è‰æœ¬ï¼ˆ4 é¡µç¾Šçš®çº¸ï¼‰\"@zh ;     dc:language \"Anglais\"@fr, \"English\"@en, \"InglÃ©s\"@es, \"InglÃªs\"@pt, \"ÐÐ½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº\"@ru, \"Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\"@ar, \"è‹±è¯­\"@zh ;     dc:publisher \"AdministraÃ§Ã£o de Registros e Arquivos Nacionais\"@pt, \"Archives Nationales et Administration des documents (NARA) des Ã‰tats-Unis d'AmÃ©rique \"@fr, \"Los Archivos Nacionales y AdministraciÃ³n de Documentos (NARA) de los Estados Unidos de AmÃ©rica\"@es, \"National Archives and Records Administration\"@en, \"Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ð°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ñ€Ñ…Ð¸Ð²Ð¾Ð² Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²\"@ru, \"Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ÙˆØ·Ù†ÙŠØ©\"@ar, \"ç¾Žå›½å›½å®¶æ–‡ä»¶ä¸Žæ¡£æ¡ˆç®¡ç†å±€\"@zh ;     dc:subject \"Constituciones\"@es, \"ConstituiÃ§Ãµes\"@pt, \"Constitutional &amp; administrative law\"@en, \"Constitutions\"@en, \"Constitutions\"@fr, \"Derecho constitucional y administrativo\"@es, \"Direito constitucional e administrativo\"@pt, \"Droit constitutionnel et administratif\"@fr, \"Politics and government\"@en, \"Politique et gouvernement\"@fr, \"PolÃ­tica e governo\"@pt, \"PolÃ­tica y gobierno\"@es, \"ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ð¸\"@ru, \"ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ð¸ Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¿Ñ€Ð°Ð²Ð¾\"@ru, \"ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸ÐºÐ° Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾\"@ru, \"Ø§Ù„Ø¯Ø³Ø§ØªÙŠØ±\"@ar, \"Ø§Ù„Ø³ÙŠØ§Ø³Ø© ÙˆØ§Ù„Ø­ÙƒÙˆÙ…Ø©\"@ar, \"Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¯Ø³ØªÙˆØ±ÙŠ ÙˆØ§Ù„Ø¥Ø¯Ø§Ø±ÙŠ.\"@ar, \"å®ªæ³•\"@zh, \"å®ªæ³• &amp; è¡Œæ”¿æ³•\"@zh, \"æ”¿æ²»å’Œæ”¿åºœ\"@zh ;     dc:title \"ConstituciÃ³n de los Estados Unidos\"@es, \"ConstituiÃ§Ã£o dos Estados Unidos\"@pt, \"Constitution des Ã‰tats-Unis\"@fr, \"Constitution of the United States\"@en, \"ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ñ Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð¨Ñ‚Ð°Ñ‚Ð¾Ð²\"@ru, \"Ø¯Ø³ØªÙˆØ± Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø©\"@ar, \"ç¾Žå›½å®ªæ³•\"@zh ;     dcterms:DDC \"342\" ;     dcterms:LCSH &lt;http://id.loc.gov/authorities/label/Constitutions&gt; ;     dcterms:alternative \"Constitution of the United States\"@en ;     dcterms:dateSubmitted \"2009-05-07T06:45:21-04:00\"^^dcterms:W3CDTF ;     dcterms:description \"1787 å¹´ 5 æœˆ 14 æ—¥ï¼Œåˆ¶å®ªä¼šè®®åœ¨è´¹åŸŽçš„è®®ä¼šå¤§æ¥¼ï¼ˆç‹¬ç«‹åŽ…ï¼‰å¬å¼€ï¼Œç›®çš„æ˜¯ä¿®è®¢ã€Šé‚¦è”æ¡ä¾‹ã€‹ã€‚ ç”±äºŽå¼€å§‹æ—¶åªæœ‰ä¸¤ä¸ªå·žçš„ä»£è¡¨å›¢å‡ºå¸­ï¼Œæˆå‘˜ä¸å¾—ä¸ä¸€å¤©å¤©åœ°ä¼‘ä¼šï¼Œç›´åˆ° 5 æœˆ 25 æ—¥ä¸Žä¼šäººæ•°è¾¾åˆ°æ³•å®šçš„ä¸ƒä¸ªå·žã€‚ é€šè¿‡è®¨è®ºå’Œäº‰è¾©ï¼Œ6 æœˆä¸­æ—¬æ—¶æ˜Žç¡®æ˜¾ç¤ºå¤§ä¼šä¸Žå…¶ä¿®æ”¹çŽ°æœ‰çš„ã€Šè”é‚¦æ¡ä¾‹ã€‹ä¸å¦‚ä¸ºæ”¿åºœé‡æ–°èµ·è‰ä¸€ä»½å…¨æ–°çš„æ¡†æž¶ã€‚ æ•´ä¸ªå¤å­£ï¼Œä»£è¡¨ä»¬éƒ½åœ¨éžå…¬å¼€ä¼šè®®ä¸­è¾©è®ºã€èµ·è‰ã€é‡æ–°èµ·è‰æ–°å®ªæ³•çš„æ¡æ¬¾ã€‚ ä¸»è¦çš„äº‰è®ºé—®é¢˜åŒ…æ‹¬è¦èµ‹äºˆä¸­å¤®æ”¿åºœå¤šå¤§æƒåˆ©ã€å…è®¸å„å·žåœ¨å›½ä¼šä¸­æœ‰å¤šå°‘ä¸ªä»£è¡¨å¸­ä½ä»¥åŠè¿™äº›ä»£è¡¨åº”è¯¥å¦‚ä½•é€‰ä¸¾äº§ç”Ÿâ€”â€”ç”±äººæ°‘ç›´æŽ¥é€‰ä¸¾è¿˜æ˜¯ç”±å„å·žç«‹æ³•äººå‘˜é€‰ä¸¾äº§ç”Ÿã€‚ è¿™éƒ¨å®ªæ³•æ˜¯å¾ˆå¤šäººæ™ºæ…§çš„ç»“æ™¶ï¼Œæ˜¯åˆä½œæ”¿æ²»è¿ä½œå’Œå¦¥åè‰ºæœ¯çš„å…¸èŒƒã€‚\"@zh, \"A ConvenÃ§Ã£o Federal reuniu-se na Casa de Estado (Hall da IndependÃªncia), em FiladÃ©lfia, em 14 de maio de 1787 para revisar os Artigos da ConfederaÃ§Ã£o. Em virtude de estarem presentes, inicialmente, as delegaÃ§Ãµes de apenas dois estados, os membros suspenderam os trabalhos, dia apÃ³s dia, atÃ© que fosse atingido o quÃ³rum de sete estados em 25 de maio. AtravÃ©s de discussÃµes e debates ficou claro, em meados de junho que, em vez de alterar os atuais artigos da ConfederaÃ§Ã£o, a convenÃ§Ã£o deveria elaborar uma estrutura inteiramente nova para o governo. Ao longo de todo o verÃ£o, os delegados debateram, elaboraram e reelaboraram os artigos da nova ConstituiÃ§Ã£o em sessÃµes fechadas. Entre os principais pontos em questÃ£o estavam o grau de poder permitido ao governo central, o nÃºmero de representantes no Congresso para cada Estado, e como estes representantes deveriam ser eleitos - diretamente pelo povo ou pelos legisladores do estado. A ConstituiÃ§Ã£o foi o trabalho de muitas mentes e permanece como um modelo de cooperaÃ§Ã£o entre lideranÃ§as polÃ­ticas e da arte da condescendÃªncia.\"@pt, \"La ConvenciÃ³n Federal se reuniÃ³ en la CÃ¡mara del Estado (SalÃ³n de la Independencia) en Filadelfia el 14 de mayo de 1787, para revisar los artÃ­culos de la ConfederaciÃ³n. Debido a que las delegaciones de sÃ³lo dos estados estuvieron presentes inicialmente, los miembros levantaron sesiÃ³n de un dÃ­a para el siguiente hasta que se obtuvo un quÃ³rum de siete estadosÂ el 25 de mayo. A travÃ©s de la discusiÃ³n y el debate se hizo evidente a mediados de junio que, en lugar de modificar los actuales artÃ­culos de la ConfederaciÃ³n, la convenciÃ³n prepararÃ­a un marco totalmente nuevo para el gobierno. Durante todo el verano, los delegados debatieron, prepararon y redactaron nuevamente los artÃ­culos de la nueva ConstituciÃ³n en sesiones a puerta cerrada. Entre los principales puntos en cuestiÃ³n estuvieron cuÃ¡ntoÂ poder otorgarÂ al gobierno central, el nÃºmero de representantes en el Congreso que se iban aÂ permitir a cada Estado y la forma en que estos representantes debÃ­an ser elegidos, directamente por el pueblo o por los legisladores estatales. La ConstituciÃ³n fue el resultado del trabajo de muchas mentes y se erige como modelo de cooperaciÃ³n polÃ­tica y del arte del compromiso.\"@es, \"La Convention FÃ©dÃ©rale s'assembla dans la Chambre LÃ©gislative (Independence Hall) Ã  Philadelphie le 14 mai 1787, pour rÃ©viser les articles de la ConfÃ©dÃ©ration. En raison de la seule prÃ©sence initiale des dÃ©lÃ©gations de deux Ã‰tats, les membres ajournÃ¨rent d'un jour Ã  l'autre jusqu'Ã  ce que le quorum de sept Ã‰tats soit obtenu le 25 mai. Ã‚ travers les discussions et les dÃ©bats, il devint clair dÃ¨s la mi-juin que, plutÃ´t que de modifier les articles existants de la ConfÃ©dÃ©ration, la convention allait plutÃ´t Ã©baucher un cadre entiÃ¨rement nouveau pour le gouvernement. Tout au long de l'Ã©tÃ©, les dÃ©lÃ©guÃ©s dÃ©battirent, Ã©laborÃ¨rent, et remaniÃ¨rent les articles de la nouvelle Constitution, Ã  huis clos. Les principaux points litigieux portaient sur la puissance Ã  accorder au gouvernement central, sur le nombre de reprÃ©sentants au CongrÃ¨s pour chaque Ã‰tat, et sur le mode d'Ã©lection de ces reprÃ©sentants - directement par le peuple ou par les lÃ©gislateurs de l'Ã©tat. La Constitution fut l'Å“uvre de nombreux esprits et reste un modÃ¨le de coopÃ©ration politique et de l'art du compromis.\"@fr, \"The Federal Convention convened in the State House (Independence Hall) in Philadelphia on May 14, 1787, to revise the Articles of Confederation. Because the delegations from only two states were present initially, the members adjourned from one day to the next until a quorum of seven states was obtained on May 25. Through discussion and debate it became clear by mid-June that, rather than amend the existing Articles of Confederation, the convention would draft an entirely new framework for the government. All through the summer, the delegates debated, drafted, and redrafted the articles of the new Constitution in closed sessions. Among the chief points at issue were how much power to allow the central government, how many representatives in Congress to allow each state, and how these representatives should be elected--directly by the people or by the state legislators. The Constitution was the work of many minds and stands as a model of cooperative statesmanship and the art of compromise.\"@en, \"Ð¤ÐµÐ´ÐµÑ€Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾Ð±Ñ€Ð°Ð½Ð¸Ðµ ÑÐ¾Ð±Ñ€Ð°Ð»Ð¾ÑÑŒ Ð½Ð° Ð·Ð°ÑÐµÐ´Ð°Ð½Ð¸Ðµ Ð² Ð”Ð¾Ð¼Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° (Ð·Ð°Ð» ÐÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸) 14 Ð¼Ð°Ñ 1787 Ð³Ð¾Ð´Ð° Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑÐ¼Ð¾Ñ‚Ñ€Ð° Ð¡Ñ‚Ð°Ñ‚ÐµÐ¹ ÐšÐ¾Ð½Ñ„ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ð¸. ÐŸÐ¾ÑÐºÐ¾Ð»ÑŒÐºÑƒ Ð²Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð½Ð° Ð·Ð°ÑÐµÐ´Ð°Ð½Ð¸Ð¸ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ð»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð²ÑƒÑ… ÑˆÑ‚Ð°Ñ‚Ð¾Ð², Ð¡Ð¾Ð±Ñ€Ð°Ð½Ð¸Ðµ Ð±Ñ‹Ð»Ð¾ Ñ€Ð°ÑÐ¿ÑƒÑ‰ÐµÐ½Ð¾ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð´Ð½ÐµÐ¹ Ð´Ð¾ Ñ‚ÐµÑ… Ð¿Ð¾Ñ€, Ð¿Ð¾ÐºÐ° 25 Ð¼Ð°Ñ Ð½Ðµ Ð±Ñ‹Ð» Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½ ÐºÐ²Ð¾Ñ€ÑƒÐ¼ Ð¸Ð· Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÐµÐ¹ ÑÐµÐ¼Ð¸ ÑˆÑ‚Ð°Ñ‚Ð¾Ð². Ð’ Ñ…Ð¾Ð´Ðµ Ð´Ð¸ÑÐºÑƒÑÑÐ¸Ð¹ Ð¸ Ð´ÐµÐ±Ð°Ñ‚Ð¾Ð² Ðº ÑÐµÑ€ÐµÐ´Ð¸Ð½Ðµ Ð¸ÑŽÐ½Ñ ÑÑ‚Ð°Ð»Ð¾ Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð±Ñ€Ð°Ð½Ð¸Ðµ Ð±Ñ‹Ð»Ð¾ Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¾ ÑÐºÐ¾Ñ€ÐµÐµ ÑÐ¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°, Ð½ÐµÐ¶ÐµÐ»Ð¸ Ñ‡ÐµÐ¼ Ð¿ÐµÑ€ÐµÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¡Ñ‚Ð°Ñ‚ÑŒÐ¸ ÐšÐ¾Ð½Ñ„ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ð¸. Ð’ Ñ‚ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð²ÑÐµÐ³Ð¾ Ð»ÐµÑ‚Ð° Ð´ÐµÐ»ÐµÐ³Ð°Ñ‚Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¸, ÑÐ¾ÑÑ‚Ð°Ð²Ð»ÑÐ»Ð¸ Ñ‡ÐµÑ€Ð½Ð¾Ð²Ñ‹Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð½Ð¾Ð²Ð¾Ð¹ ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ð¸ Ð¸ Ñ‚ÑƒÑ‚ Ð¶Ðµ Ð¸Ñ… Ð¿ÐµÑ€ÐµÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°Ð»Ð¸ Ð² Ñ…Ð¾Ð´Ðµ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð·Ð°ÑÐµÐ´Ð°Ð½Ð¸Ð¹. Ð¡Ñ€ÐµÐ´Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð²ÑˆÐ¸Ñ…ÑÑ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð±Ñ‹Ð»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ ÑÑ‚ÐµÐ¿ÐµÐ½Ð¸ Ð²Ð»Ð°ÑÑ‚Ð¸ Ð¸ Ð¿Ð¾Ð»Ð½Ð¾Ð¼Ð¾Ñ‡Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼Ð¸ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ Ð½Ð°Ð´ÐµÐ»ÐµÐ½Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾, ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÐµÐ¹ Ð² ÐšÐ¾Ð½Ð³Ñ€ÐµÑÑÐµ Ð¾Ñ‚ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑˆÑ‚Ð°Ñ‚Ð°, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ñ‹ Ð¿ÐµÑ€ÐµÐ¸Ð·Ð±Ñ€Ð°Ð½Ð¸Ñ ÑÑ‚Ð¸Ñ… Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÐµÐ¹Â â€” Ð½ÐµÐ¿Ð¾ÑÑ€ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð¶Ð¸Ñ‚ÐµÐ»ÑÐ¼Ð¸ ÑˆÑ‚Ð°Ñ‚Ð¾Ð² Ð¸Ð»Ð¸ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ ÑÐ¾Ð±Ñ€Ð°Ð½Ð¸ÑÐ¼Ð¸ ÑˆÑ‚Ð°Ñ‚Ð¾Ð². ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ñ Ð±Ñ‹Ð»Ð° Ð¿Ð»Ð¾Ð´Ð¾Ð¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¼Ð½Ð¾Ð³Ð¸Ñ… Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¾Ð² Ð¸ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑÑ€ÐºÐ¸Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð¼ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð³Ð¾ÑÑƒÐ´Ð°Ñ€ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð´ÐµÑÑ‚ÐµÐ»ÐµÐ¹ Ð¸ Ð¸ÑÐºÑƒÑÑÑ‚Ð²Ð° ÐºÐ¾Ð¼Ð¿Ñ€Ð¾Ð¼Ð¸ÑÑÐ°.\"@ru, \"Ø§Ø¬ØªÙ…Ø¹ Ù…Ù…Ø«Ù„Ùˆ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„ÙØ¯Ø±Ø§Ù„ÙŠ ÙÙŠ Ù‚ØµØ± Ø§Ù„Ø¯ÙˆÙ„Ø© (Ù‚Ø§Ø¹Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ù„Ø§Ù„) ÙÙŠ ÙÙŠÙ„Ø§Ø¯Ù„ÙÙŠØ§ ÙŠÙˆÙ… Ù¡Ù¤Â  Ø£ÙŠØ§Ø± Ù¡Ù§Ù¨Ù§ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø§ØªØ­Ø§Ø¯. ÙˆØ­ÙŠØ« Ø­Ø¶Ø± ÙˆÙØ¯Ø§Ù† Ø§Ø«Ù†Ø§Ù† ÙÙ‚Ø· Ù…Ù† ÙˆÙÙˆØ¯ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ Ø±ÙØ¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ø­Ø¶ÙˆØ± Ø§Ù„Ø¬Ù„Ø³Ø© Ù…Ù† ÙŠÙˆÙ… Ø¥Ù„Ù‰ Ø¢Ø®Ø± Ø­ØªÙ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù†ØµØ§Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø­Ø¶ÙˆØ± ÙˆÙÙˆØ¯ Ø³Ø¨Ø¹ ÙˆÙ„Ø§ÙŠØ§Øª ÙÙŠ Ù¢Ù¥ Ø£ÙŠØ§Ø±. ÙˆÙ‚Ø¯ Ø§ØªØ¶Ø­ Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª ÙˆØ§Ù„Ø­ÙˆØ§Ø± Ø¨Ø­Ù„ÙˆÙ„ Ù…Ù†ØªØµÙ Ø­Ø²ÙŠØ±Ø§Ù† Ø£Ù†Ù‡ Ø¨Ø¯Ù„Ø§ Ù…Ù† ØªØ¹Ø¯ÙŠÙ„ Ù…ÙˆØ§Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙØ¯Ø±Ø§Ù„ÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ ÙƒØ§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤ØªÙ…Ø±ÙŠÙ† ØµÙŠØ§ØºØ© Ø¥Ø·Ø§Ø± Ø¬Ø¯ÙŠØ¯ ØªÙ…Ø§Ù…Ø§ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø­ÙƒÙˆÙ…Ø©. ÙˆØ·ÙˆØ§Ù„ Ø°Ù„Ùƒ Ø§Ù„ØµÙŠÙØŒ Ù†Ø§Ù‚Ø´ Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ÙˆÙ† ÙˆØµØ§ØºÙˆØ§ Ø«Ù… Ø£Ø¹Ø§Ø¯ÙˆØ§ ØµÙŠØ§ØºØ© Ù…ÙˆØ§Ø¯ Ø§Ù„Ø¯Ø³ØªÙˆØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø¬Ù„Ø³Ø§Øª Ù…ØºÙ„Ù‚Ø©. ÙˆÙ…Ù† Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ Ø¯Ø§Ø± Ø­ÙˆÙ„Ù‡Ø§ Ø§Ù„Ø¬Ø¯Ù„ Ù…Ø¯Ù‰ ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù…Ø«Ù„ÙŠÙ† ÙÙŠ Ø§Ù„ÙƒÙˆÙ†ØºØ±Ø³ Ù„ÙƒÙ„ ÙˆÙ„Ø§ÙŠØ© ØŒ ÙˆÙƒÙŠÙÙŠØ© Ø§Ù†ØªØ®Ø§Ø¨ Ù‡Ø¤Ù„Ø§Ø¡ Ù…Ù…Ø«Ù„ÙŠÙ† -- Ø¨Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„Ø´Ø¹Ø¨ Ø£Ùˆ Ù…Ù† Ù‚Ø¨Ù„ Ù…Ø´Ø±Ù‘Ø¹ÙŠ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª. Ù„Ù‚Ø¯ ÙƒØ§Ù† Ø§Ù„Ø¯Ø³ØªÙˆØ± Ù…Ù† Ø¹Ù…Ù„ Ø¹Ù‚ÙˆÙ„ ÙƒØ«ÙŠØ±Ø© ÙˆÙ‡Ùˆ ÙŠÙ…Ø«Ù„ Ù†Ù…ÙˆØ°Ø¬Ø§ Ù„ÙÙ† Ø§Ù„Ø­ÙƒÙ… Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠ Ø­Ù†ÙƒØ© Ø§Ù„ØªÙˆØµÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„ÙˆØ³Ø·.\"@ar ;     dcterms:identifier \"http://localhost/item/2708/about.rdf#item\" ;     dcterms:language &lt;http://www.lingvoj.org/lang/en&gt; ;     dcterms:publisher &lt;http://dbpedia.org/resource/National_Archives_and_Records_Administration&gt; ;     dcterms:spatial &lt;http://dbpedia.org/resource/North_America&gt;, &lt;http://dbpedia.org/resource/United_States_of_America&gt;, \"AmÃ©rica del Norte\"@es, \"AmÃ©rica do Norte\"@pt, \"AmÃ©rique du Nord\"@fr, \"Estados Unidos da AmÃ©rica\"@pt, \"Estados Unidos de AmÃ©rica\"@es, \"North America\"@en, \"United States of America\"@en, \"Ã‰tats-Unis d'AmÃ©rique\"@fr, \"Ð¡ÐµÐ²ÐµÑ€Ð½Ð°Ñ ÐÐ¼ÐµÑ€Ð¸ÐºÐ°\"@ru, \"Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ðµ Ð¨Ñ‚Ð°Ñ‚Ñ‹ ÐÐ¼ÐµÑ€Ð¸ÐºÐ¸\"@ru, \"Ø£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ©\"@ar, \"Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©\"@ar, \"åŒ—ç¾Ž\"@zh, \"ç¾Žå›½\"@zh ;     dcterms:subject &lt;http://dbpedia.org/resource/Constitutions&gt; ;     dcterms:temporal \"1700 AD - 1799 AD\"@en, \"1700 ap. J.-C. - 1799 ap. J.-C.\"@fr, \"1700 d.C. - 1799 d.C.\"@es, \"1700 d.C. - 1799 d.C.\"@pt, \"1700 Ð½.Ñ. - 1799 Ð½.Ñ.\"@ru, \"1700 å…¬å…ƒ - 1799 å…¬å…ƒ\"@zh, \"Ù¡Ù§Ù Ù  Ù… - Ù¡Ù§Ù©Ù© Ù…\"@ar ;     dcterms:title &lt;http://dbpedia.org/resource/Constitution_of_the_United_States&gt; ;     ore:aggregates &lt;http://localhost/static/c/2708/reference/00303_2003_004_pr_thumb_item.gif&gt;, &lt;http://localhost/static/c/2708/service/00303_2003_001_pr.jpg&gt;, &lt;http://localhost/static/c/2708/service/00303_2003_002_pr.jpg&gt;, &lt;http://localhost/static/c/2708/service/00303_2003_003_pr.jpg&gt;, &lt;http://localhost/static/c/2708/service/00303_2003_004_pr.jpg&gt; ;     ore:isDescribedBy &lt;http://localhost/item/2708/about.rdf&gt; ;     a &lt;http://purl.org/ontology/bibo/Manuscript&gt; ;     rdfs:seeAlso &lt;http://hdl.loc.gov/loc.wdl/dna.2708&gt; .                   Sadly, the URIs are uglyish due to some constraints from our caching configuration.  I figure we can redirect uglyish URIs to cool ones and make use of owl:sameAs` if those constraints go away. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/08/10/linking-world-digital-library-data/",
        "teaser": null
      },{
        "title": "JSONovich emerges",
        "excerpt":"JSONovich has now emerged from the Mozilla Add-ons sandbox and is available to the masses: http://addons.mozilla.org/en-US/firefox/addon/10122.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/08/24/jsonovich-emerges/",
        "teaser": null
      },{
        "title": "I2: Survey results",
        "excerpt":"I wrote in June that the I2 subgroup surveyed “repository managers to determine the current practices and needs of the repository community regarding institutional identifiers. Results from the survey will inform a set of use cases that will be shared with the community, and that are expected to drive the development of a new standard for institutional identifiers.”   The survey closed in July, and the subgroup spent August writing a report on the survey results.  That report is now final and it’s available to the public.  Feedback may be sent to our (woefully underutilized) public i2info mailing list, left as a comment on this post, or e-mailed to me privately which I can forward to our internal list.   The next step is to build upon the report to draw yet more conclusions from the data – there’s an awful lot there – and flesh out some repository use cases for institutional identifiers.  The I2 core group is moving quickly towards finalizing identifier metadata elements so that a standard may be drafted, and I think having some use cases documented will help drive the standard in a direction the community can get behind.   Onward and upward.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/09/15/i2-survey-results/",
        "teaser": null
      },{
        "title": "Command-line shuffle",
        "excerpt":"Being a nerd, I tend to like the command-line.  When I’m working on my laptop at home, I tend to like listening to music.  Before I discovered that mplayer had a really convenient shuffle idiom, I would invoke it thusly (to listen to all my Pavement tracks in shuffle mode):    export IFS=$'\\n' for track in $(find /mnt/upnp/MediaTomb/Audio/Artists/Pavement -name \\*.mp3 | ~/bin/shuffle.py); do mplayer $track; done   And the wee shuffle script I whipped together looks like this:    #!/usr/bin/env python # shuffle.py  import sys import random  args = list(sys.stdin) random.shuffle(args) sys.stdout.writelines(args)   And here’s the convenient shuffle idiom that renders my arg-shuffling script somewhat useless:    find /mnt/upnp/MediaTomb/Audio/Artists/Pavement -name \\*.mp3 | mplayer -playlist - -shuffle -loop 0  ","categories": [],
        "tags": [],
        "url": "/blog/2009/09/26/command-line-shuffle/",
        "teaser": null
      },{
        "title": "Exploring curation micro-services",
        "excerpt":"As far as I’m concerned, the most exciting developments this year in  repositories and digital curation have come out of the California Digital Library.  It has been impossible not to notice their papers and presentations.  Put simply, their idea is that digital curation is enabled by “micro-services” built upon well-known abstractions such as the filesystem.  The benefits are obvious: filesystem tools are ubiquitous and cross-platform, and there are strong market forces to ensure the filesystem persists.  The idea is radically simple and straightforward, though many questions remain about such a paradigm.  I’ll return to those later.   If you have not yet taken a look at CDL’s curation micro-service specifications, most of which may be printed on as few as one or two sheets of paper, see the Digital Library Building Blocks.   My co-workers in the LC Repository Development Center have been chatting about these specs on and off throughout the year.  After months of procrastinating, I finally read all of the specs on Thursday; it’s wonderful that you can do so in the course of one reading session, I might add.  Yesterday a bunch of us RDCers got together to chat (informally) about the specs: what they’re for, how they work, and how they interact with one another.  I learn by doing, by examples, so I combed through each of the specs in advance of our meeting and tried to construct a minimal repository1 based on micro-services.  Here is a tree visualization of the final product, inevitable warts and all:   The services I used were Namaste, Content Access Node (CAN), Pairtree, Dflat, Reverse Directory Deltas (ReDD), Class-based System for Managing Object Properties (CLOP), and BagIt (co-developed by LC and CDL).   As I mentioned in our Friday meeting, recounting my experience exploring the specs: the bad thing is that I spent an hour building a repository with rudimentary tools such as mkdir, touch, cp, ln, and emacs; but the good thing is that I built a repository in one hour using common, rudimentary tools.  It’s a very compelling paradigm.  Ed’s already built a tool implementing some of Dflat, further demonstrating how lightweight these micro-services are.  (UPDATE: Ed notes that this code is a work in progress and is “barely functional.”)  (UPDATE 2: The dflat library has come a long way.  Check it out if you’re interested.  Also, I just committed a pretty basic Namaste library: http://github.com/mjgiarlo/namaste.  Only took about an hour, which is a testament to the power of lightweight specs.)   I am certain this will be a running thread at work as the specifications evolve and our understanding of them grows.  Some questions and comments that occurred to me while exploring the micro-service specs and building the minimal repo:    \tCAN was a bit puzzling.  The spec is simple enough, but I found some of the conventions confusing, and I was left wondering what CAN provides other than a container.  What I would like to see is a simple use case and perhaps more examples.  Thus, the CAN stuff in my sample repo doesn't feel very useful only because I had a hard time working with the spec. \tCLOP feels like the least mature of the specifications.  It seems generally useful to be able to put digital objects, however you define that, into classes and define properties on those classes.  The spec did not clearly convey to me just how it accomplishes that aim.  A few examples would go a very long way.  I've got some CLOP stuff in the sample repo but I have no idea how close my implementation matches the spec. \tIs Dflat dependent on ReDD?  One would assume not since there's an optional property in the dflat-info.txt file for specifying a delta scheme.  But, say, could you stub out the v001 directory (reserved to hold the initial version of a digital object) and use a system such as git or bazaar?    One might argue that these established delta schemes, if you want to call them that, have many more developers and users than a system such as ReDD and thus should persist longer and have more tools built around them.  I imagine the micro-service viewpoint would acknowledge that point, but counter that the spirit of these specs is to avoid dependencies from outside the filesystem? \tIs the ReDD specification meaningful outside of a Dflat given that any one ReDD directory knows nothing of its successors and predecessors, or is it dependent upon Dflat? \tCould a BagIt bag live inside of the ReDD reserved \"full\" directory?  That is, could the \"full\" directory be marked up appropriately to be a BagIt bag? \tHow many tools exist for these specs?  I notice there's code in CPAN for Pairtree and Namaste, which is a fabulous start.  Tools are the difference between YAMF (Yet Another Messy Filesystem) and reliably managed curation services.  Granted, tools such as cp and emacs already exist and are part of the appeal of these micro-services, but there's also tremendous room for error if operations are all done \"by hand.\" \tTo what extent has CDL transitioned to using these specs/tools?         Are other institutions using these specs/tools?  I have heard tell that digital library folks from the University of Michigan and the University of North Texas may be involved.   I hope I don’t sound overly critical.  I’m really glad our colleagues at the California Digital Library have written these specifications and applied their deep experience to what could be a transformative paradigm2 in the digital curation world.  Kudos to them!                  Perhaps it’s more in line with the specs to refer to this space as “a managed filesystem that drives repository and curation services,” given the CDL philosophy that preservation is not a place/repository.  But it’s easier to say “repository,” so there you go. &#8617;                  Please excuse the fanboyishness; this filesystem fetishism is exciting stuff! &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2009/09/26/exploring-curation-micro-services/",
        "teaser": null
      },{
        "title": "Getting Started (with Digital Library Architecture)",
        "excerpt":"I’m not particularly fond of the term “digital library architecture” because it suggests capital ‘A’ architecture.  It’s big; it’s monolithic.  The term leaves lots of room for abstraction and complexity and formalism, and I am a fan of simplicity and practicality.  I have tried to be upfront about my inclinations, and so I can only assume that the hiring committee found them acceptable. Great.   My charge as digital library architect is to work with a team of folks in ITS and the Libraries, among other groups on campus and in the digital library community abroad, to design systems for curation of digital content.  It’s a huge problem space involving dozens of stakeholders across campus, terabytes of content, and diverse use cases and requirements.   At the outset, I expect to work – especially with the fellow members of the Digital Library Technologies team, the Libraries’ digital collections curator, and the Digital Initiatives Steering Committee – on building out the university’s e-Content Stewardship program.  I intend to start with a ton of contextualization: reading functional requirements, sitting on various committees, tracking down key articles and local wiki pages, and engaging folks over caffeinated beverages.   Beyond that, I imagine we will want to turn to building a team to work on a pilot project. There are many potential pilot projects competing for time, to be sure.   I’m excited to be working on this at Penn State, and can’t wait to dig in.  I’ll share more as I go along, and hope that folks reading along will participate in the discussion.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/12/19/getting-started/",
        "teaser": null
      },{
        "title": "Micro-musings after a night of oenophilia",
        "excerpt":"Here’s a list of questions I hope to grapple with.  And, yes, I am fixating on terms a bit here.  I blame it on the philosophy courses.      What is a digital library architecture?   When we talk about digital library architectures do we normally tend to talk about systems?   Is it wise to have a system-centric view of digital library architectures – i.e., “just install Fedora/DSpace/Drupal/aDORe!” – or should we think instead in terms of APIs, standards, and access requirements?  Maybe this is a false dichotomy.   Do digital library architectures need to be so esoteric, or may they reduce to garden-variety information architectures?  Formulated otherwise: are our problems really that special?   How do repositories and digital library architectures intersect?   Would a UNIX filesystem w/ certain naming and directory conventions suffice as a digital library architecture?  Formulated otherwise: would the California Digital Library’s curation microservices suffice?   How do HTTP, web architecture, linked data, RDF-based ontologies, and REST help us with digital library architectures?   How might messaging architectures such as AMQP, XMPP, and OpenSRF fit into the digital library problem space?   Am I overthinking this/fixating too much on the phrase “digital library architecture?”   I’ve been trying to track down relevant literature on digital library architecture and have found a modest number of articles.  Any suggestions would be much appreciated.   Happy holidays, folks.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/12/21/micro-musings-after-a-night-of-oenophilia/",
        "teaser": null
      },{
        "title": "Forking",
        "excerpt":"I am not certain if this is a good idea or not, but I decided to set up a “work blog” as I set off on my new path as a digital library architect.  The lines between this blog and that blog are fuzzy – most lines are, in my eyes – so bear with me.  I’ve never been a prolific writer – it’s always a chore, an activity I simultaneously want to do more of, and do better, and also struggle mightily with.  (It’s the public school education?  HAR HAR!)   But even so, the posts here may slow yet more.  Or maybe that will be true of the new blog.  We shall see.   I’ve found that microblogging has largely filled the blogging gap for me; I’m more comfortable, somehow, posting smaller, more easily digestible “thoughtlets” via Twitter/identi.ca/Facebook.  Perhaps I’ve succumbed to attention deficit disorder, flitting from one tiny undeveloped idea to the next.  It’s probable but I digress.   If you’re interested, you can follow along as I grapple with questions about digital library architecture.  Comments are most welcome, both here and there, as always.  ","categories": [],
        "tags": [],
        "url": "/blog/2009/12/22/forking/",
        "teaser": null
      },{
        "title": "Data management discussion",
        "excerpt":"My first week on campus is cruising by.  On Monday I sat in on a meeting called by our Chief Information Officer (and my boss’s boss), Kevin Morooney, to discuss what data management means to folks in Penn State’s central information technology group, Information Technology Services.   Attendees came from all across the ITS organization: Administrative Information Services, Security Operations and Services, Consulting and Support Services, Teaching and Learning with Technology, Marketing and Communications, Research Computing and Cyberinfrastructure, Identity and Access Management, and Digital Library Technologies (my department).   The meeting was chiefly for exchanging information, for reconstituting the discussion about what we talk about when we talk about data management.  Some of the preceding high-level work done in this area has been around a business intelligence initiative – though I’m not sure what this means, exactly – and development of a University data classification scheme.   I wasn’t terribly surprised to learn how different the perspectives around the table were, but there were also some common themes such as security and identity management.   We’ve all got data, and so data management is done by just about everyone everywhere.  It gets very tricky, naturally, when you start talking about data management planning across an institution as large and diverse as Penn State.  Kevin asked for folks to mention examples of institutions of higher learning that have tackled data management at the institutional level.  Most of the examples given were private, resource-rich schools – no shock there, perhaps.   I’ve been somewhat disconnected from academia for a few years now, so I was hesitant to mention my perhaps outdated examples.  I’ve had a chance to poke around and verify what I suspected, in the meantime.           Indiana University’s Information Policy Office has published a Data Management website listing policies, guidelines, a classification scheme and dictionary, data managers, and the membership of the Committee of Data Stewards, “a group… responsible for establishing policies, procedures, and guidelines for management of institutional data across Indiana University.”            The University of Washington has also been active in this area.  UW Technology and the eScience Institute published a report, Conversations with University of Washington Research Leaders, on “a large-scale effort to assess the information technology needs of the Univeristy of Washington’s top researchers. … [T]he goals of the project were (1) to understand how UW researchers currently use technology and anticipate using technology in the future to support their research activities, and (2) to identify the resources and services they need to maintain and build upon their remarkable record of success.  To accomplish these goals [UW Technology and the eScience Institute] interviewed 127 researchers.”  The first recommendation in the report, which contains an entire section on data management, is that the University should provide a new data management paradigm.       Many big questions remain.  What does “data management” even mean?  Who are the stakeholders and what are their expectations?  How would data management responsibilities be divvied up?  In which directions should outreach be concentrated?  What data is even out there?   Among the big and challenging topics, thoroughly intertwingled, are: security, privacy, and access control; scalability and performance; provenance and auditing; metadata and discoverability; persistence; access vectors; buy-in; the notion of “one-sized-fits-allness;” trust (five huge gigantic scary letters); incorporation w/ existing workflows; and probably eleventy zillion others.   All of which I solved in a dream last night – and then forgot.  It probably involved a cloud (or SOA, right?).   The meeting adjourned shortly after determining that there were no immediate follow-ons or action items, except to keep thinking about data management and looking for good reasons to reconvene the group.  (Sidebar: I was involved with an effort within the Libraries at the University of Washington to develop and conduct an institutional data census, a difficult and involved process aimed at answering at least one question – what data is out there? – and so I’d be stoked to see a similar effort at Penn State.)  This ad hoc group will probably meet once or twice a year and I look forward to watching things develop in this space.  I learned a bunch from the meeting, and that’s extremely valuable while I put my work here into context.   When isn’t learning valuable?  ","categories": [],
        "tags": [],
        "url": "/blog/2010/01/06/data-management-discussion/",
        "teaser": null
      },{
        "title": "e-Content Stewardship program kick-off",
        "excerpt":"One of my primary foci is a new program jointly undertaken by the libraries and ITS, known as e-Content Stewardship.  (For more background information, Mairead Martin set the scene.)  The program is largely the brain-child of Mairead (Sr. Director of Digital Library Technologies), Mike Furlough (Assistant Dean for Scholarly Communications), and Lisa German (Assistant Dean for Technical and Collections Services), but has doubtless been shaped by many others in the libraries and scholarly communication circles.   Yesterday the three of those folks got together with myself and Patricia Hswe, Digital Collections Curator, to orient us – it’s Patricia’s and my first week at Penn State – and also to set some direction for how the two of us might carve out an initial project.   Our first project is quite a clever idea (for which I take no credit, since it came from Mairead, Lisa, and the other Mike): we will review the digital library platforms currently being used by the libraries.  In so doing we will better orient ourselves for later efforts, so it’s beneficial to us, but it’s also beneficial to the libraries and ITS to have new and fresh eyes looking at how folks are using our systems.  I’m particularly interested in what APIs the platforms support and how we might get them interoperating, in addition to how the products themselves are evolving – is the software moribund or under active development?  What’s the support like?  What’s the user community like?   Interop is but one piece of the puzzle.  There’s also structure of collections, relationships between items, organization of information, and so forth.  We will also want to talk with users of the systems to suss out usage patterns, features liked and hated, quirks, and so forth.  Administrators and developers, too, of course.   I’m looking forward to making some headway on this review.  I have some suspicions about what we may conclude, but I’m keeping those to myself and trying to remain as objective as I can so as to give the platforms a fair shake.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/01/08/e-content-stewardship-program-kick-off/",
        "teaser": null
      },{
        "title": "My agenda for Q1 2010",
        "excerpt":"My agenda for the first few months of 2010 is becoming clearer. Consider this a snapshot of the things that will be crossing my desk and bouncing around my mind.      Reviewing digital library platforms for the e-Content Stewardship Council   Reviewing functional requirements for an institution-wide repository of electronic records   Learning more about “big data” and continuing the data management discussion    Evaluating the DLT archival storage prototype and joining the technical team of the Data Storage Working Group   Evaluating next-generation information discovery tools for the libraries with I-Tech   Evaluating change management solutions with a team from Penn State’s ITANA group   Working on requirements for a draft institutional identifier standard with the NISO I2 working group   Attending Code4Lib 2010   Meetings, meetings, meetings   Continuing to absorb as many of the following as possible: strategic plans, project portfolios, process management documents, and various and sundry reports, wikis, and blogs   My plate is rather full; fortunately, for now, I’ve got a big appetite.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/01/13/my-agenda-for-q1-2010/",
        "teaser": null
      },{
        "title": "What's in a title?",
        "excerpt":"Hamlet.  The Declaration of Independence.  Gutenberg Bible.  Holy Roman Emperor.  All of these are words, but more than being words they are titles.  Titles are names we give things, things such as works of art and stations we hold.   Names are important to the extent that being able to talk about things is important.  Names identify things.  The way we indicate things when we talk about them, typically, is names or substitutes therefor, e.g., deixis.   Ergo, titles are important.  That’s been my line of thinking, at least, as I continue to struggle – here meant in a light sense, such as “I am struggling with eating this delicious cookie” – with what precisely it means to have a title with the word “architect” in it.   I say that with a twang of cognitive dissonance, for I know and understand very well what I do on a daily basis and what I will be doing in the near future.  Perhaps then titles are not very important, or I should say, more important than titles is knowing what is expected of you and exceeding those expectations.  Shape your title rather than allowing it to shape you.   Ergo, maybe titles aren’t equally important in all contexts.   Self-help tropes aside, I still wonder about what folks’ expectations of a digital library architect are.  There is a line of thinking in libraries that our problems are unique rather than of a class.  Some argue fiercely that library issues are, in fact, not special.   I’m undecided.  For instance, would a digital library architect have any concerns or areas of expertise an IT/enterprise architect would not?  Or does digital library architecture amount to little more than a re-brand reflecting the “we’re special” way of thinking?   Related, I would wager that the number of titular digital library architects is much smaller than the number of folks doing architecture work in digital libraries.  Digital repository librarians and library systems analysts, etc., I’m looking at you.   Why am I thinking about this?  In my last two jobs as a software developer in academic and research libraries I was spoiled by being in a large and vibrant community of similar folks: code4lib and also the Access folks up north.  I’m looking for the same in my current job: some forum, conference, mailing list, or what have you, where there is discussion of architectural issues in the digital libraries context.   For now, I have contented myself that a digital library architect is a technologist who thinks architecturally about digital libraries.  What does that mean?  Someone who, to mix metaphors mightily, puts his or her arms around the big picture (rather like an art thief).   What does that mean?  Someone who knows all of the systems and standards and protocols and workflows and operations and the connections between them, in an institution, in the context (typically) of serving digital content over the web (though this context is expanding into other areas such as institutional e-records management and research data curation).  Said someone will probably have been hired in fact not merely to know all of that mess but to think systematically and strategically about whether all of that mess meets needs and requirements and best practices, and not only think about that but work deliberately to make that so.   Is there a community for such folks?  There are many possible related communities (and here I’m intentionally casting a broad net by mingling conferences, lists, and professional organizations): code4lib, Access, Open Repositories, digital-curation, ITANA, EDUCAUSE, ASIS&amp;T, DigCCurr, iPres, SPARC, CNI, DLF/CLIR, and so on.  Heck if I know.  Do you?  ","categories": [],
        "tags": [],
        "url": "/blog/2010/01/20/whats-in-a-title/",
        "teaser": null
      },{
        "title": "JSONovich crawls into the future",
        "excerpt":"For those of you who use JSONovich rather than, e.g., JSONView, I’ve tweaked the plugin (now at version 1.3) to work with Firefox 3.6.   I updated the version up on the Mozilla site as well, but things tend to stay in the sandbox for months at a time.  (For instance, I submitted version 1.2 back in November, and it’s not yet available.)  Feel free to install via my page.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/01/24/jsonovich-crawls-into-the-future/",
        "teaser": null
      },{
        "title": "Web discovery and library resources, or, SEO 101",
        "excerpt":"I attended the Penn State library faculty research colloquium on Wednesday, during which I learned all sorts of things about the interesting research being done by my colleagues in the University Libraries.  One very interesting talk was by Doris Malkmus, one of our archivists, who was studying how history professors use primary sources, online and otherwise, in their undergraduate lectures.  It was no surprise to me that the #1 discovery method for online primary sources was Google, and that institutional repositories hardly rank at all.   (Sidebar: I wonder: with online content fragmenting, multiplying, and getting remixed and aggregated, does the definition of “primary source” strain for digital networked resources?)   This discovery elicited a number of responses about how difficult search engine optimization is and how we really need to ramp up our marketing efforts.   I wouldn’t argue with either reaction, really.  I do sense a huge missed opportunity here, though, one that we are perfectly capable of not missing.  And let me be perfectly clear: I’m no SEO expert.  But let me also say that I’ve seen, firsthand, major SEO advancements in libraries I’ve worked at, and much of the work was pretty straightforward.   I tweeted my “SEO for dummies” list and got a couple of very good responses in addition to a retweet or two.  Here’s what I said:      Googleability = increased findability + low-cost marketing. How do to it: 1. allow crawlers; 2. clean URLs; 3. rich item metadata; 4. links.    To this list, folks suggested I add “0. stable application” and “5. sitemaps”, both key suggestions, though I don’t have much experience with sitemaps so I won’t say more about those.   What’s my point?  It’s not rocket science to get our web resources discoverable on Google and the other major search engines.   What’s the value in that?  More people are going to find library materials via a Google search than by navigating the dark alleys and dead ends of library websites.  Yes, our silo boundaries have been useful to us to keep dissimilar materials apart for management and such, but no, they are totally useless to our users.  My former colleague Ed Summers reminded me today that a silo is not really a silo if it’s on the web.  Merely being on the web isn’t enough, though, and here are the simple and practical lessons I’ve learned that may be the difference between getting found on Google and “NO JUICE FOR YOU!”      Stable application: If your site isn’t reliably up, user-agents will have a hard time finding it.  That means disgruntled users and crawlers who never fully find you.   Allow web crawlers: Unless you have a really compelling (read: legal) reason to disallow crawlers (and robots and spiders, oh my), you really ought to allow them.  But only if you care about discoverability.  If your app cannot handle the load of crawlers, go back to #1 and start over.  Hire an engineer who knows about scale and performance, preferably.  (See anecdote 1 later in this post.)   Clean URLs: I’m not sure this is entirely necessary for SEO, to be honest, but it does seem like a common practice among those who are good web citizens.   Rich item metadata: Collection-level metadata is not good enough.  Collections are a useful abstraction for librarians but less so for users.  Rather than impose a collection view upon users, move relationships among items and common metadata elements into item pages.  (See anecdote 2 later in this post.)   Links: Link out to stuff.  Get folks to link in to your stuff.  (See anecdote 3 later in this post.)   Anecdote 1: The Library of Congress has a digital newspaper application called Chronicling America.  At the time it was created, it served as a test bed for some technologies that had not seen wide uptake at the Library, but in time its developers realized the architecture couldn’t keep up with the traffic coming in from the web crawlers.  A robots.txt file was created restricting crawlers and time went by.  The application was rebuilt from the ground up with the intent “to increase the usability of [the] application by providing faster responses to HTTP requests, allowing these requests via standardized APIs, as well as allowing all pages to be crawled by search engines.”  The results were remarkable: average hits per day grew from roughly 75,000 to nearly 500,000.   Anecdote 2: When the Library of Congress went live with the World Digital Library, clearly helped by a massive press event at UNESCO in Paris (the largest such event in UNESCO history, apparently), its developers watched the mentions roll in via Twitter Search.  The most interesting thing I learned that day is despite all the cool maps and timelines and facets, users were primarily linking directly to item pages (each of which was helped by surfacing all of the rich descriptive metadata as well as links to related and similar items).   Anecdote 3: The digital initiatives team at the University of Washington libraries has done some studies assessing the impact of adding links to their digital collections from Wikipedia pages.  Usage spiked after the links were put in place, thanks to Wikipedia’s popularity and the mechanics of Google’s PageRank algorithm for judging relevance.   These are practical steps we can take, and frankly may be the best marketing (judging cost v. impact) libraries can do to increase usage of our digital materials.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/02/19/web-discovery-and-library-resources-or-seo-101/",
        "teaser": null
      },{
        "title": "Braindump for Q2 2010",
        "excerpt":"My my, has it really been three months since I wrote up my agenda?  I’ve been busy chipping away at the agenda so I thought I’d document my progress now that Q2 is underway.           Reviewing digital library platforms for the e-Content Stewardship Council       The platform review project that our digital collections curator and I have undertaken continues.  We began the project by having folks demonstrate each platform and how they use it, and have been busy with small, informal interview sessions with many of the same folks but also others who work outside of the Libraries.  We have a few more interview sessions to conduct and document, so the data gathering portion of the project is nearly complete.  In the meantime we’ve been discussing evaluation criteria.  We started off with a short list of criteria, but then noticed the criteria Purdue are using for their comparative analysis of institutional repository software and adopted those instead.  We sketched out a structure for the final report, which we hope to finish in May.            Reviewing functional requirements for an institution-wide repository of electronic records       This work is still under way.  We have a set of well-documented functional requirements for an e-records repository service but have yet to make progress on building anything.  We’ve been talking about applying for a grant to help fund some additional staffing which might be used to help build out proof-of-concept curation services (preservation, provenance, description, discovery) for e-records.  I’m really keen on applying curation micro-services, such as those used at CDL, to the e-records domain.  I see this effort as benefiting both the curation micro-services community and the e-records community – not to mention our own electronic records initiatives here at Penn state.  An all-around win, if you ask me, but then I’m biased.  This will be a major activity in the latter half of this year continuing into the next.            Learning more about “big data” and continuing the data management discussion       Our content stewardship program will doubtless need to address research data.  We’re not there yet.  In the meantime, Penn State’s ITANA chapter will be pulling together a working group on the technological and architectural challenges of research data.  Jeff Nucciarone and I will be chairing the group.  In the meantime, research data has been on my mind for two reasons: Michael Lesk gave a talk at the information school urging libraries to turn their attention to research data; and I’ll be attending the Research Data Access and Preservation Summit in Phoenix later this week.            Evaluating the DLT archival storage prototype and joining the technical team of the Data Storage Working Group       The Data Storage Working Group effort has been repurposed.  The steering team will continue to meet informally and discuss archival storage and curation needs across the campuses.  The technical team has been dissolved, and the majority of us (who already work together in DLT in support of the same mission) will continue to work in this space.            Evaluating next-generation information discovery tools for the libraries with the Libraries’ Department of Information Technology       The RFP process continues.  We hope to have wrapped up our evaluation by the beginning of summer.[       ](http://www.libraries.psu.edu/psul/itech.html)            Evaluating change management solutions with a team from Penn State’s ITANA group       I haven’t found much time to stay involved with this team, unfortunately, but their work continues apace.            Working on requirements for a draft institutional identifier standard with the NISO I2 working group       The I2 working group is putting finishing touches on a draft standard and on core metadata required to identify institutions.  We hope to share this draft and put out a request for comments in the coming months.  I’ve been modeling the I2 domain in RDF both for more RDF experience and also with the hope that an eventual I2 core service will be exposed as linked data.            Attending Code4Lib 2010       You can tell how good a code4lib conference is by how little you remember of it.  By that measure, this year’s conference was the best yet.  Some of the highlights for me: 1) Linked data, a pattern for exposing resources and metadata via the web, continues to be a hot topic among cutting-edge library developers. There was a focus this year on how to participate in the linked data web in practical and lowish-barrier ways.  The speed with which concepts move, at code4lib, from “novel, and interesting to a few” to “widely talked about and deployed” is dizzying; 2) Software development practices continue to mature in libraries.  We’re talking more and more about test-driven design and agile development.  While these methodologies are beneficial to developers themselves, I find this remarkable because it means the gap between coders and stakeholders is being bridged, and that means better and more usable software, and happier users; 3) Repositories are not typically a hot topic at code4lib, but there were a number of prepared talks, lightning talks, and breakout sessions on the topic.  Fedora tends to be the repository most often talked about, if only because it is the repository that requires the most hacking – and these are the people doing the hacking.  What I found interesting this year was the dissatisfaction with monolithic repository software packages, and the movement towards “homebrewed”, though standards-based, repository services, such as those being advocated by the California Digital Library.            Meetings, meetings, meetings       The meetings, they continue.            Continuing to absorb as many of the following as possible: strategic plans, project portfolios, process management documents, and various and sundry reports, wikis, and blogs       And this continues as well, though it’s hard to find time to contextualize when you’ve got actual tasks and deadlines.       And here are some new and upcoming things.           I’ve written about my search for a practice-oriented curation technology/architecture community, and I’m glad to say I’ve made some progress on finding said community.  I’ve been part of a conversation revolving loosely around the digital-curation group and that conversation has now turned to planning a curation technology workshop which we’re called CURATEcamp (CURAtion TEchnology Camp).  I hope to have more details to share soon.            I am attending Open Repositories 2010 in Madrid this July.  I expect to learn about how folks are using repository systems such as Fedora, DSpace, and ePrints, but am more interested in all the other stuff happening on the periphery.  There has also been talk of a curation micro-services birds-of-a-feather session, which might serve as a good event to get potential CURATEcampers talking.            I’ll be in Washington, DC in a few weeks working on a team to evaluate IMLS National Leadership grant applications.  This will be a new experience for me, and one to which I need to devote a significant chunk of time between now and then, so I’m excited.  It will be interesting to see what folks are doing outside of Penn State, and also to get an idea for what sorts of projects wind up getting funded.       I have some vague ideas for project charters but have yet to really flesh them out.  One involves some collaborative development on tools around curation microservices, to be used and evaluated by honest-to-goodness curators with honest-to-goodness data, and the other is about benchmarking some distributed filesystems.        Techies at Penn State need to talk more.  I want a BarCamp-style event for PSU techies so that we can discuss issues across departmental boundaries.  Administrators have been nothing but supportive of the idea, and now I just need to find some time to sketch what I have in mind.       Digital Library Technologies, my department, is hiring!  We’re looking for someone to come develop software to support our content stewardship program.  Like writing code?  Interested in how data is curated, stored, and discovered at scale?  Consider applying.  (Will link to position when it goes public later this week.)   Braindump complete.  Brain now empty, except to say: boy, State College sure is lovely in the spring.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/04/06/braindump-for-q2-2010/",
        "teaser": null
      },{
        "title": "I2: Resource Description",
        "excerpt":"I can hardly believe it’s been eight months since I last wrote about the NISO I2 project.  A lot has changed since then1.  I continue to work on I2 however; they won’t get rid of me that easily.   In the last post, I wrote:&lt;blockquote&gt;The next step is to build upon the report to draw yet more conclusions from the data â€” there’s an awful lot there â€” and flesh out some repository use cases for institutional identifiers. The I2 core group is moving quickly towards finalizing identifier metadata elements so that a standard may be drafted, and I think having some use cases documented will help drive the standard in a direction the community can get behind.&lt;/blockquote&gt; Since that time, the three scenario groups – Electronic Resources; Institutional Repositories and Learning Management Systems; and Library Resource Management – have concluded their work.  The work of the scenario groups included surveys of over 300 people working in these fields.  The survey results have been analyzed and reports were posted on the NISO website.  These reports have been used to flesh out use cases for an institutional identifier.  Upon completion of this work, the scenario groups were disbanded and work continued in a broader I2 working group.   The I2 working group has concentrated its work on analysis of similar standards and, as I alluded to earlier, significant effort has gone into defining core metadata to identify institutions, such as institution name, institution type, location information, variant identifiers, domain name(s), URL(s), and (optionally-typed) relationships to other institutions.  During these discussions it was difficult for me to hear the issues and needs around I2’s metadata and identifiers without linked data springing to mind.   While we are designing a standard and not a system or a service per se, it seems useful to include in the standard an informative section about implementation and architecture2; I find that reading standards is much easier on the brain when you get not only the standard itself but some examples of implementation, and that will be true as well, one hopes, of I2 standard implementers.  To that end, the group will be producing an XML schema of the I2 metadata elements and also an RDF schema.   I have been working on the RDF for I2 on and off for the past month or two.  Below are my impressions, as someone who is new to modeling in RDF, and the procedures I used to produce the draft RDF schema.  Despite their names, RDF schema and XML schema are quite different3.  The XML schema is a tool for validating an XML-based document or record, and it’s a common tool for modeling metadata in libraryland.  Not so with RDF schema, where the notion of document or record is replaced by the notion of a set of triples.  The focus in RDF is on the triple not on the document, and so validation of documents or records is not the point of RDF schema.  This took some effort to wrap my mind around.   Before I modeled I2 in RDF, I sketched out a domain model of I2 by copying relevant bits of information from I2 documents and pasting them into a text editor.  Then I put them into classes.  In I2’s case, the domain model contained three classes of things: metadata elements about an institution, relationships between institutions, and types of institution.   I gathered some examples of relatively simple RDF schemas and transformed them into the Turtle serialization format4 for ease of reading, using them as a template for the I2 schema.   In the RDF schema (RDFS) specification, there are two classes of things in the domain model: classes and properties.  If you are familiar with object-oriented programming, chances are you already grok this way of modeling, but otherwise, generally: a class is like a type and a property is an attribute.  If I were to model myself in RDF schema, then, I might say I am in the class of human beings, and one of my many properties is having a particular birth date, and another is having been born in a particular city.  The next step was to take the I2 domain model (metadata elements about an institution, relationships between institutions, and types of institution) and decide whether each thing was a class or a property.  I decided that the former two were sets of properties and that type of institution could be modeled as a set of classes.   Having a conceptual model of I2 and how it fit into the RDF schema way of thinking about things, I wrote a simple ontology defining one RDFS class per type of institution, and one RDFS property per metadata element and one per relationship type.  This would have sufficed as an ontology.   Exposing RDF-based resources on the web as linked data, however, represents an opportunity for metadata element-level interoperability at global scale.  In order to interoperate with the existing corpus of linked data available on the web, I went through the new I2 ontology and looked for areas where I could re-use, or subclass or otherwise link to, classes and properties already defined in more widely-used ontologies.  I realized at this point just how different coming up with a new XML document format was from writing an RDF ontology; whereas I might have wanted the former to be comprehensive and inclusive of every single aspect within the I2 domain model, my goal with the latter became to eliminate it (by trimming it down to only those bits which are not defined elsewhere).   Since the RDF ontology for I2 is not inclusive of the entire domain model, it seemed necessary to produce another reference document: a set of instances of I2 resources showing the mingling of new I2-specific classes and properties with well-defined classes and properties from other ontologies.   I shared rough first drafts of these documents and received very helpful feedback from some folks who are better-versed in this than myself.  I’ve now incorporated their feedback into the latest I2 ontology and instance document.  I hope to include both of these into a draft of the I2 specification which will go out for comment in the coming months.  Here’s the latest ontology and the latest set of instances.                  I’ve moved and changed jobs, in fact &#8617;                  This practice seems more or less common in my (admittedly limited) experience, cf. the unAPI specification. &#8617;                  This reflection should come as little surprise since RDF and XML are different kinds of things: RDF is a data model and XML is a serialization format. &#8617;                  Using rapper, a nifty little tool. &#8617;           ","categories": [],
        "tags": [],
        "url": "/blog/2010/05/19/i2-resource-description/",
        "teaser": null
      },{
        "title": "Institutional Identifiers and RDF",
        "excerpt":"In my last braindump, I wrote:      The I2 working group is putting finishing touches on a draft standard and on core metadata required to identify institutions.  We hope to share this draft and put out a request for comments in the coming months.  I’ve been modeling the I2 domain in RDF both for more RDF experience and also with the hope that an eventual I2 core service will be exposed as linked data.    I’ve now documented this experience on my other blog, which is the home of my I2 ramblings.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/05/19/institutional-identifiers-and-rdf/",
        "teaser": null
      },{
        "title": "JSONovich: Now with code-folding!",
        "excerpt":"Thanks to a clean patch from Sean Coates, I’m releasing v1.5 of JSONovich.  It now supports code-folding.  Great hack, Sean!  ","categories": [],
        "tags": [],
        "url": "/blog/2010/05/19/jsonovich-now-with-code-folding/",
        "teaser": null
      },{
        "title": "Digital curation community",
        "excerpt":"I wrote before about a potential curation technology unconference which has been dubbed CURATEcamp 2010.  Not in my wildest dreams could I have imagined just how receptive folks have been to the idea.   An ad hoc planning team – consisting of folks from Penn State, the California Digital Library, the University of California-San Diego, and the Library of Congress – has been hard at work bringing this idea to life.  On June 15th, we announced on that registration for the event opened.  Eight days later, we announced that all seventy-five slots had been filled.  Fret not, though; you can still be added to a waitlist.   The camp is now yours, digital curation community – let’s see what you’ve got.   CURATEcamp 2010 is but one of many events within our community.  I’d like to highlight some others.      Delphine Khanna (UPenn) and Stephen Abrams (CDL) have planned a microservices BOF session at Open Repositories 2010.  It will focus on discussion of community around curation microservices.   Peter Van Garderen has proposed a microservices BOF at iPRES 2010.   Delphine Khanna has proposed a pre- or post-conference session on curation microservices for the DLF Fall Forum this November.   Patricia Hswe, Declan Fleming, and I have had a workshop proposal accepted for IDCC10, and the provisional name for this workshop, a full-day unconference, is CURATEcamp II.   Whereas CURATEcamp 2010 focuses on the curation microservices approach, CURATEcamp II is a bit more general.      ABSTRACT: As the community of digital curation practitioners has grown, so has the need for collaboration and community.  A small number of communities have been formed around digital curation, a few of which focus on the technical aspects of the practice.  Extant communities address the implementation and support needs of specific curation platforms, without broader focus on common services and potential points of intersection. There is however a rich ecosystem of tools, practices, and standards around these platforms, and some that require no such platforms, that have potential to benefit the wider community of practitioners. CURATEcamp II is an unconference-style workshop for practitioners of digital curation to share best practices and discuss tools and technologies in a free-form and highly interactive forum.  Topics of interest might include identifiers, versioning, transfer, packaging, object structure, filesystem usage, archiving / storage, metadata standards / vocabularies, discovery, and interoperability. The unconference format ensures that all participants are actively engaged in the workshop and gives everyone an opportunity to contribute.  Activities may include roundtable discussions, presentations, whiteboard sessions, collaborative software development, and whatever else emerges from the collective creativity of participants.     AIMS: CURATEcamp II is an opportunity to build a community of practice around curation tools, that bridges system-specific gaps that have formed in the community. It will encourage discussion about curation tools and practices across software-, project-, and institution-specific boundaries, and attempt to identify best practices and points of collaboration across these boundaries.  The community that CURATEcamp II nurtures is intended to persist beyond the end of the IDCC, so another point of discussion will be around how to maintain connections between face-to-face gatherings. The informal approach of CURATEcamp II might also serve as a way to model knowledge sharing for the curator community, not unlike what occurs at BarCamp events, which are loosely structured but highly productive participatory sessions.     AUDIENCE: CURATEcamp II will be of interest to digital curation practitioners (curators and technologists alike), especially those who have been using and building tools and architectures, and digital curators with experience assessing or evaluating curation tools and services.    This is an exciting time to be working in the digital curation community!  Wondering how to get involved?  Hop on over to the digital-curation Google group and join the discussion; it’s just getting started.   P.S. CURATEcamp 2010 would not be happening without the active engagement of all the folks doing the planning.  Thanks to: Declan from UCSD for proposing the idea for the camp over Belgian beer at the Thirsty Monk during Code4Lib 2010; Dan from LC for focusing on the practical; Ed from LC for evangelism and support; Perry from CDL for all of his work with the conference venue and the registration system; my colleagues at Penn State for their support; and both Penn State and CDL, without whose contributions and commitment the camp would not have been possible.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/06/24/digital-curation-community/",
        "teaser": null
      },{
        "title": "Braindump for Q3 2010",
        "excerpt":"   Reviewing digital library platforms for the e-Content Stewardship Council – Patricia and I have completed all user interviews and platform demonstration sessions, and have finished evaluating all four in-scope platforms (CONTENTdm, Olive, DPubS, and ETD-db) along a set of twenty-odd criteria defined in a comparative analysis project at Purdue.  Next up is identifying themes from the evaluation for our report’s executive summary.  We had hoped to finish this work in May, but apparently summer is a hard time to get stuff done.  Who knew?   Institutional repository of electronic records – Work has begun on our e-records system via the inclusion of records use cases in another pilot project.  More on that later.   Learning more about “big data” and continuing the data management discussion – I attended Research Data Access and Preservation Summit in April. A number of themes emerged from RDAP: 1) methods for involving researchers in curation activities, 2) the user-friendliness of the data deposit process, and 3) the boundary between preservation and curation, caused by the dynamic nature of research data and barriers to repository ingest such as complicated processes and a write-once assumption.  We at Penn State have not yet gotten our big data focus group, under ITANA, off the ground but hope to do so later this year.   Storage strategies – Following the dissolution of the Data Storage Working Group, Digital Library Technologies continued the discussion of storage strategies to guide purchase, allocation, and management of storage from the short- to the mid-term.  We have just this week written a project charter to explore the idea, culminating in a strategic plan for storage in December.   Evaluating next-generation information discovery tools for the libraries with the Libraries’ Department of Information Technology – The RFP process has finished and we have selected a product that meets our many needs.  We will be announcing our decision as soon as the ink dries on the paper.   Working on requirements for a draft institutional identifier standard with the NISO I2 working group – The I2 group distributed a survey about features and requirements of the draft I2 standard, and has begun analyzing the results.  Feedback has been provided primarily from the library sector, and has largely validated our work thus far.   Attending Open Repositories 2010 – See conference report.   Planning Curation Technology Camp (CURATEcamp) 2010 – Since I last wrote about the camp, the conference planning group has been busy dotting “i”s and crossing “t”s.  We’re all looking forward to the camp which is coming up soon (mid-August).   Curation microservices pilot – A short-term pilot project involving software developers and curators will explore a number of strategic aims of the Content Stewardship Program: defining curatorial requirements, building and testing a curation architecture, engaging software developers and curators at other institutions, treating data in a cross-platform manner, exploring roles and workflows that cross unit boundaries, and building a testbed for electronic records curation services.  Project work will include curating copies of a small sample of data selected from e-records, CONTENTdm, Olive, DPubS, and ETD-db; building and integrating existing lightweight digital curation tools based upon curation microservice specifications; applying those tools and specifications to curate the sample dataset; examining the benefits, costs, and limitations of the microservice approach; and determining if microservice-based curation architecture is viable at Penn State.   MetaArchive implementation roadmap – Penn State is now a member of the MetaArchive distributed digital preservat cooperative.  I am working with a team of four on an implementation roadmap, detailing a timeline, new roles that will need to be defined for our involvement, and hardware specifications.  This is a short-term project.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/07/02/braindump-for-q3-2010/",
        "teaser": null
      },{
        "title": "Impressions from Open Repositories 2010",
        "excerpt":"One minor concern I brought to the conference, which has roots in my attendance at the 2007 conference, was whether it would be too system-oriented to be relevant, since Penn State doesn’t plan to use Fedora, DSpace, or ePrints.  I was pleased to see the increased attention to alternative approaches to preservation and to repositories as a set of services rather than (necessarily) as a system.   Penn State’s institutional digital stewardship program is investigating curation microservices, such as those developed by the University of California Curation Center, as an architecture for digital curation. So I came to OR2010 with an eye towards development in this space.  I wasn’t the only one; both the PASIG session and the DuraSpace strategic overview identified microservices as a trend, and a number of microservices seem likely to be built into the 1.7 release of DSpace.   I attended the curation microservices BOF, which was well-attended taking into account it was up against a developer challenge event – institutions represented include Universitat AutÃ²noma de Barcelona, Harvard, U. of Hull, California Digital Library, MIT, UNC-Chapel Hill, San Diego Supercomputer Center, Penn State, Northwestern, U. of Pennsylvania, and Princeton.   We discussed our interests in the topic, experiences w/ the microservices approach, development of a community around microservices, the California Digital Library’s role in sustaining said community, and governance of collaborative software development and of the community.   The BOF covered a lot of ground in a short period of time, and we agreed to start having periodic open teleconferences to share information about microservices development.  We’ll also utilize the digital-curation Google Group for virtual communication, and use events such as Open Repositories, IDCC, and iPRES – in addition to Curation Technology Camp (CURATEcamp) events – for microservices get-togethers.  ","categories": [],
        "tags": [],
        "url": "/blog/2010/07/28/impressions-from-open-repositories-2010/",
        "teaser": null
      },{
        "title": "Ingest is a barrier to ingest",
        "excerpt":"Last week I attended the latest iteration of one of my favorite conferences, Code4Lib 2011, which included a full-day CURATEcamp hackfest as a pre-conference session (sponsored by the Digital Library Federation).  Rather than writing up a full report of the event – no one really reads those, right? – I wanted to comment on a conversation from the hackfest.   A group gathered to discuss digital forensics, specifically in the context of forensics work done pre-ingest.  I’ve heard other folks talk about pre-ingest processes and so I wondered aloud: what does it say about our repositories, and the ingest process, that we do so much pre-ingest?  The consensus was that the ingest process is frequently expensive.  A subgroup split off to explore this.   The ingest process is a topic I’m keenly interested in since we (Penn State’s digital stewardship program) are in the middle of building a prototype ingest application (“CAPS”).  If we can learn some lessons from our peers about how to make ingest easier and faster, the timing is right to build on these lessons and make novel, more interesting mistakes rather than boring, well-known ones.   Here are the barriers to ingest that were identified:      Identifiers are precious – Ingesting an object usually kicks off a series of processes, one of which mints a new identifier for an object.  There is a perception that identifiers are a limited commodity, that they are somehow precious or rare.   Promise of permanence – There is a perception that ingesting an object creates a contract for the permanence of that object.  The contract may be illusory depending on the “repository” into which the object was ingested.   Findability – Once an object is ingested, it is difficult to find.  I would have liked to pursue this point a bit further.  What it suggests to me is that in some contexts, the repository has not been sufficiently incorporated into the workflows or work environments of those doing the ingest, so it feels like alien territory rather than the local filesystems and mapped drives they are accustomed to.  Pure speculation on my part.   Complex downstream workflows – Given that ingest is a series of processes, there is concern that “just ingesting something” might cause breakage downstream.  For instance, if an object is ingested, is it automatically published somewhere end-users can get to it, and has the object been fully prepared for publication?  One such workflow might be automatic generation of derivatives, which is an expensive operation for certain formats and large files.   Rights – Related to the above bullet, there is concern that end-user access rights be cleared in advance to ingest, for fear that the object will wind up in the wrong hands.   Metadata – The ingest process requires too much metadata input.  This concern is tied to findability above, and together they suggest an all-too-familiar tension: how much metadata is enough to make an object findable later, and how much is enough to make the ingest process cumbersome?   Psychological factors – There is a mindset wherein curation happens outside of the repository and preservation happens inside – that these are distinctly different activities which happen serially if at all – in which case one might be loath to ingest an object until it’s “ready” for the repository, whatever that means.   Personal time – The ingester simply lacks the time to push the right buttons.   Software performance – The ingest process is slow due to lack of optimization, lack of attention to scale, lack of performance tuning, and so forth.   There are a number of lessons to be learned from the above.  I’ll write soon about those and how we’re applying them to our CAPS project at Penn State.   Have any barriers to add to the list?  ","categories": [],
        "tags": [],
        "url": "/blog/2011/02/11/ingest-is-a-barrier-to-ingest/",
        "teaser": null
      },{
        "title": "Ingest: Lessons learned",
        "excerpt":"Now that we have a by-no-means-complete-but-still-useful list of common barriers to ingest, I thought I’d share the lessons learned.  We hope to apply these lessons in building CAPS, our prototype curation services platform.      Create a namespace, or namespaces, for identifiers that far exceeds foreseeable needs – Our first namespace can accommodate 7,072,810,000 identifiers.  We’re using the Archival Resource Key specification for identifiers (each of which will be mapped to HTTP URIs), and the Python-based arkpy library for minting.   Decouple the ingest process from the publication process – We plan to build a small suite of applications and tools upon our curation services platform, the first of which is for what we’ve been unimaginatively calling “generic ingest &amp; management.”  The application is for authenticated, authorized users only – it’s a tool for curatorial operations not for end-user display.  The ingest application will never automatically publish objects and it makes the assumption that all objects are private to the curator until the curator decides otherwise.   Plan for scale and test performance from the outset – The current phase of development on CAPS was given a very ambitious deadline so we have not had the time to focus on performance and scale as much as we would have liked.  We have a list of areas to address in the next phase, however, and a laundry list of technologies to vet and test for our scaling needs.  We’ve also lined up a small team of folks to help out with system testing &amp; QA.   Make metadata input optional – We believe that curators, not systems, curate, and thus allow them to decide how richly objects ought to be described.  We intend to provide curators with tools that allow (and perhaps encourage) rich metadata to be attached to objects but as far as the “generic” ingest application (and the curation services platform underneath) is concerned, all elements in the data dictionary are repeatable and none are required.  We will be building similar “profiled” ingest applications for specific purposes in the near future, such as an ingest application for electronic business records, which will, however, be more stringent about metadata (and also about file formats, which the generic ingest app couldn’t care less about).   Allow stakeholders to drive decisions and, above all, communicate with users – This may be a meta-lesson, and it feels like the most important of them all.  Our development team for CAPS consists of our lead developer, a digital curator, an archivist, a metadata librarian, a project manager, and an architect.  Our project team is made up of our development team plus stakeholders from across the University Libraries including representation from our Digitization &amp; Preservation department, the Arts &amp; Architecture library, and University Archives.  Our project team meets for one hour a week – a Herculean task to find a mutually convenient slot, let me tell you – and our development team meets for fifteen minutes every morning.  The point here is that our stakeholders – the primary eventual users of the ingest application – are invested in the project, and they get a chance to see, criticize, and drive what we’ve developed every week as it evolves. Because we’re in the same room so often, we get to communicate certain points regularly, e.g., what you ingest is only as permanent as you wish; identifiers are not precious at all; the curation platform is there for you to use in ways useful to you, so the typical “don’t put that into the repo yet” mindset doesn’t apply; your stuff is as findable as the richness of your metadata, but we can provide other ways to find your stuff (full-text search, etc.); and so forth.  ","categories": [],
        "tags": [],
        "url": "/blog/2011/02/17/ingest-lessons-learned/",
        "teaser": null
      },{
        "title": "Understanding (e.g.) DOIs for data sets",
        "excerpt":"Data citation is a topic that frequently comes up in conversations around data management. During a call with a community of data curators yesterday, I was asked whether ScholarSphere supported DOIs for citing data sets.   I have to admit that while I understand the value of data citation — tracking use &amp; re-use, measuring impact of data sets independent of their publications, giving credit to data publishers, &amp;c. — I continually get stuck on how identifiers such as DOIs from DataCite or ARKs from EZID fit into the picture. Or, rather, why such indirect identifiers are valued more than the native HTTP URIs that are minted and managed by data repositories. Here I assume that these data repositories are run by institutions whose missions &amp; business interests include a commitment to persistence of content and identifiers held within their repositories. (Is that a faulty or naïve assumption?)   The argument for indirect identifiers — identifiers that point at and resolve to other identifiers — like DOIs usually goes like this: hey there, cultural heritage organizations and publishers have done a pretty poor job of persisting their identifiers so far, partly because they didn’t grok the commitment they were undertaking, or because they weren’t deliberate about crafting sustainable URIs from the outset, or because they selected software with brittle URIs, or because they fell flat on some area of sustainability planning (financial, technical, or otherwise), and so because you can’t trust these organizations or their software with your identifiers, you should use this other infrastructure for minting and managing quote persistent unquote identifiers.   SIDEBAR: That’s a lot of becauses, all of which (to be perfectly frank) are painfully true. As an employee of a service provider within a very large academic library, I find this unacceptable. The solution from my perspective is not to punt responsibility for persistent identifiers. The solution is to confront each of those becauses and learn from our mistakes, and (as information service providers who oughta know better) to better steward and manage identifiers for data sets (and other deposits). I digress.   Are there other compelling arguments for using indirect identifiers to cite data sets? This is where you come in.   Back to the main point. Here is the million-dollar question about using (e.g.) DOIs for data sets: who manages these DOIs? Is it the service provider (such as DataCite, or Penn State ScholarSphere)? Or is it the owner of the data set?   If it’s the service provider, how are they to know when data owners move their content elsewhere? And how does that scale?   If it’s the data owner, uh, really? Do we realistically expect data owners to manage their own DOIs? I may be being cynical here, but I somehow don’t see that happening on any scale that has an appreciable impact on the broader issue of data citability and identifier persistence.  ","categories": [],
        "tags": [],
        "url": "/blog/2012/10/27/understanding-eg-dois-for-data-sets/",
        "teaser": null
      }]
